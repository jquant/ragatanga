{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ragatanga","text":"<p>Ragatanga is a hybrid retrieval system that combines ontology-based reasoning with semantic search for powerful knowledge retrieval.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\udcaa Hybrid Retrieval: Combines SPARQL queries against an ontology with semantic search for comprehensive knowledge retrieval</li> <li>\ud83e\udde0 Adaptive Parameters: Dynamically adjusts retrieval parameters based on query complexity and type</li> <li>\ud83d\udd04 Multiple Embedding Providers: Support for OpenAI, HuggingFace, and Sentence Transformers embeddings</li> <li>\ud83d\udcac Multiple LLM Providers: Support for OpenAI, HuggingFace, Ollama, and Anthropic LLMs</li> <li>\ud83c\udf10 Comprehensive API: FastAPI endpoints for querying and managing knowledge</li> <li>\ud83d\udcca Confidence Scoring: Ranks results with confidence scores for higher quality answers</li> <li>\ud83c\udf0d Multilingual Support: Translates queries to match your ontology's language</li> <li>\u2699\ufe0f Flexible Configuration: Comprehensive configuration options through environment variables and config module</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install the latest version from PyPI\npip install ragatanga\n</code></pre> <pre><code>import asyncio\nfrom ragatanga.core.ontology import OntologyManager\nfrom ragatanga.core.retrieval import AdaptiveRetriever\n\nasync def main():\n    # Initialize with your ontology file\n    ontology_manager = OntologyManager(\"path/to/ontology.ttl\")\n    await ontology_manager.load_and_materialize()\n\n    # Create retriever\n    retriever = AdaptiveRetriever(ontology_manager)\n\n    # Query your knowledge base\n    results = await retriever.retrieve(\"What is Ragatanga?\")\n    print(results)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Explore the documentation to learn more about Ragatanga:</p> <ul> <li>Getting Started: Installation and basic setup</li> <li>Usage Guide: Detailed usage examples</li> <li>Architecture: Technical overview of Ragatanga's design</li> <li>API Reference: Detailed API documentation</li> <li>Contributing: Guidelines for contributing to Ragatanga </li> </ul>"},{"location":"DOCUMENTATION_GUIDE/","title":"Documentation Guide","text":"<p>This guide provides information on how to maintain and enhance the Ragatanga documentation.</p>"},{"location":"DOCUMENTATION_GUIDE/#documentation-structure","title":"Documentation Structure","text":"<p>The documentation is built using MkDocs with the Material theme and several enhancements:</p> <ul> <li>API Documentation: Automatically generated using mkdocstrings</li> <li>Diagrams: Interactive diagrams using Mermaid.js</li> <li>Versioning: Document versioning using mike</li> <li>Search: Enhanced search functionality</li> <li>Examples: Real-world usage examples</li> </ul>"},{"location":"DOCUMENTATION_GUIDE/#building-the-documentation","title":"Building the Documentation","text":"<p>To build and serve the documentation locally:</p> <pre><code># Install dependencies\npip install mkdocs mkdocs-material mkdocstrings mkdocstrings-python mike\n\n# Serve documentation locally (for development)\nmkdocs serve\n\n# Build documentation\nmkdocs build\n</code></pre>"},{"location":"DOCUMENTATION_GUIDE/#versioning-documentation","title":"Versioning Documentation","text":"<p>The documentation uses mike for versioning. This allows you to maintain documentation for multiple versions of Ragatanga.</p>"},{"location":"DOCUMENTATION_GUIDE/#creating-a-new-version","title":"Creating a New Version","text":"<p>To create a new version of the documentation:</p> <pre><code># Deploy a new version\nmike deploy [version]\n\n# Example: Deploy version 0.3.1 and set it as the latest\nmike deploy 0.3.1 latest\n\n# Set the default version (shown when no version is specified)\nmike set-default latest\n</code></pre>"},{"location":"DOCUMENTATION_GUIDE/#updating-existing-versions","title":"Updating Existing Versions","text":"<p>To update an existing version of the documentation:</p> <pre><code># Update a specific version\nmike deploy [version] --update\n\n# Example: Update version 0.3.1\nmike deploy 0.3.1 --update\n</code></pre>"},{"location":"DOCUMENTATION_GUIDE/#deploying-to-github-pages","title":"Deploying to GitHub Pages","text":"<p>The documentation can be deployed to GitHub Pages in two ways:</p> <ol> <li>Automatically: Through GitHub Actions workflows</li> <li><code>deploy-docs.yml</code>: Deploys the latest documentation when changes are pushed to main</li> <li> <p><code>versioned-docs.yml</code>: Deploys versioned documentation when a release is published</p> </li> <li> <p>Manually: Using the following command:    <pre><code>mike deploy [version] --push\n</code></pre></p> </li> </ol>"},{"location":"DOCUMENTATION_GUIDE/#adding-content","title":"Adding Content","text":""},{"location":"DOCUMENTATION_GUIDE/#adding-new-pages","title":"Adding New Pages","text":"<ol> <li>Create a new Markdown file in the <code>docs</code> directory</li> <li>Add the file to the navigation in <code>mkdocs.yml</code>:    <pre><code>nav:\n  - Home: index.md\n  - Your New Page: your-new-page.md\n</code></pre></li> </ol>"},{"location":"DOCUMENTATION_GUIDE/#adding-diagrams","title":"Adding Diagrams","text":"<p>Add diagrams using Mermaid.js:</p> <pre><code>graph TD\n    A[Start] --&gt; B[Process]\n    B --&gt; C[End]</code></pre>"},{"location":"DOCUMENTATION_GUIDE/#adding-api-documentation","title":"Adding API Documentation","text":"<p>Update the API reference with docstrings from your code:</p> <pre><code>::: ragatanga.your.module.YourClass\n    options:\n      show_root_heading: true\n      show_source: true\n      members: true\n</code></pre>"},{"location":"DOCUMENTATION_GUIDE/#enhanced-features","title":"Enhanced Features","text":""},{"location":"DOCUMENTATION_GUIDE/#improved-search","title":"Improved Search","text":"<p>The documentation includes enhanced search functionality:</p> <ul> <li>Syntax highlighting in search results</li> <li>Search suggestions</li> <li>Search sharing</li> <li>Full text indexing</li> </ul>"},{"location":"DOCUMENTATION_GUIDE/#additional-features","title":"Additional Features","text":"<ul> <li>Code Annotations: Add annotations to code examples</li> <li>Linked Content Tabs: Link tabs with the same labels</li> <li>Social Cards: Generated social media cards</li> <li>Analytics: Google Analytics integration (requires setting <code>GOOGLE_ANALYTICS_KEY</code>)</li> <li>Minification: HTML minification for faster loading</li> </ul>"},{"location":"DOCUMENTATION_GUIDE/#maintenance-checklist","title":"Maintenance Checklist","text":"<ul> <li>[ ] Update documentation when adding new features</li> <li>[ ] Maintain proper versioning</li> <li>[ ] Ensure API documentation is up-to-date</li> <li>[ ] Add diagrams for new architectural components</li> <li>[ ] Add examples for new functionality</li> <li>[ ] Check for broken links periodically</li> <li>[ ] Optimize search keywords and metadata </li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>This page provides documentation for Ragatanga's API. The documentation is automatically generated from docstrings in the codebase.</p>"},{"location":"api-reference/#core-module","title":"Core Module","text":""},{"location":"api-reference/#ontologymanager","title":"OntologyManager","text":"<pre><code># Example usage\nfrom ragatanga.core.ontology import OntologyManager\n\nontology_manager = OntologyManager(\"path/to/ontology.ttl\")\nawait ontology_manager.load_and_materialize()\n</code></pre>"},{"location":"api-reference/#adaptiveretriever","title":"AdaptiveRetriever","text":"<pre><code># Example usage\nfrom ragatanga.core.retrieval import AdaptiveRetriever\n\nretriever = AdaptiveRetriever(ontology_manager)\nresults = await retriever.retrieve(\"What is Ragatanga?\")\n</code></pre>"},{"location":"api-reference/#configuration-module","title":"Configuration Module","text":""},{"location":"api-reference/#retrievalconfig","title":"RetrievalConfig","text":"<pre><code># Example usage\nfrom ragatanga.core.config import RetrievalConfig\n\nconfig = RetrievalConfig(\n    semantic_search_weight=0.7,\n    ontology_search_weight=0.3,\n    max_results=10\n)\n</code></pre>"},{"location":"api-reference/#utility-module","title":"Utility Module","text":""},{"location":"api-reference/#knowledgebasebuilder","title":"KnowledgeBaseBuilder","text":"<pre><code># Example usage\nfrom ragatanga.core.knowledge import KnowledgeBaseBuilder\n\nkb_builder = KnowledgeBaseBuilder(ontology_manager)\nawait kb_builder.add_markdown_file(\"path/to/knowledge.md\")\nawait kb_builder.save(\"knowledge_base.md\")\n</code></pre> <p>Note on API Documentation</p> <p>As the codebase evolves, make sure to update the documentation by adding proper docstrings to your code. Uncomment the API reference sections above when the corresponding modules are available. </p>"},{"location":"architecture/","title":"Ragatanga Architecture","text":"<p>This document provides a technical overview of Ragatanga's architecture, components, and design decisions.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>Ragatanga is a hybrid retrieval system that combines ontology-based reasoning with semantic search. This combination provides several advantages:</p> <ol> <li>Structure + Unstructured: Combines the precision of ontology/SPARQL queries with the flexibility of semantic search</li> <li>Adaptive Parameters: Adjusts retrieval strategies based on query characteristics</li> <li>Extensible Design: Supports multiple embedding and LLM providers</li> <li>Confidence Scoring: Ranks results by confidence for better answers</li> </ol>"},{"location":"architecture/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Ragatanga System                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502                   \u2502                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Core         \u2502 \u2502    Config    \u2502   \u2502     Version     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                        \n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502                   \u2502        API         \u2502\n\u2502  \u2502  Ontology   \u2502\u25c4\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502                   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n</code></pre>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#ontology-manager","title":"Ontology Manager","text":"<p>The <code>OntologyManager</code> is responsible for:</p> <ul> <li>Loading and parsing ontology files (TTL format)</li> <li>Materializing inferences using RDFS/OWL reasoning</li> <li>Providing SPARQL query capabilities</li> <li>Managing the knowledge graph</li> </ul>"},{"location":"architecture/#adaptive-retriever","title":"Adaptive Retriever","text":"<p>The <code>AdaptiveRetriever</code> is the central component that:</p> <ul> <li>Analyzes queries to determine the best retrieval strategy</li> <li>Combines ontology-based and semantic search results</li> <li>Ranks and scores results based on confidence</li> <li>Provides a unified interface for all retrieval operations</li> </ul>"},{"location":"architecture/#embedding-manager","title":"Embedding Manager","text":"<p>The <code>EmbeddingManager</code> handles:</p> <ul> <li>Text embedding generation using various providers</li> <li>Vector storage and retrieval</li> <li>Similarity search operations</li> <li>Caching of embeddings for performance</li> </ul>"},{"location":"architecture/#llm-manager","title":"LLM Manager","text":"<p>The <code>LLMManager</code> provides:</p> <ul> <li>Integration with multiple LLM providers</li> <li>Query enhancement and reformulation</li> <li>Structured answer generation</li> <li>Confidence estimation</li> </ul>"},{"location":"architecture/#data-flow","title":"Data Flow","text":"<ol> <li>Query Analysis: Incoming queries are analyzed to determine complexity and type</li> <li>Retrieval Strategy Selection: Based on analysis, the system selects appropriate retrieval methods</li> <li>Parallel Execution: Ontology and semantic search operations run in parallel</li> <li>Result Fusion: Results from different sources are combined and ranked</li> <li>Answer Generation: Final answers are generated with confidence scores</li> </ol>"},{"location":"architecture/#extension-points","title":"Extension Points","text":"<p>Ragatanga is designed to be extensible in several ways:</p> <ul> <li>Custom Embedding Providers: Add support for new embedding models</li> <li>Custom LLM Providers: Integrate with additional LLM services</li> <li>Custom Retrieval Strategies: Implement specialized retrieval methods</li> <li>Custom Scoring Functions: Define domain-specific relevance scoring </li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to Ragatanga will be documented in this file.</p>"},{"location":"changelog/#031-2025-02-25","title":"[0.3.1] - 2025-02-25","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Support for Anthropic Claude models</li> <li>Multilingual query translation</li> <li>Confidence scoring for all result types</li> <li>New utility functions for knowledge base management</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Improved SPARQL query generation</li> <li>Enhanced result ranking algorithm</li> <li>Better error handling for API requests</li> <li>Updated documentation</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Issue with embedding cache persistence</li> <li>Bug in ontology materialization process</li> <li>Performance bottleneck in parallel retrieval</li> </ul>"},{"location":"changelog/#030-2025-02-10","title":"[0.3.0] - 2025-02-10","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Adaptive retrieval system</li> <li>Support for multiple embedding providers</li> <li>Support for multiple LLM providers</li> <li>FastAPI integration</li> <li>Comprehensive configuration system</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Complete architecture redesign</li> <li>Improved ontology reasoning</li> <li>Enhanced semantic search capabilities</li> <li>Better documentation</li> </ul>"},{"location":"changelog/#021-2025-01-20","title":"[0.2.1] - 2025-01-20","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Basic semantic search functionality</li> <li>Support for OpenAI embeddings</li> <li>Simple API for querying</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Various bugs and performance issues</li> </ul>"},{"location":"changelog/#020-2025-01-10","title":"[0.2.0] - 2025-01-10","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Ontology-based reasoning</li> <li>SPARQL query support</li> <li>Knowledge base management</li> </ul>"},{"location":"changelog/#010-2025-01-05","title":"[0.1.0] - 2025-01-05","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Initial release</li> <li>Basic project structure</li> <li>Core functionality </li> </ul>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>This document provides detailed information about configuring Ragatanga for your specific needs.</p>"},{"location":"configuration/#configuration-methods","title":"Configuration Methods","text":"<p>Ragatanga can be configured in several ways:</p> <ol> <li>Environment Variables: Set configuration through environment variables</li> <li>Configuration Files: Use <code>.env</code> files for persistent configuration</li> <li>Programmatic Configuration: Configure components directly in code</li> <li>Configuration Objects: Use configuration classes for fine-grained control</li> </ol>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"configuration/#core-configuration","title":"Core Configuration","text":"Variable Description Default Example <code>RAGATANGA_LOG_LEVEL</code> Logging level <code>INFO</code> <code>DEBUG</code> <code>RAGATANGA_CACHE_DIR</code> Directory for caching <code>~/.ragatanga/cache</code> <code>/tmp/ragatanga_cache</code> <code>RAGATANGA_MAX_RESULTS</code> Maximum results to return <code>5</code> <code>10</code> <code>RAGATANGA_CONFIDENCE_THRESHOLD</code> Minimum confidence score <code>0.6</code> <code>0.8</code>"},{"location":"configuration/#embedding-configuration","title":"Embedding Configuration","text":"Variable Description Default Example <code>RAGATANGA_EMBEDDING_PROVIDER</code> Embedding provider <code>openai</code> <code>sentence_transformers</code> <code>RAGATANGA_EMBEDDING_MODEL</code> Embedding model name <code>text-embedding-ada-002</code> <code>all-MiniLM-L6-v2</code> <code>RAGATANGA_EMBEDDING_BATCH_SIZE</code> Batch size for embeddings <code>32</code> <code>64</code> <code>RAGATANGA_EMBEDDING_CACHE</code> Enable embedding cache <code>true</code> <code>false</code>"},{"location":"configuration/#llm-configuration","title":"LLM Configuration","text":"Variable Description Default Example <code>RAGATANGA_LLM_PROVIDER</code> LLM provider <code>openai</code> <code>anthropic</code> <code>RAGATANGA_LLM_MODEL</code> LLM model name <code>gpt-3.5-turbo</code> <code>claude-2</code> <code>RAGATANGA_LLM_TEMPERATURE</code> LLM temperature <code>0.0</code> <code>0.7</code> <code>RAGATANGA_LLM_MAX_TOKENS</code> Maximum tokens for LLM <code>1024</code> <code>2048</code>"},{"location":"configuration/#api-keys","title":"API Keys","text":"Variable Description Example <code>OPENAI_API_KEY</code> OpenAI API key <code>sk-...</code> <code>ANTHROPIC_API_KEY</code> Anthropic API key <code>sk-ant-...</code> <code>HUGGINGFACE_API_KEY</code> HuggingFace API key <code>hf_...</code>"},{"location":"configuration/#configuration-file","title":"Configuration File","text":"<p>You can create a <code>.env</code> file in your project root with the following format:</p> <pre><code># Core Configuration\nRAGATANGA_LOG_LEVEL=INFO\nRAGATANGA_CACHE_DIR=~/.ragatanga/cache\nRAGATANGA_MAX_RESULTS=5\nRAGATANGA_CONFIDENCE_THRESHOLD=0.6\n\n# Embedding Configuration\nRAGATANGA_EMBEDDING_PROVIDER=openai\nRAGATANGA_EMBEDDING_MODEL=text-embedding-ada-002\nRAGATANGA_EMBEDDING_BATCH_SIZE=32\nRAGATANGA_EMBEDDING_CACHE=true\n\n# LLM Configuration\nRAGATANGA_LLM_PROVIDER=openai\nRAGATANGA_LLM_MODEL=gpt-3.5-turbo\nRAGATANGA_LLM_TEMPERATURE=0.0\nRAGATANGA_LLM_MAX_TOKENS=1024\n\n# API Keys\nOPENAI_API_KEY=your-openai-api-key\n</code></pre>"},{"location":"configuration/#programmatic-configuration","title":"Programmatic Configuration","text":""},{"location":"configuration/#configuring-the-ontology-manager","title":"Configuring the Ontology Manager","text":"<pre><code>from ragatanga.core.ontology import OntologyManager\nfrom ragatanga.core.config import OntologyConfig\n\n# Create configuration\nontology_config = OntologyConfig(\n    reasoning_level=\"RDFS\",\n    cache_materialized=True,\n    prefixes={\n        \"ex\": \"http://example.org/\",\n        \"schema\": \"http://schema.org/\"\n    }\n)\n\n# Initialize with configuration\nontology_manager = OntologyManager(\n    \"path/to/ontology.ttl\",\n    config=ontology_config\n)\n</code></pre>"},{"location":"configuration/#configuring-the-embedding-manager","title":"Configuring the Embedding Manager","text":"<pre><code>from ragatanga.core.embeddings import EmbeddingManager\nfrom ragatanga.core.config import EmbeddingConfig\n\n# Create configuration\nembedding_config = EmbeddingConfig(\n    provider=\"sentence_transformers\",\n    model_name=\"all-MiniLM-L6-v2\",\n    batch_size=64,\n    cache_embeddings=True\n)\n\n# Initialize with configuration\nembedding_manager = EmbeddingManager(config=embedding_config)\n</code></pre>"},{"location":"configuration/#configuring-the-llm-manager","title":"Configuring the LLM Manager","text":"<pre><code>from ragatanga.core.llm import LLMManager\nfrom ragatanga.core.config import LLMConfig\n\n# Create configuration\nllm_config = LLMConfig(\n    provider=\"openai\",\n    model_name=\"gpt-4\",\n    temperature=0.2,\n    max_tokens=2048,\n    system_prompt=\"You are a helpful assistant specialized in knowledge retrieval.\"\n)\n\n# Initialize with configuration\nllm_manager = LLMManager(config=llm_config)\n</code></pre>"},{"location":"configuration/#configuring-the-retriever","title":"Configuring the Retriever","text":"<pre><code>from ragatanga.core.retrieval import AdaptiveRetriever\nfrom ragatanga.core.config import RetrievalConfig\n\n# Create configuration\nretrieval_config = RetrievalConfig(\n    semantic_search_weight=0.7,\n    ontology_search_weight=0.3,\n    max_results=10,\n    confidence_threshold=0.6,\n    use_query_enhancement=True,\n    use_multilingual=True\n)\n\n# Initialize with configuration\nretriever = AdaptiveRetriever(\n    ontology_manager,\n    embedding_manager=embedding_manager,\n    llm_manager=llm_manager,\n    config=retrieval_config\n)\n</code></pre>"},{"location":"configuration/#configuration-classes","title":"Configuration Classes","text":"<p>Ragatanga provides several configuration classes for fine-grained control:</p> <ul> <li><code>OntologyConfig</code>: Configuration for the ontology manager</li> <li><code>EmbeddingConfig</code>: Configuration for embedding generation and storage</li> <li><code>LLMConfig</code>: Configuration for language model interactions</li> <li><code>RetrievalConfig</code>: Configuration for retrieval operations</li> <li><code>APIConfig</code>: Configuration for the API server</li> </ul> <p>See the API Reference for detailed documentation of these classes. </p>"},{"location":"contributing/","title":"Contributing to Ragatanga","text":"<p>Thank you for your interest in contributing to Ragatanga! This document provides guidelines and instructions for contributing to the project.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please read and follow our Code of Conduct to ensure a positive and inclusive environment for everyone.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/yourusername/ragatanga.git\ncd ragatanga\n</code></pre></li> <li>Set up a virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></li> <li>Install development dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre></li> </ol>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<p>We use pytest for testing:</p> <pre><code># Run all tests\npytest\n\n# Run specific tests\npytest tests/test_ontology.py\n\n# Run with coverage\npytest --cov=ragatanga\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#branching-strategy","title":"Branching Strategy","text":"<ul> <li><code>main</code>: Stable release branch</li> <li><code>develop</code>: Development branch for next release</li> <li>Feature branches: Create from <code>develop</code> with format <code>feature/your-feature-name</code></li> <li>Bugfix branches: Create from <code>develop</code> with format <code>bugfix/issue-description</code></li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create a new branch from <code>develop</code></li> <li>Make your changes</li> <li>Run tests and ensure they pass</li> <li>Update documentation if necessary</li> <li>Submit a pull request to the <code>develop</code> branch</li> <li>Wait for code review and address any feedback</li> </ol>"},{"location":"contributing/#coding-standards","title":"Coding Standards","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We follow PEP 8 and use Black for code formatting:</p> <pre><code># Format code\nblack ragatanga tests\n\n# Check code style\nflake8 ragatanga tests\n</code></pre>"},{"location":"contributing/#type-hints","title":"Type Hints","text":"<p>We use type hints throughout the codebase:</p> <pre><code>def example_function(param1: str, param2: int) -&gt; bool:\n    \"\"\"\n    Example function with type hints.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2\n\n    Returns:\n        Description of return value\n    \"\"\"\n    return True\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Use docstrings for all public classes and methods</li> <li>Follow Google style docstrings</li> <li>Update documentation when adding or changing features</li> </ul>"},{"location":"contributing/#adding-features","title":"Adding Features","text":""},{"location":"contributing/#adding-a-new-embedding-provider","title":"Adding a New Embedding Provider","text":"<ol> <li>Create a new file in <code>ragatanga/core/embeddings/providers/</code></li> <li>Implement the <code>EmbeddingProvider</code> interface</li> <li>Register the provider in <code>ragatanga/core/embeddings/manager.py</code></li> <li>Add tests in <code>tests/core/embeddings/</code></li> <li>Update documentation</li> </ol>"},{"location":"contributing/#adding-a-new-llm-provider","title":"Adding a New LLM Provider","text":"<ol> <li>Create a new file in <code>ragatanga/core/llm/providers/</code></li> <li>Implement the <code>LLMProvider</code> interface</li> <li>Register the provider in <code>ragatanga/core/llm/manager.py</code></li> <li>Add tests in <code>tests/core/llm/</code></li> <li>Update documentation</li> </ol>"},{"location":"contributing/#release-process","title":"Release Process","text":"<ol> <li>Update version in <code>pyproject.toml</code></li> <li>Update <code>CHANGELOG.md</code></li> <li>Create a pull request from <code>develop</code> to <code>main</code></li> <li>After approval and merge, create a new release on GitHub</li> <li>CI/CD will automatically publish to PyPI</li> </ol>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<p>If you need help or have questions:</p> <ul> <li>Open an issue on GitHub</li> <li>Join our community discussions</li> <li>Reach out to the maintainers</li> </ul> <p>Thank you for contributing to Ragatanga! </p>"},{"location":"diagrams/","title":"Diagrams","text":"<p>This page demonstrates how to use diagrams to better explain Ragatanga's architecture and workflow.</p>"},{"location":"diagrams/#system-architecture","title":"System Architecture","text":"<p>The following diagram illustrates the high-level architecture of Ragatanga:</p> <pre><code>graph TD\n    User[User/Client] --&gt;|Query| API[API Layer]\n    API --&gt;|Process Query| Core[Core Components]\n\n    subgraph Core Components\n        Retriever[Adaptive Retriever] --&gt;|Query| Ontology[Ontology Manager]\n        Retriever --&gt;|Search| Embeddings[Embedding Manager]\n        Retriever --&gt;|Generate Answer| LLM[LLM Manager]\n    end\n\n    Ontology --&gt;|Load| OntologyFile[(Ontology File)]\n    Embeddings --&gt;|Access| KnowledgeBase[(Knowledge Base)]\n\n    Core --&gt;|Results| API\n    API --&gt;|Response| User</code></pre>"},{"location":"diagrams/#data-flow","title":"Data Flow","text":"<p>The following diagram illustrates the data flow during a query:</p> <pre><code>sequenceDiagram\n    participant User\n    participant API\n    participant Retriever\n    participant Ontology\n    participant Embeddings\n    participant LLM\n\n    User-&gt;&gt;API: Submit Query\n    API-&gt;&gt;Retriever: Process Query\n\n    par Parallel Processing\n        Retriever-&gt;&gt;Ontology: SPARQL Query\n        Ontology--&gt;&gt;Retriever: Ontology Results\n\n        Retriever-&gt;&gt;Embeddings: Semantic Search\n        Embeddings--&gt;&gt;Retriever: Semantic Results\n    end\n\n    Retriever-&gt;&gt;Retriever: Combine Results\n    Retriever-&gt;&gt;LLM: Generate Structured Answer\n    LLM--&gt;&gt;Retriever: Structured Answer\n\n    Retriever--&gt;&gt;API: Complete Results\n    API--&gt;&gt;User: Response</code></pre>"},{"location":"diagrams/#configuration-components","title":"Configuration Components","text":"<p>The following diagram shows the configuration components:</p> <pre><code>classDiagram\n    class RetrievalConfig {\n        +float semantic_search_weight\n        +float ontology_search_weight\n        +int max_results\n        +float confidence_threshold\n        +apply_defaults()\n    }\n\n    class OntologyConfig {\n        +string reasoning_level\n        +bool cache_materialized\n        +dict prefixes\n        +validate()\n    }\n\n    class EmbeddingConfig {\n        +string provider\n        +string model_name\n        +int batch_size\n        +bool cache_embeddings\n        +get_provider()\n    }\n\n    class LLMConfig {\n        +string provider\n        +string model_name\n        +float temperature\n        +int max_tokens\n        +string system_prompt\n        +get_provider()\n    }\n\n    RetrievalConfig --&gt; OntologyConfig\n    RetrievalConfig --&gt; EmbeddingConfig\n    RetrievalConfig --&gt; LLMConfig</code></pre>"},{"location":"diagrams/#adding-your-own-diagrams","title":"Adding Your Own Diagrams","text":"<p>You can add your own diagrams to the documentation using Mermaid.js. Here's how:</p> <ol> <li>Add the Mermaid extension to your <code>mkdocs.yml</code>:</li> </ol> <pre><code>markdown_extensions:\n  - pymdownx.superfences:\n      custom_fences:\n        - name: mermaid\n          class: mermaid\n          format: !!python/name:pymdownx.superfences.fence_code_format\n</code></pre> <ol> <li>Create your diagram using Mermaid syntax:</li> </ol> <p><pre><code>```mermaid\ngraph TD\n    A[Start] --&gt; B[Process]\n    B --&gt; C[End]\n</code></pre> ```</p> <ol> <li>You can create various types of diagrams:</li> <li>Flowcharts (<code>graph TD</code> or <code>graph LR</code>)</li> <li>Sequence diagrams (<code>sequenceDiagram</code>)</li> <li>Class diagrams (<code>classDiagram</code>)</li> <li>Entity Relationship diagrams (<code>erDiagram</code>)</li> <li>Gantt charts (<code>gantt</code>)</li> <li>State diagrams (<code>stateDiagram-v2</code>) </li> </ol>"},{"location":"examples/","title":"Real-World Examples","text":"<p>This page provides in-depth examples of using Ragatanga in real-world scenarios.</p>"},{"location":"examples/#1-building-a-custom-knowledge-base","title":"1. Building a Custom Knowledge Base","text":"<p>This example demonstrates how to build a custom knowledge base from various sources including Markdown files, websites, and structured data.</p> <pre><code>import asyncio\nfrom ragatanga.core.ontology import OntologyManager\nfrom ragatanga.core.knowledge import KnowledgeBaseBuilder\nfrom ragatanga.utils.web import extract_content\n\nasync def build_custom_kb():\n    # Initialize ontology\n    ontology_path = \"company_ontology.ttl\"\n    ontology_manager = OntologyManager(ontology_path)\n    await ontology_manager.load_and_materialize()\n\n    # Create knowledge base builder\n    kb_builder = KnowledgeBaseBuilder(ontology_manager)\n\n    # Add content from Markdown files\n    await kb_builder.add_markdown_file(\"company_handbook.md\")\n    await kb_builder.add_markdown_file(\"product_documentation.md\")\n\n    # Add content from websites\n    company_blog_content = await extract_content(\"https://example.com/blog\")\n    await kb_builder.add_text(company_blog_content, source=\"Company Blog\")\n\n    # Add content from structured data\n    with open(\"product_catalog.json\", \"r\") as f:\n        import json\n        products = json.load(f)\n\n    for product in products:\n        product_text = f\"\"\"\n        # {product['name']}\n\n        {product['description']}\n\n        - Price: ${product['price']}\n        - Category: {product['category']}\n        - Features: {', '.join(product['features'])}\n        \"\"\"\n        await kb_builder.add_text(product_text, source=f\"Product: {product['name']}\")\n\n    # Save the knowledge base\n    await kb_builder.save(\"company_knowledge_base.md\")\n\n    # Generate embeddings\n    from ragatanga.core.embeddings import EmbeddingManager\n    embedding_manager = EmbeddingManager()\n    await embedding_manager.generate_embeddings(\"company_knowledge_base.md\")\n\n    print(\"Knowledge base built successfully!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(build_custom_kb())\n</code></pre>"},{"location":"examples/#2-building-a-question-answering-system","title":"2. Building a Question-Answering System","text":"<p>This example shows how to build a question-answering system using Ragatanga:</p> <pre><code>import asyncio\nfrom ragatanga.core.ontology import OntologyManager\nfrom ragatanga.core.retrieval import AdaptiveRetriever\nfrom ragatanga.core.query import generate_structured_answer\nfrom ragatanga.core.config import RetrievalConfig, LLMConfig\n\nasync def initialize_qa_system():\n    # Initialize ontology\n    ontology_path = \"knowledge_ontology.ttl\"\n    ontology_manager = OntologyManager(ontology_path)\n    await ontology_manager.load_and_materialize()\n\n    # Configure retrieval parameters\n    retrieval_config = RetrievalConfig(\n        semantic_search_weight=0.7,\n        ontology_search_weight=0.3,\n        max_results=5,\n        confidence_threshold=0.6,\n        use_query_enhancement=True\n    )\n\n    # Configure LLM\n    llm_config = LLMConfig(\n        provider=\"openai\",\n        model_name=\"gpt-4\",\n        temperature=0.2,\n        max_tokens=500,\n        system_prompt=\"You are an expert assistant that provides accurate and concise answers based on the provided context.\"\n    )\n\n    # Initialize retriever\n    from ragatanga.core.llm import LLMManager\n    llm_manager = LLMManager(config=llm_config)\n\n    retriever = AdaptiveRetriever(\n        ontology_manager,\n        llm_manager=llm_manager,\n        config=retrieval_config\n    )\n\n    return retriever\n\nasync def answer_question(retriever, question):\n    # Retrieve information\n    results = await retriever.retrieve(question)\n\n    # Generate structured answer\n    answer = await generate_structured_answer(question, results)\n\n    # Format output\n    if not results:\n        return {\n            \"question\": question,\n            \"answer\": \"I don't have enough information to answer this question.\",\n            \"confidence\": 0.0,\n            \"sources\": []\n        }\n\n    sources = [\n        {\"content\": r.content[:100] + \"...\", \"confidence\": r.confidence}\n        for r in results\n    ]\n\n    return {\n        \"question\": question,\n        \"answer\": answer,\n        \"confidence\": max(r.confidence for r in results),\n        \"sources\": sources\n    }\n\nasync def main():\n    retriever = await initialize_qa_system()\n\n    questions = [\n        \"What is Ragatanga's architecture?\",\n        \"How do I configure the embedding provider?\",\n        \"What are the key features of the adaptive retriever?\"\n    ]\n\n    for question in questions:\n        response = await answer_question(retriever, question)\n        print(f\"\\nQ: {response['question']}\")\n        print(f\"A: {response['answer']}\")\n        print(f\"Confidence: {response['confidence']:.2f}\")\n        print(\"Sources:\")\n        for i, source in enumerate(response['sources']):\n            print(f\"  {i+1}. {source['content']} (Confidence: {source['confidence']:.2f})\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/#3-building-a-rest-api-with-fastapi","title":"3. Building a REST API with FastAPI","text":"<p>This example demonstrates how to create a RESTful API using Ragatanga and FastAPI:</p> <pre><code>import asyncio\nfrom fastapi import FastAPI, HTTPException, Depends\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any, Optional\nfrom contextlib import asynccontextmanager\n\nfrom ragatanga.core.ontology import OntologyManager\nfrom ragatanga.core.retrieval import AdaptiveRetriever\n\n# Define models\nclass QueryRequest(BaseModel):\n    query: str\n    max_results: Optional[int] = 5\n    confidence_threshold: Optional[float] = 0.6\n\nclass ResultItem(BaseModel):\n    content: str\n    confidence: float\n    source: str\n\nclass QueryResponse(BaseModel):\n    query: str\n    answer: str\n    results: List[ResultItem]\n    execution_time_ms: float\n\n# Global variables\nontology_manager = None\nretriever = None\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Initialize on startup\n    global ontology_manager, retriever\n\n    print(\"Initializing Ragatanga...\")\n\n    ontology_path = \"knowledge_ontology.ttl\"\n    ontology_manager = OntologyManager(ontology_path)\n    await ontology_manager.load_and_materialize()\n\n    retriever = AdaptiveRetriever(ontology_manager)\n\n    print(\"Ragatanga initialized successfully!\")\n\n    yield\n\n    # Cleanup on shutdown\n    print(\"Shutting down Ragatanga...\")\n\napp = FastAPI(lifespan=lifespan, title=\"Ragatanga API\", description=\"API for querying knowledge using Ragatanga\")\n\nasync def get_retriever():\n    if retriever is None:\n        raise HTTPException(status_code=503, detail=\"Service not initialized\")\n    return retriever\n\n@app.post(\"/query\", response_model=QueryResponse)\nasync def query(request: QueryRequest, retriever: AdaptiveRetriever = Depends(get_retriever)):\n    import time\n    start_time = time.time()\n\n    try:\n        # Process query\n        results = await retriever.retrieve(\n            request.query,\n            max_results=request.max_results,\n            confidence_threshold=request.confidence_threshold\n        )\n\n        # Generate answer\n        from ragatanga.core.query import generate_structured_answer\n        answer = await generate_structured_answer(request.query, results)\n\n        # Format response\n        result_items = [\n            ResultItem(\n                content=result.content,\n                confidence=result.confidence,\n                source=result.source\n            )\n            for result in results\n        ]\n\n        execution_time = (time.time() - start_time) * 1000  # convert to ms\n\n        return QueryResponse(\n            query=request.query,\n            answer=answer,\n            results=result_items,\n            execution_time_ms=execution_time\n        )\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"examples/#4-command-line-interface","title":"4. Command-Line Interface","text":"<p>This example shows how to create a command-line interface for Ragatanga:</p> <pre><code>import asyncio\nimport argparse\nimport sys\nimport json\nfrom ragatanga.core.ontology import OntologyManager\nfrom ragatanga.core.retrieval import AdaptiveRetriever\nfrom ragatanga.core.config import RetrievalConfig\n\nasync def setup_ragatanga(ontology_path, kb_path=None):\n    # Initialize ontology manager\n    ontology_manager = OntologyManager(ontology_path)\n    await ontology_manager.load_and_materialize()\n\n    # Initialize adaptive retriever\n    retriever = AdaptiveRetriever(ontology_manager)\n\n    return retriever\n\nasync def process_query(retriever, query, output_format=\"text\", max_results=5):\n    # Retrieve information\n    results = await retriever.retrieve(query, max_results=max_results)\n\n    # Generate structured answer\n    from ragatanga.core.query import generate_structured_answer\n    answer = await generate_structured_answer(query, results)\n\n    # Format output\n    if output_format == \"json\":\n        output = {\n            \"query\": query,\n            \"answer\": answer,\n            \"results\": [\n                {\n                    \"content\": r.content,\n                    \"confidence\": r.confidence,\n                    \"source\": r.source\n                }\n                for r in results\n            ]\n        }\n        print(json.dumps(output, indent=2))\n    else:\n        print(f\"Query: {query}\")\n        print(f\"Answer: {answer}\")\n        print(\"\\nSupporting information:\")\n        for i, result in enumerate(results):\n            print(f\"\\n--- Result {i+1} (Confidence: {result.confidence:.2f}) ---\")\n            print(f\"Source: {result.source}\")\n            print(f\"{result.content[:300]}...\")\n\nasync def main():\n    parser = argparse.ArgumentParser(description=\"Ragatanga CLI\")\n    parser.add_argument(\"--ontology\", required=True, help=\"Path to ontology file\")\n    parser.add_argument(\"--knowledge-base\", help=\"Path to knowledge base file\")\n    parser.add_argument(\"--query\", help=\"Query to process\")\n    parser.add_argument(\"--format\", choices=[\"text\", \"json\"], default=\"text\", help=\"Output format\")\n    parser.add_argument(\"--max-results\", type=int, default=5, help=\"Maximum number of results\")\n    parser.add_argument(\"--interactive\", action=\"store_true\", help=\"Run in interactive mode\")\n\n    args = parser.parse_args()\n\n    # Set up Ragatanga\n    retriever = await setup_ragatanga(args.ontology, args.knowledge_base)\n\n    if args.interactive:\n        print(\"Ragatanga Interactive Mode (Ctrl+C to exit)\")\n        print(\"-------------------------------------------\")\n        try:\n            while True:\n                query = input(\"\\nEnter query: \")\n                if not query:\n                    continue\n                await process_query(retriever, query, args.format, args.max_results)\n        except KeyboardInterrupt:\n            print(\"\\nExiting...\")\n    elif args.query:\n        await process_query(retriever, args.query, args.format, args.max_results)\n    else:\n        parser.print_help()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/#5-integration-with-streamlit","title":"5. Integration with Streamlit","text":"<p>This example shows how to create a simple web interface using Streamlit:</p> <pre><code>import streamlit as st\nimport asyncio\nfrom ragatanga.core.ontology import OntologyManager\nfrom ragatanga.core.retrieval import AdaptiveRetriever\nfrom ragatanga.core.query import generate_structured_answer\n\n@st.cache_resource\ndef load_ragatanga():\n    # Create an async wrapper for initialization\n    async def init():\n        ontology_path = \"knowledge_ontology.ttl\"\n        ontology_manager = OntologyManager(ontology_path)\n        await ontology_manager.load_and_materialize()\n        return AdaptiveRetriever(ontology_manager)\n\n    # Run the async init function\n    return asyncio.run(init())\n\n# Initialize Ragatanga\nretriever = load_ragatanga()\n\n# Streamlit UI\nst.title(\"Ragatanga Knowledge Assistant\")\nst.write(\"Ask questions about Ragatanga and get answers from its knowledge base.\")\n\nquery = st.text_input(\"Your question:\", placeholder=\"e.g., What is Ragatanga's architecture?\")\n\nif st.button(\"Submit\") or query:\n    if query:\n        st.write(\"Processing query...\")\n\n        # Create an async function for the query\n        async def process_query():\n            results = await retriever.retrieve(query, max_results=5)\n            answer = await generate_structured_answer(query, results)\n            return results, answer\n\n        # Run the async function\n        results, answer = asyncio.run(process_query())\n\n        # Display the answer\n        st.write(\"## Answer\")\n        st.write(answer)\n\n        # Display the source information\n        st.write(\"## Sources\")\n        for i, result in enumerate(results):\n            with st.expander(f\"Source {i+1} (Confidence: {result.confidence:.2f})\"):\n                st.write(f\"**Source:** {result.source}\")\n                st.write(result.content)\n    else:\n        st.write(\"Please enter a question.\")\n</code></pre>"},{"location":"examples/#running-the-examples","title":"Running the Examples","text":"<p>To run these examples, you'll need to:</p> <ol> <li>Install Ragatanga and its dependencies:</li> </ol> <pre><code>pip install ragatanga\n</code></pre> <ol> <li>Install additional dependencies for specific examples:</li> </ol> <pre><code># For API example\npip install fastapi uvicorn\n\n# For Streamlit example\npip install streamlit\n</code></pre> <ol> <li> <p>Prepare your ontology and knowledge base files as needed.</p> </li> <li> <p>Run the Python scripts for the examples you want to try. </p> </li> </ol>"},{"location":"getting-started/","title":"Getting Started with Ragatanga","text":"<p>This guide will help you get started with Ragatanga, from installation to your first query.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before installing Ragatanga, ensure you have:</p> <ul> <li>Python 3.8 or higher</li> <li>pip (Python package installer)</li> <li>(Optional) A virtual environment tool like venv or conda</li> </ul>"},{"location":"getting-started/#installing-from-pypi","title":"Installing from PyPI","text":"<p>The easiest way to install Ragatanga is from PyPI:</p> <pre><code>pip install ragatanga\n</code></pre>"},{"location":"getting-started/#installing-from-source","title":"Installing from Source","text":"<p>To install the latest development version:</p> <pre><code>git clone https://github.com/jquant/ragatanga.git\ncd ragatanga\npip install -e .\n</code></pre>"},{"location":"getting-started/#basic-setup","title":"Basic Setup","text":""},{"location":"getting-started/#setting-up-your-ontology","title":"Setting Up Your Ontology","text":"<p>Ragatanga requires an ontology file in Turtle (.ttl) format. If you don't have one, you can use a sample ontology:</p> <pre><code>import os\nfrom ragatanga.utils.samples import download_sample_ontology\n\n# Download a sample ontology\nontology_path = download_sample_ontology()\nprint(f\"Sample ontology downloaded to: {ontology_path}\")\n</code></pre>"},{"location":"getting-started/#creating-a-knowledge-base","title":"Creating a Knowledge Base","text":"<p>You can create a knowledge base from Markdown files or other text sources:</p> <pre><code>import asyncio\nfrom ragatanga.core.ontology import OntologyManager\nfrom ragatanga.core.knowledge import KnowledgeBaseBuilder\n\nasync def build_kb():\n    # Initialize ontology\n    ontology_path = \"path/to/ontology.ttl\"\n    ontology_manager = OntologyManager(ontology_path)\n    await ontology_manager.load_and_materialize()\n\n    # Build knowledge base from Markdown\n    kb_builder = KnowledgeBaseBuilder(ontology_manager)\n    await kb_builder.add_markdown_file(\"path/to/knowledge.md\")\n\n    # Save knowledge base\n    await kb_builder.save(\"knowledge_base.md\")\n\nif __name__ == \"__main__\":\n    asyncio.run(build_kb())\n</code></pre>"},{"location":"getting-started/#your-first-query","title":"Your First Query","text":"<p>Here's a simple example to get you started with querying:</p> <pre><code>import asyncio\nfrom ragatanga.core.ontology import OntologyManager\nfrom ragatanga.core.retrieval import AdaptiveRetriever\n\nasync def first_query():\n    # Initialize with ontology\n    ontology_path = \"path/to/ontology.ttl\"\n    ontology_manager = OntologyManager(ontology_path)\n    await ontology_manager.load_and_materialize()\n\n    # Create retriever\n    retriever = AdaptiveRetriever(ontology_manager)\n\n    # Make a simple query\n    query = \"What is Ragatanga?\"\n    results = await retriever.retrieve(query)\n\n    # Print results\n    print(f\"Query: {query}\")\n    for i, result in enumerate(results):\n        print(f\"\\nResult {i+1} (Confidence: {result.confidence:.2f}):\")\n        print(f\"Content: {result.content[:150]}...\")\n\nif __name__ == \"__main__\":\n    asyncio.run(first_query())\n</code></pre>"},{"location":"getting-started/#configuration","title":"Configuration","text":""},{"location":"getting-started/#basic-configuration","title":"Basic Configuration","text":"<p>You can configure Ragatanga using environment variables or a configuration file:</p> <pre><code># Set API keys\nexport OPENAI_API_KEY=\"your-openai-api-key\"\n\n# Configure embedding model\nexport RAGATANGA_EMBEDDING_PROVIDER=\"openai\"\nexport RAGATANGA_EMBEDDING_MODEL=\"text-embedding-ada-002\"\n</code></pre>"},{"location":"getting-started/#using-a-configuration-file","title":"Using a Configuration File","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code>OPENAI_API_KEY=your-openai-api-key\nRAGATANGA_EMBEDDING_PROVIDER=openai\nRAGATANGA_EMBEDDING_MODEL=text-embedding-ada-002\nRAGATANGA_LLM_PROVIDER=openai\nRAGATANGA_LLM_MODEL=gpt-3.5-turbo\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have Ragatanga set up, you can:</p> <ul> <li>Explore the Usage Guide for more detailed examples</li> <li>Learn about the Architecture to understand how Ragatanga works</li> <li>Check out the API Reference for detailed documentation of all classes and methods </li> </ul>"},{"location":"usage/","title":"Ragatanga Usage Guide","text":"<p>This document provides comprehensive usage guidance for the Ragatanga hybrid retrieval system.</p>"},{"location":"usage/#basic-usage","title":"Basic Usage","text":""},{"location":"usage/#setting-up-ragatanga","title":"Setting Up Ragatanga","text":"<pre><code>import asyncio\nfrom ragatanga.core.ontology import OntologyManager\nfrom ragatanga.core.retrieval import AdaptiveRetriever\nfrom ragatanga.core.query import generate_structured_answer\n\nasync def setup_ragatanga(ontology_path, kb_path=None):\n    \"\"\"Set up Ragatanga with ontology and knowledge base.\"\"\"\n    # Initialize ontology manager\n    ontology_manager = OntologyManager(ontology_path)\n    await ontology_manager.load_and_materialize()\n\n    # Initialize adaptive retriever\n    retriever = AdaptiveRetriever(ontology_manager)\n\n    return ontology_manager, retriever\n</code></pre>"},{"location":"usage/#processing-queries","title":"Processing Queries","text":"<pre><code>async def process_query(query, retriever):\n    \"\"\"Process a natural language query and return results.\"\"\"\n    # Retrieve relevant information\n    results = await retriever.retrieve(query)\n\n    # Generate a structured answer\n    answer = await generate_structured_answer(query, results)\n\n    return {\n        \"query\": query,\n        \"results\": results,\n        \"answer\": answer\n    }\n</code></pre>"},{"location":"usage/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom ragatanga.core.ontology import OntologyManager\nfrom ragatanga.core.retrieval import AdaptiveRetriever\nfrom ragatanga.core.query import generate_structured_answer\n\nasync def main():\n    # Initialize with ontology\n    ontology_path = \"path/to/ontology.ttl\"\n    ontology_manager = OntologyManager(ontology_path)\n    await ontology_manager.load_and_materialize()\n\n    # Create retriever\n    retriever = AdaptiveRetriever(ontology_manager)\n\n    # Process a query\n    query = \"What are the main features of Ragatanga?\"\n    results = await retriever.retrieve(query)\n\n    # Generate answer\n    answer = await generate_structured_answer(query, results)\n\n    print(f\"Query: {query}\")\n    print(f\"Answer: {answer}\")\n\n    # Display results\n    for i, result in enumerate(results):\n        print(f\"\\nResult {i+1} (Confidence: {result.confidence:.2f}):\")\n        print(f\"Source: {result.source}\")\n        print(f\"Content: {result.content[:100]}...\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"usage/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/#customizing-retrieval-parameters","title":"Customizing Retrieval Parameters","text":"<pre><code>from ragatanga.core.config import RetrievalConfig\n\n# Create custom retrieval configuration\nconfig = RetrievalConfig(\n    semantic_search_weight=0.7,\n    ontology_search_weight=0.3,\n    max_results=10,\n    confidence_threshold=0.6\n)\n\n# Initialize retriever with custom config\nretriever = AdaptiveRetriever(ontology_manager, config=config)\n</code></pre>"},{"location":"usage/#using-different-embedding-providers","title":"Using Different Embedding Providers","text":"<pre><code>from ragatanga.core.embeddings import EmbeddingManager\nfrom ragatanga.core.config import EmbeddingConfig\n\n# Configure embedding provider\nembedding_config = EmbeddingConfig(\n    provider=\"sentence_transformers\",\n    model_name=\"all-MiniLM-L6-v2\"\n)\n\n# Create embedding manager\nembedding_manager = EmbeddingManager(config=embedding_config)\n\n# Initialize retriever with custom embedding manager\nretriever = AdaptiveRetriever(\n    ontology_manager, \n    embedding_manager=embedding_manager\n)\n</code></pre>"},{"location":"usage/#using-different-llm-providers","title":"Using Different LLM Providers","text":"<pre><code>from ragatanga.core.llm import LLMManager\nfrom ragatanga.core.config import LLMConfig\n\n# Configure LLM provider\nllm_config = LLMConfig(\n    provider=\"huggingface\",\n    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\"\n)\n\n# Create LLM manager\nllm_manager = LLMManager(config=llm_config)\n\n# Initialize retriever with custom LLM manager\nretriever = AdaptiveRetriever(\n    ontology_manager, \n    llm_manager=llm_manager\n)\n</code></pre>"},{"location":"usage/#api-usage","title":"API Usage","text":""},{"location":"usage/#starting-the-api-server","title":"Starting the API Server","text":"<pre><code># Start the API server\npython -m ragatanga.api.server --ontology path/to/ontology.ttl --port 8000\n</code></pre>"},{"location":"usage/#making-api-requests","title":"Making API Requests","text":"<pre><code>import requests\nimport json\n\n# Query endpoint\nresponse = requests.post(\n    \"http://localhost:8000/query\",\n    json={\"query\": \"What is Ragatanga?\"}\n)\n\n# Parse response\nresult = response.json()\nprint(json.dumps(result, indent=2))\n</code></pre>"},{"location":"usage/#environment-variables","title":"Environment Variables","text":"<p>Ragatanga can be configured using environment variables:</p> <pre><code># Set OpenAI API key\nexport OPENAI_API_KEY=\"your-api-key\"\n\n# Configure embedding provider\nexport RAGATANGA_EMBEDDING_PROVIDER=\"openai\"\nexport RAGATANGA_EMBEDDING_MODEL=\"text-embedding-ada-002\"\n\n# Configure LLM provider\nexport RAGATANGA_LLM_PROVIDER=\"openai\"\nexport RAGATANGA_LLM_MODEL=\"gpt-3.5-turbo\"\n\n# Configure retrieval parameters\nexport RAGATANGA_MAX_RESULTS=5\nexport RAGATANGA_CONFIDENCE_THRESHOLD=0.7\n</code></pre>"}]}