[
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/index.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/index.md \"View source of this page\")\n\n# Overview\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/",
      "ogUrl": "https://python.useinstructor.com/tutorials/",
      "title": "SEO-Friendly Guide to Content Optimization - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/index.png",
      "ogTitle": "SEO-Friendly Guide to Content Optimization - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/index.png",
      "og:title": "SEO-Friendly Guide to Content Optimization - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/",
      "statusCode": 200,
      "description": "Learn effective strategies for optimizing your content for search engines, enhancing visibility, and attracting organic traffic.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn effective strategies for optimizing your content for search engines, enhancing visibility, and attracting organic traffic.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/index.png",
      "twitter:title": "SEO-Friendly Guide to Content Optimization - Instructor",
      "og:description": "Learn effective strategies for optimizing your content for search engines, enhancing visibility, and attracting organic traffic.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn effective strategies for optimizing your content for search engines, enhancing visibility, and attracting organic traffic."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/3-1-validation-rag/#understanding-validators)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/3-1-validation-rag.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/3-1-validation-rag.ipynb \"View source of this page\")\n\n# Understanding Validators [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/\\#understanding-validators)\n\nPydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic [docs](https://docs.pydantic.dev/latest/) on validators.\n\nThen we'll bring it all together into the context of RAG from the previous notebook.\n\nValidators will enable us to control outputs by defining a function like so:\n\n```\ndef validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n\n```\n\nBefore we get started lets go over the general shape of a validator:\n\n## Defining Validator Functions [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/\\#defining-validator-functions)\n\nIn \\[18\\]:\n\nCopied!\n\n```\nfrom typing import Annotated\nfrom pydantic import BaseModel, AfterValidator, WithJsonSchema\n\ndef name_must_contain_space(v: str) -> str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v\n\ndef uppercase_name(v: str) -> str:\n    return v.upper()\n\nFullName = Annotated[\\\n    str,\\\n    AfterValidator(name_must_contain_space),\\\n    AfterValidator(uppercase_name),\\\n    WithJsonSchema(\\\n        {\\\n            \"type\": \"string\",\\\n            \"description\": \"The user's full name\",\\\n        }\\\n    )]\n\nclass UserDetail(BaseModel):\n    age: int\n    name: FullName\n\n```\n\nfrom typing import Annotated from pydantic import BaseModel, AfterValidator, WithJsonSchema def name\\_must\\_contain\\_space(v: str) -> str: if \" \" not in v: raise ValueError(\"Name must contain a space.\") return v def uppercase\\_name(v: str) -> str: return v.upper() FullName = Annotated\\[ str, AfterValidator(name\\_must\\_contain\\_space), AfterValidator(uppercase\\_name), WithJsonSchema( { \"type\": \"string\", \"description\": \"The user's full name\", } )\\] class UserDetail(BaseModel): age: int name: FullName\n\nIn \\[19\\]:\n\nCopied!\n\n```\nUserDetail(age=30, name=\"Jason Liu\")\n\n```\n\nUserDetail(age=30, name=\"Jason Liu\")\n\nOut\\[19\\]:\n\n```\nUserDetail(age=30, name='JASON LIU')\n```\n\nIn \\[20\\]:\n\nCopied!\n\n```\nUserDetail.model_json_schema()\n\n```\n\nUserDetail.model\\_json\\_schema()\n\nOut\\[20\\]:\n\n```\n{'properties': {'age': {'title': 'Age', 'type': 'integer'},\n  'name': {'description': \"The user's full name\",\n   'title': 'Name',\n   'type': 'string'}},\n 'required': ['age', 'name'],\n 'title': 'UserDetail',\n 'type': 'object'}\n```\n\nIn \\[21\\]:\n\nCopied!\n\n```\ntry:\n    person = UserDetail.model_validate({\"age\": 24, \"name\": \"Jason\"})\nexcept Exception as e:\n    print(e)\n\n```\n\ntry: person = UserDetail.model\\_validate({\"age\": 24, \"name\": \"Jason\"}) except Exception as e: print(e)\n\n```\n1 validation error for UserDetail\nname\n  Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n\n```\n\n## Using Field [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/\\#using-field)\n\nWe can also use the `Field` class to define validators. This is useful when we want to define a validator for a field that is primative, like a string or integer which supports a limited number of validators.\n\nIn \\[22\\]:\n\nCopied!\n\n```\nfrom pydantic import Field\n\nAge = Annotated[int, Field(gt=0)]\n\nclass UserDetail(BaseModel):\n    age: Age\n    name: FullName\n\ntry:\n    person = UserDetail(age=-10, name=\"Jason\")\nexcept Exception as e:\n    print(e)\n\n```\n\nfrom pydantic import Field Age = Annotated\\[int, Field(gt=0)\\] class UserDetail(BaseModel): age: Age name: FullName try: person = UserDetail(age=-10, name=\"Jason\") except Exception as e: print(e)\n\n```\n2 validation errors for UserDetail\nage\n  Input should be greater than 0 [type=greater_than, input_value=-10, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.5/v/greater_than\nname\n  Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n\n```\n\n## Providing Context [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/\\#providing-context)\n\nIn \\[7\\]:\n\nCopied!\n\n```\nfrom pydantic import ValidationInfo\n\ndef message_cannot_have_blacklisted_words(v: str, info: ValidationInfo) -> str:\n    blacklist = info.context.get(\"blacklist\", [])\n    for word in blacklist:\n        assert word not in v.lower(), f\"`{word}` was found in the message `{v}`\"\n    return v\n\nModeratedStr = Annotated[str, AfterValidator(message_cannot_have_blacklisted_words)]\n\nclass Response(BaseModel):\n    message: ModeratedStr\n\ntry:\n    Response.model_validate(\n        {\"message\": \"I will hurt them.\"},\n        context={\n            \"blacklist\": {\n                \"rob\",\n                \"steal\",\n                \"hurt\",\n                \"kill\",\n                \"attack\",\n            }\n        },\n    )\nexcept Exception as e:\n    print(e)\n\n```\n\nfrom pydantic import ValidationInfo def message\\_cannot\\_have\\_blacklisted\\_words(v: str, info: ValidationInfo) -> str: blacklist = info.context.get(\"blacklist\", \\[\\]) for word in blacklist: assert word not in v.lower(), f\"\\`{word}\\` was found in the message \\`{v}\\`\" return v ModeratedStr = Annotated\\[str, AfterValidator(message\\_cannot\\_have\\_blacklisted\\_words)\\] class Response(BaseModel): message: ModeratedStr try: Response.model\\_validate( {\"message\": \"I will hurt them.\"}, context={ \"blacklist\": { \"rob\", \"steal\", \"hurt\", \"kill\", \"attack\", } }, ) except Exception as e: print(e)\n\n```\n1 validation error for Response\nmessage\n  Assertion failed, `hurt` was found in the message `I will hurt them.` [type=assertion_error, input_value='I will hurt them.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/assertion_error\n\n```\n\n## Using OpenAI Moderation [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/\\#using-openai-moderation)\n\nTo enhance our validation measures, we'll extend the scope to flag any answer that contains hateful content, harassment, or similar issues. OpenAI offers a moderation endpoint that addresses these concerns, and it's freely available when using OpenAI models.\n\nWith the `instructor` library, this is just one function edit away:\n\nIn \\[13\\]:\n\nCopied!\n\n```\nfrom typing import Annotated\nfrom pydantic import AfterValidator\nfrom instructor import openai_moderation\n\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\n# This uses Annotated which is a new feature in Python 3.9\n# To define custom metadata for a type hint.\nModeratedStr = Annotated[str, AfterValidator(openai_moderation(client=client))]\n\nclass Response(BaseModel):\n    message: ModeratedStr\n\ntry:\n    Response(message=\"I want to make them suffer the consequences\")\nexcept Exception as e:\n    print(e)\n\n```\n\nfrom typing import Annotated from pydantic import AfterValidator from instructor import openai\\_moderation import instructor from openai import OpenAI client = instructor.patch(OpenAI()) # This uses Annotated which is a new feature in Python 3.9 # To define custom metadata for a type hint. ModeratedStr = Annotated\\[str, AfterValidator(openai\\_moderation(client=client))\\] class Response(BaseModel): message: ModeratedStr try: Response(message=\"I want to make them suffer the consequences\") except Exception as e: print(e)\n\n```\n1 validation error for Response\nmessage\n  Value error, `I want to make them suffer the consequences` was flagged for harassment, harassment_threatening, violence, harassment/threatening [type=value_error, input_value='I want to make them suffer the consequences', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n\n```\n\n## General Validator [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/\\#general-validator)\n\nIn \\[ \\]:\n\nCopied!\n\n```\nfrom instructor import llm_validator\n\nHealthTopicStr = Annotated[\\\n    str,\\\n    AfterValidator(\\\n        llm_validator(\\\n            \"don't talk about any other topic except health best practices and topics\",\\\n            client=client,\\\n        )\\\n    ),\\\n]\n\nclass AssistantMessage(BaseModel):\n    message: HealthTopicStr\n\nAssistantMessage(\n    message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\"\n)\n\n```\n\nfrom instructor import llm\\_validator HealthTopicStr = Annotated\\[ str, AfterValidator( llm\\_validator( \"don't talk about any other topic except health best practices and topics\", client=client, ) ), \\] class AssistantMessage(BaseModel): message: HealthTopicStr AssistantMessage( message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\" )\n\n### Avoiding hallucination with citations [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/\\#avoiding-hallucination-with-citations)\n\nWhen incorporating external knowledge bases, it's crucial to ensure that the agent uses the provided context accurately and doesn't fabricate responses. Validators can be effectively used for this purpose. We can illustrate this with an example where we validate that a provided citation is actually included in the referenced text chunk:\n\nIn \\[27\\]:\n\nCopied!\n\n```\nfrom pydantic import ValidationInfo\n\ndef citation_exists(v: str, info: ValidationInfo):\n    context = info.context\n    if context:\n        context = context.get(\"text_chunk\")\n        if v not in context:\n            raise ValueError(f\"Citation `{v}` not found in text, only use citations from the text.\")\n    return v\n\nCitation = Annotated[str, AfterValidator(citation_exists)]\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: Citation\n\ntry:\n    AnswerWithCitation.model_validate(\n        {\n            \"answer\": \"Blueberries are packed with protein\",\n            \"citation\": \"Blueberries contain high levels of protein\",\n        },\n        context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"},\n    )\nexcept Exception as e:\n    print(e)\n\n```\n\nfrom pydantic import ValidationInfo def citation\\_exists(v: str, info: ValidationInfo): context = info.context if context: context = context.get(\"text\\_chunk\") if v not in context: raise ValueError(f\"Citation \\`{v}\\` not found in text, only use citations from the text.\") return v Citation = Annotated\\[str, AfterValidator(citation\\_exists)\\] class AnswerWithCitation(BaseModel): answer: str citation: Citation try: AnswerWithCitation.model\\_validate( { \"answer\": \"Blueberries are packed with protein\", \"citation\": \"Blueberries contain high levels of protein\", }, context={\"text\\_chunk\": \"Blueberries are very rich in antioxidants\"}, ) except Exception as e: print(e)\n\n```\n1 validation error for AnswerWithCitation\ncitation\n  Value error, Citation `Blueberries contain high levels of protein` not found in text, only use citations from the text. [type=value_error, input_value='Blueberries contain high levels of protein', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n\n```\n\nHere we assume that there is a \"text\\_chunk\" field that contains the text that the model is supposed to use as context. We then use the `field_validator` decorator to define a validator that checks if the citation is included in the text chunk. If it's not, we raise a `ValueError` with a message that will be returned to the user.\n\nIf we want to pass in the context through the ```chat.completions.create`` endpoint, we can use the ``` validation\\_context\\` parameter\n\n```\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=AnswerWithCitation,\n    messages=[\\\n        {\"role\": \"user\", \"content\": f\"Answer the question `{q}` using the text chunk\\n`{text_chunk}`\"},\\\n    ],\n    validation_context={\"text_chunk\": text_chunk},\n)\n\n```\n\nIn practice there are many ways to implement this: we could use a regex to check if the citation is included in the text chunk, or we could use a more sophisticated approach like a semantic similarity check. The important thing is that we have a way to validate that the model is using the provided context accurately.\n\n## Reasking with validators [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/\\#reasking-with-validators)\n\nFor most of these examples all we've done we've mostly only defined the validation logic. Which can be seperate from generation, however when we are given validation errors, we shouldn't end there! Instead instructor allows us to collect all the validation errors and reask the llm to rewrite their answer.\n\nLets try to use a extreme example to illustrate this point:\n\nIn \\[15\\]:\n\nCopied!\n\n```\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n\nquestion = \"What is the meaning of life?\"\ncontext = (\n    \"The according to the devil the meaning of life is a life of sin and debauchery.\"\n)\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\\\n        },\\\n    ],\n)\n\nprint(resp.model_dump_json(indent=2))\n\n```\n\nclass QuestionAnswer(BaseModel): question: str answer: str question = \"What is the meaning of life?\" context = ( \"The according to the devil the meaning of life is a life of sin and debauchery.\" ) resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\_model=QuestionAnswer, messages=\\[ { \"role\": \"system\", \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\", }, { \"role\": \"user\", \"content\": f\"using the context: \\`{context}\\`\\\\n\\\\nAnswer the following question: \\`{question}\\`\", }, \\], ) print(resp.model\\_dump\\_json(indent=2))\n\n```\n{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"According to the devil, the meaning of life is a life of sin and debauchery.\"\n}\n\n```\n\nIn \\[20\\]:\n\nCopied!\n\n```\nfrom instructor import llm_validator\n\nNotEvilAnswer = Annotated[\\\n    str,\\\n    AfterValidator(\\\n        llm_validator(\"don't say objectionable things\", client=client)\\\n    ),\\\n]\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: NotEvilAnswer\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    max_retries=2,\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\\\n        },\\\n    ],\n)\n\n```\n\nfrom instructor import llm\\_validator NotEvilAnswer = Annotated\\[ str, AfterValidator( llm\\_validator(\"don't say objectionable things\", client=client) ), \\] class QuestionAnswer(BaseModel): question: str answer: NotEvilAnswer resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\_model=QuestionAnswer, max\\_retries=2, messages=\\[ { \"role\": \"system\", \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\", }, { \"role\": \"user\", \"content\": f\"using the context: \\`{context}\\`\\\\n\\\\nAnswer the following question: \\`{question}\\`\", }, \\], )\n\n```\nRetrying, exception: 1 validation error for QuestionAnswer\nanswer\n  Assertion failed, The statement promotes sin and debauchery, which can be considered objectionable. [type=assertion_error, input_value='The meaning of life, acc... of sin and debauchery.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/assertion_error\nTraceback (most recent call last):\n  File \"/Users/jasonliu/dev/instructor/instructor/patch.py\", line 277, in retry_sync\n    return process_response(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/jasonliu/dev/instructor/instructor/patch.py\", line 164, in process_response\n    model = response_model.from_response(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jasonliu/dev/instructor/instructor/function_calls.py\", line 137, in from_response\n    return cls.model_validate_json(\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jasonliu/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py\", line 532, in model_validate_json\n    return cls.__pydantic_validator__.validate_json(json_data, strict=strict, context=context)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for QuestionAnswer\nanswer\n  Assertion failed, The statement promotes sin and debauchery, which can be considered objectionable. [type=assertion_error, input_value='The meaning of life, acc... of sin and debauchery.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/assertion_error\n\n```\n\nIn \\[21\\]:\n\nCopied!\n\n```\nprint(resp.model_dump_json(indent=2))\n\n```\n\nprint(resp.model\\_dump\\_json(indent=2))\n\n```\n{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life is subjective and can vary depending on one's beliefs and perspectives. According to the devil, it is a life of sin and debauchery. However, this viewpoint may not be universally accepted and should be evaluated critically.\"\n}\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/3-1-validation-rag/",
      "ogUrl": "https://python.useinstructor.com/tutorials/3-1-validation-rag/",
      "title": "Applications RAG - 2 - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/3-1-validation-rag/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/3-1-validation-rag.png",
      "ogTitle": "Applications RAG - 2 - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/3-1-validation-rag.png",
      "og:title": "Applications RAG - 2 - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/3-1-validation-rag/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/3-1-validation-rag.png",
      "twitter:title": "Applications RAG - 2 - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/1-introduction/#working-with-structured-outputs)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/1-introduction.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/1-introduction.ipynb \"View source of this page\")\n\n# Working with structured outputs [¶](https://python.useinstructor.com/tutorials/1-introduction/\\#working-with-structured-outputs)\n\nIf you've seen my [talk](https://www.youtube.com/watch?v=yj-wSRJwrrc&t=1s) on this topic, you can skip this chapter.\n\ntl;dr\n\nWhen we work with LLMs you find that many times we are not building chatbots, instead we're working with structured outputs in order to solve a problem by returning machine readable data. However the way we think about the problem is still very much influenced by the way we think about chatbots. This is a problem because it leads to a lot of confusion and frustration. In this chapter we'll try to understand why this happens and how we can fix it.\n\nIn \\[1\\]:\n\nCopied!\n\n```\nimport traceback\n\n```\n\nimport traceback\n\nIn \\[2\\]:\n\nCopied!\n\n```\nRED = \"\\033[91m\"\\\nRESET = \"\\033[0m\"\\\n\\\n```\\\n\\\nRED = \"\\\\033\\[91m\" RESET = \"\\\\033\\[0m\"\\\n\\\n## The fundamental problem with JSON and Dictionaries [¶](https://python.useinstructor.com/tutorials/1-introduction/\\#the-fundamental-problem-with-json-and-dictionaries)\\\n\\\nLets say we have a simple JSON object, and we want to work with it. We can use the `json` module to load it into a dictionary, and then work with it. However, this is a bit of a pain, because we have to manually check the types of the data, and we have to manually check if the data is valid. For example, lets say we have a JSON object that looks like this:\\\n\\\nIn \\[3\\]:\\\n\\\nCopied!\\\n\\\n```\\\ndata = [{\"first_name\": \"Jason\", \"age\": 10}, {\"firstName\": \"Jason\", \"age\": \"10\"}]\\\n\\\n```\\\n\\\ndata = \\[{\"first\\_name\": \"Jason\", \"age\": 10}, {\"firstName\": \"Jason\", \"age\": \"10\"}\\]\\\n\\\nWe have a `name` field, which is a string, and an `age` field, which is an integer. However, if we were to load this into a dictionary, we would have no way of knowing if the data is valid. For example, we could have a string for the age, or we could have a float for the age. We could also have a string for the name, or we could have a list for the name.\\\n\\\nIn \\[4\\]:\\\n\\\nCopied!\\\n\\\n```\\\nfor obj in data:\\\n    name = obj.get(\"first_name\")\\\n    age = obj.get(\"age\")\\\n    print(f\"{name} is {age}\")\\\n\\\nfor obj in data:\\\n    name = obj.get(\"first_name\")\\\n    age = obj.get(\"age\")\\\n    try:\\\n        age_next_year = age + 1\\\n        print(f\"Next year {name} will be {age_next_year} years old\")\\\n    except TypeError:\\\n        traceback.print_exc()\\\n\\\n```\\\n\\\nfor obj in data: name = obj.get(\"first\\_name\") age = obj.get(\"age\") print(f\"{name} is {age}\") for obj in data: name = obj.get(\"first\\_name\") age = obj.get(\"age\") try: age\\_next\\_year = age + 1 print(f\"Next year {name} will be {age\\_next\\_year} years old\") except TypeError: traceback.print\\_exc()\\\n\\\n```\\\nJason is 10\\\nNone is 10\\\nNext year Jason will be 11 years old\\\n\\\n```\\\n\\\n```\\\nTraceback (most recent call last):\\\n  File \"/var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_24047/2607506000.py\", line 10, in <module>\\\n    age_next_year = age + 1\\\n                    ~~~~^~~\\\nTypeError: can only concatenate str (not \"int\") to str\\\n\\\n```\\\n\\\nYou see that while we were able to program with a dictionary, we had issues with the data being valid. We would have had to manually check the types of the data, and we had to manually check if the data was valid. This is a pain, and we can do better.\\\n\\\n## Pydantic to the rescue [¶](https://python.useinstructor.com/tutorials/1-introduction/\\#pydantic-to-the-rescue)\\\n\\\nPydantic is a library that allows us to define data structures, and then validate them.\\\n\\\nIn \\[5\\]:\\\n\\\nCopied!\\\n\\\n```\\\nfrom pydantic import BaseModel, Field, ValidationError\\\n\\\nclass Person(BaseModel):\\\n    name: str\\\n    age: int\\\n\\\nperson = Person(name=\"Sam\", age=30)\\\nperson\\\n\\\n```\\\n\\\nfrom pydantic import BaseModel, Field, ValidationError class Person(BaseModel): name: str age: int person = Person(name=\"Sam\", age=30) person\\\n\\\nOut\\[5\\]:\\\n\\\n```\\\nPerson(name='Sam', age=30)\\\n```\\\n\\\nIn \\[6\\]:\\\n\\\nCopied!\\\n\\\n```\\\n# Data is correctly casted to the right type\\\nperson = Person.model_validate({\"name\": \"Sam\", \"age\": \"30\"})\\\nperson\\\n\\\n```\\\n\\\n\\# Data is correctly casted to the right type person = Person.model\\_validate({\"name\": \"Sam\", \"age\": \"30\"}) person\\\n\\\nOut\\[6\\]:\\\n\\\n```\\\nPerson(name='Sam', age=30)\\\n```\\\n\\\nIn \\[7\\]:\\\n\\\nCopied!\\\n\\\n```\\\nassert person.name == \"Sam\"\\\nassert person.age == 30\\\n\\\ntry:\\\n    assert person.age == 20\\\nexcept AssertionError:\\\n    traceback.print_exc()\\\n\\\n```\\\n\\\nassert person.name == \"Sam\" assert person.age == 30 try: assert person.age == 20 except AssertionError: traceback.print\\_exc()\\\n\\\n```\\\nTraceback (most recent call last):\\\n  File \"/var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_24047/3040264600.py\", line 5, in <module>\\\n    assert person.age == 20\\\n           ^^^^^^^^^^^^^^^^\\\nAssertionError\\\n\\\n```\\\n\\\nIn \\[8\\]:\\\n\\\nCopied!\\\n\\\n```\\\n# Data is validated to get better error messages\\\ntry:\\\n    person = Person.model_validate({\"first_name\": \"Sam\", \"age\": \"30.2\"})\\\nexcept ValidationError as e:\\\n    print(\"Validation Error:\")\\\n    for error in e.errors():\\\n        print(f\"Field: {error['loc'][0]}, Error: {error['msg']}\")\\\n\\\n    print(f\"{RED}\\nOriginal Traceback Below{RESET}\")\\\n    traceback.print_exc()\\\n\\\n```\\\n\\\n\\# Data is validated to get better error messages try: person = Person.model\\_validate({\"first\\_name\": \"Sam\", \"age\": \"30.2\"}) except ValidationError as e: print(\"Validation Error:\") for error in e.errors(): print(f\"Field: {error\\['loc'\\]\\[0\\]}, Error: {error\\['msg'\\]}\") print(f\"{RED}\\\\nOriginal Traceback Below{RESET}\") traceback.print\\_exc()\\\n\\\n```\\\nValidation Error:\\\nField: name, Error: Field required\\\nField: age, Error: Input should be a valid integer, unable to parse string as an integer\\\n\\\nOriginal Traceback Below\\\n\\\n```\\\n\\\n```\\\nTraceback (most recent call last):\\\n  File \"/var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_24047/621989455.py\", line 3, in <module>\\\n    person = Person.model_validate({\"first_name\": \"Sam\", \"age\": \"30.2\"})\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/instructor/lib/python3.11/site-packages/pydantic/main.py\", line 509, in model_validate\\\n    return cls.__pydantic_validator__.validate_python(\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\npydantic_core._pydantic_core.ValidationError: 2 validation errors for Person\\\nname\\\n  Field required [type=missing, input_value={'first_name': 'Sam', 'age': '30.2'}, input_type=dict]\\\n    For further information visit https://errors.pydantic.dev/2.6/v/missing\\\nage\\\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='30.2', input_type=str]\\\n    For further information visit https://errors.pydantic.dev/2.6/v/int_parsing\\\n\\\n```\\\n\\\nBy introducing pydantic into any python codebase you can get a lot of benefits. You can get type checking, you can get validation, and you can get autocomplete. This is a huge win, because it means you can catch errors before they happen. This is even more useful when we rely on language models to generate data for us.\\\n\\\nYou can also define validators that are run on the data. This is useful because it means you can catch errors before they happen. For example, you can define a validator that checks if the age is greater than 0. This is useful because it means you can catch errors before they happen.\\\n\\\n## Fundamental problem with asking for JSON from OpenAI [¶](https://python.useinstructor.com/tutorials/1-introduction/\\#fundamental-problem-with-asking-for-json-from-openai)\\\n\\\nAs we shall see below, the correct json format would be something of the format below:\\\n\\\n```\\\n{\\\n    \"name\": \"Jason\",\\\n    \"age\": 10\\\n}\\\n\\\n```\\\n\\\nHowever, we get errorenous outputs like:\\\n\\\n```\\\n{\\\n  \"jason\": 10\\\n}\\\n\\\n```\\\n\\\nIn \\[9\\]:\\\n\\\nCopied!\\\n\\\n````\\\nfrom openai import OpenAI\\\n\\\nclient = OpenAI()\\\n\\\nresp = client.chat.completions.create(\\\n    model=\"gpt-3.5-turbo\",\\\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Please give me jason is 10 as a json object ```json\\n\"},\\\n    ],\\\n    n=10,\\\n    temperature=1,\\\n)\\\n\\\nprint(\"json that we want:\")\\\nprint(\"\"\"\\\n{\\\n    \"name\": \"Jason\",\\\n    \"age\": 10\\\n}\\\n\"\"\")\\\n\\\nfor choice in resp.choices:\\\n    json = choice.message.content\\\n    try:\\\n        person = Person.model_validate_json(json)\\\n        print(f\"correctly parsed {person=}\")\\\n    except Exception as e:\\\n        print(\"error!!\")\\\n        print(json)\\\n\\\n````\\\n\\\nfrom openai import OpenAI client = OpenAI() resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\[ {\"role\": \"user\", \"content\": \"Please give me jason is 10 as a json object \\`\\`\\`json\\\\n\"}, \\], n=10, temperature=1, ) print(\"json that we want:\") print(\"\"\" { \"name\": \"Jason\", \"age\": 10 } \"\"\") for choice in resp.choices: json = choice.message.content try: person = Person.model\\_validate\\_json(json) print(f\"correctly parsed {person=}\") except Exception as e: print(\"error!!\") print(json)\\\n\\\n```\\\njson that we want:\\\n\\\n{\\\n    \"name\": \"Jason\",\\\n    \"age\": 10\\\n}\\\n\\\nerror!!\\\n{\\\n  \"jason\": 10\\\n}\\\ncorrectly parsed person=Person(name='Jason', age=10)\\\ncorrectly parsed person=Person(name='jason', age=10)\\\nerror!!\\\n{\\\n  \"Jason\": {\\\n    \"age\": 10\\\n  }\\\n}\\\nerror!!\\\n{\\\n  \"Jason\": {\\\n    \"age\": 10\\\n  }\\\n}\\\nerror!!\\\n{\\\n  \"Jason\": {\\\n    \"age\": 10\\\n  }\\\n}\\\nerror!!\\\n{\\\n  \"Jason\": {\\\n    \"age\": 10\\\n  }\\\n}\\\ncorrectly parsed person=Person(name='Jason', age=10)\\\ncorrectly parsed person=Person(name='Jason', age=10)\\\nerror!!\\\n{\\\n  \"jason\": 10\\\n}\\\n\\\n```\\\n\\\n## Introduction to Function Calling [¶](https://python.useinstructor.com/tutorials/1-introduction/\\#introduction-to-function-calling)\\\n\\\nThe json could be anything! We could add more and more into a prompt and hope it works, or we can use something called [function calling](https://platform.openai.com/docs/guides/function-calling) to directly specify the schema we want.\\\n\\\n**Function Calling**\\\n\\\nIn an API call, you can describe _functions_ and have the model intelligently choose to output a _JSON object_ containing _arguments_ to call one or many functions. The Chat Completions API does **not** call the function; instead, the model generates _JSON_ that you can use to call the function in **your code**.\\\n\\\nIn \\[10\\]:\\\n\\\nCopied!\\\n\\\n```\\\nimport datetime\\\n\\\nclass PersonBirthday(BaseModel):\\\n    name: str\\\n    age: int\\\n    birthday: datetime.date\\\n\\\nschema = {\\\n    \"properties\": {\\\n        \"name\": {\"type\": \"string\"},\\\n        \"age\": {\"type\": \"integer\"},\\\n        \"birthday\": {\"type\": \"string\", \"format\": \"YYYY-MM-DD\"},\\\n    },\\\n    \"required\": [\"name\", \"age\"],\\\n    \"type\": \"object\",\\\n}\\\n\\\nresp = client.chat.completions.create(\\\n    model=\"gpt-3.5-turbo\",\\\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": f\"Extract `Jason Liu is thirty years old his birthday is yesturday` into json today is {datetime.date.today()}\",\\\n        },\\\n    ],\\\n    functions=[{\"name\": \"Person\", \"parameters\": schema}],\\\n    function_call=\"auto\",\\\n)\\\n\\\nPersonBirthday.model_validate_json(resp.choices[0].message.function_call.arguments)\\\n\\\n```\\\n\\\nimport datetime class PersonBirthday(BaseModel): name: str age: int birthday: datetime.date schema = { \"properties\": { \"name\": {\"type\": \"string\"}, \"age\": {\"type\": \"integer\"}, \"birthday\": {\"type\": \"string\", \"format\": \"YYYY-MM-DD\"}, }, \"required\": \\[\"name\", \"age\"\\], \"type\": \"object\", } resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\[ { \"role\": \"user\", \"content\": f\"Extract \\`Jason Liu is thirty years old his birthday is yesturday\\` into json today is {datetime.date.today()}\", }, \\], functions=\\[{\"name\": \"Person\", \"parameters\": schema}\\], function\\_call=\"auto\", ) PersonBirthday.model\\_validate\\_json(resp.choices\\[0\\].message.function\\_call.arguments)\\\n\\\nOut\\[10\\]:\\\n\\\n```\\\nPersonBirthday(name='Jason Liu', age=30, birthday=datetime.date(1994, 3, 26))\\\n```\\\n\\\nBut it turns out, pydantic actually not only does our serialization, we can define the schema as well as add additional documentation!\\\n\\\nIn \\[11\\]:\\\n\\\nCopied!\\\n\\\n```\\\nPersonBirthday.model_json_schema()\\\n\\\n```\\\n\\\nPersonBirthday.model\\_json\\_schema()\\\n\\\nOut\\[11\\]:\\\n\\\n```\\\n{'properties': {'name': {'title': 'Name', 'type': 'string'},\\\n  'age': {'title': 'Age', 'type': 'integer'},\\\n  'birthday': {'format': 'date', 'title': 'Birthday', 'type': 'string'}},\\\n 'required': ['name', 'age', 'birthday'],\\\n 'title': 'PersonBirthday',\\\n 'type': 'object'}\\\n```\\\n\\\nWe can even define nested complex schemas, and documentation with ease.\\\n\\\nIn \\[12\\]:\\\n\\\nCopied!\\\n\\\n```\\\nclass Address(BaseModel):\\\n    address: str = Field(description=\"Full street address\")\\\n    city: str\\\n    state: str\\\n\\\nclass PersonAddress(Person):\\\n    \"\"\"A Person with an address\"\"\"\\\n\\\n    address: Address\\\n\\\nPersonAddress.model_json_schema()\\\n\\\n```\\\n\\\nclass Address(BaseModel): address: str = Field(description=\"Full street address\") city: str state: str class PersonAddress(Person): \"\"\"A Person with an address\"\"\" address: Address PersonAddress.model\\_json\\_schema()\\\n\\\nOut\\[12\\]:\\\n\\\n```\\\n{'$defs': {'Address': {'properties': {'address': {'description': 'Full street address',\\\n     'title': 'Address',\\\n     'type': 'string'},\\\n    'city': {'title': 'City', 'type': 'string'},\\\n    'state': {'title': 'State', 'type': 'string'}},\\\n   'required': ['address', 'city', 'state'],\\\n   'title': 'Address',\\\n   'type': 'object'}},\\\n 'description': 'A Person with an address',\\\n 'properties': {'name': {'title': 'Name', 'type': 'string'},\\\n  'age': {'title': 'Age', 'type': 'integer'},\\\n  'address': {'$ref': '#/$defs/Address'}},\\\n 'required': ['name', 'age', 'address'],\\\n 'title': 'PersonAddress',\\\n 'type': 'object'}\\\n```\\\n\\\nThese simple concepts become what we built into `instructor` and most of the work has been around documenting how we can leverage schema engineering. Except now we use `instructor.patch()` to add a bunch more capabilities to the OpenAI SDK.\\\n\\\n# The core idea around Instructor [¶](https://python.useinstructor.com/tutorials/1-introduction/\\#the-core-idea-around-instructor)\\\n\\\n1. Using function calling allows us use a llm that is finetuned to use json\\_schema and output json.\\\n2. Pydantic can be used to define the object, schema, and validation in one single class, allow us to encapsulate everything neatly\\\n3. As a library with 100M downloads, we can leverage pydantic to do all the heavy lifting for us and fit nicely with the python ecosystem\\\n\\\nIn \\[13\\]:\\\n\\\nCopied!\\\n\\\n```\\\nimport instructor\\\nimport datetime\\\n\\\n# patch the client to add `response_model` to the `create` method\\\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\\\n\\\nresp = client.chat.completions.create(\\\n    model=\"gpt-3.5-turbo-1106\",\\\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": f\"\"\"\\\n            Today is {datetime.date.today()}\\\n\\\n            Extract `Jason Liu is thirty years old his birthday is yesturday`\\\n            he lives at 123 Main St, San Francisco, CA\"\"\",\\\n        },\\\n    ],\\\n    response_model=PersonAddress,\\\n)\\\nresp\\\n\\\n```\\\n\\\nimport instructor import datetime # patch the client to add \\`response\\_model\\` to the \\`create\\` method client = instructor.patch(OpenAI(), mode=instructor.Mode.MD\\_JSON) resp = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", messages=\\[ { \"role\": \"user\", \"content\": f\"\"\" Today is {datetime.date.today()} Extract \\`Jason Liu is thirty years old his birthday is yesturday\\` he lives at 123 Main St, San Francisco, CA\"\"\", }, \\], response\\_model=PersonAddress, ) resp\\\n\\\nOut\\[13\\]:\\\n\\\n```\\\nPersonAddress(name='Jason Liu', age=30, address=Address(address='123 Main St', city='San Francisco', state='CA'))\\\n```\\\n\\\nBy defining `response_model` we can leverage pydantic to do all the heavy lifting. Later we'll introduce the other features that `instructor.patch()` adds to the OpenAI SDK. but for now, this small change allows us to do a lot more with the API.\\\n\\\n## Is instructor the only way to do this? [¶](https://python.useinstructor.com/tutorials/1-introduction/\\#is-instructor-the-only-way-to-do-this)\\\n\\\nNo. Libraries like Marvin, Langchain, and Llamaindex all now leverage the Pydantic object in similar ways. The goal is to be as light weight as possible, get you as close as possible to the openai api, and then get out of your way.\\\n\\\nMore importantly, we've also added straight forward validation and reasking to the mix.\\\n\\\nThe goal of instructor is to show you how to think about structured prompting and provide examples and documentation that you can take with you to any framework.\\\n\\\nFor further exploration:\\\n\\\n- [Marvin](https://www.askmarvin.ai/)\\\n- [Langchain](https://python.langchain.com/docs/modules/model_io/output_parsers/pydantic)\\\n- [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/examples/output_parsing/openai_pydantic_program.html)\\\n\\\nWas this page helpful?\\\n\\\nThanks for your feedback!\\\n\\\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\\\n\\\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/1-introduction/",
      "ogUrl": "https://python.useinstructor.com/tutorials/1-introduction/",
      "title": "Tutorials (Notebooks) - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/1-introduction/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/1-introduction.png",
      "ogTitle": "Tutorials (Notebooks) - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/1-introduction.png",
      "og:title": "Tutorials (Notebooks) - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/1-introduction/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/1-introduction.png",
      "twitter:title": "Tutorials (Notebooks) - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/5-knowledge-graphs/#knowledge-graphs-for-complex-topics)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/5-knowledge-graphs.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/5-knowledge-graphs.ipynb \"View source of this page\")\n\n# Knowledge Graphs for Complex Topics [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/\\#knowledge-graphs-for-complex-topics)\n\n# Introduction [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/\\#introduction)\n\n**What is a knowledge graph?**\n\nA knowledge graph, also known as a semantic network, represents real-world entities and their relationships. It consists of nodes, edges, and labels. Nodes can represent any entity, while edges define the connections between them. For example, a node representing an author like \"J.K. Rowling\" can be connected to another node representing one of her books, \"Harry Potter\", with the edge \"author of\".\n\n**Applications of knowledge graphs**\n\nKnowledge graphs have various applications, including:\n\n- Search Engines: They enhance search results by incorporating semantic-search information from diverse sources.\n- Recommendation Systems: They suggest products or services based on user behavior and preferences.\n- Natural Language Processing: They aid in understanding and generating human language.\n- Data Integration: They facilitate the integration of data from different sources by identifying relationships.\n- Artificial Intelligence and Machine Learning: They provide contextual information to improve decision-making.\n\n* * *\n\n## Setup and Dependencies [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/\\#setup-and-dependencies)\n\nToday, we're going to use the [`instructor`](https://github.com/jxnl/instructor) library to simplify the interaction between OpenAI and our code. Along with [Graphviz](https://graphviz.org) library to bring structure to our intricate subjects and have a graph visualization.\n\nIn \\[2\\]:\n\nCopied!\n\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\n```\n\nimport instructor from openai import OpenAI client = instructor.patch(OpenAI())\n\nInstall the Graphviz based on your operation system [https://graphviz.org/download/](https://graphviz.org/download/)\n\n## Node and Edge Classes [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/\\#node-and-edge-classes)\n\nWe begin by modeling our knowledge graph with Node and Edge objects.\n\nNode objects represent key concepts or entities, while Edge objects signify the relationships between them.\n\nIn \\[3\\]:\n\nCopied!\n\n```\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\n```\n\nfrom pydantic import BaseModel, Field from typing import Optional class Node(BaseModel): id: int label: str color: str class Edge(BaseModel): source: int target: int label: str color: str = \"black\"\n\n## `KnowledgeGraph` Class [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/\\#knowledgegraph-class)\n\nThe `KnowledgeGraph` class combines nodes and edges to create a comprehensive graph structure. It includes lists of nodes and edges, where each node represents a key concept or entity, and each edge represents a relationship between two nodes.\n\nLater on, you'll see that we designed this class to match the graph object in the graphviz library, which makes it easier to visualize our graph.\n\nThe `visualize_knowledge_graph` function is used to visualize a knowledge graph. It takes a `KnowledgeGraph` object as input, which contains nodes and edges. The function utilizes the `graphviz` library to generate a directed graph ( `Digraph`). Each node and edge from the `KnowledgeGraph` is added to the `Digraph` with their respective attributes (id, label, color). Finally, the graph is rendered and displayed.\n\nIn \\[4\\]:\n\nCopied!\n\n```\nfrom graphviz import Digraph\nfrom IPython.display import display\n\nclass KnowledgeGraph(BaseModel):\n    nodes: list[Node] = Field(..., default_factory=list)  # A list of nodes in the knowledge graph.\n    edges: list[Edge] = Field(..., default_factory=list)  # A list of edges in the knowledge graph.\n\n    def visualize_knowledge_graph(self):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:\n            dot.node(name=str(node.id), label=node.label, color=node.color)\n        for edge in self.edges:\n            dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n\n        return display(dot)\n\n```\n\nfrom graphviz import Digraph from IPython.display import display class KnowledgeGraph(BaseModel): nodes: list\\[Node\\] = Field(..., default\\_factory=list) # A list of nodes in the knowledge graph. edges: list\\[Edge\\] = Field(..., default\\_factory=list) # A list of edges in the knowledge graph. def visualize\\_knowledge\\_graph(self): dot = Digraph(comment=\"Knowledge Graph\") for node in self.nodes: dot.node(name=str(node.id), label=node.label, color=node.color) for edge in self.edges: dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color) return display(dot)\n\n## Generating the Knowledge Graph [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/\\#generating-the-knowledge-graph)\n\n### generate\\_graph function [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/\\#generate_graph-function)\n\nThe `generate_graph` function uses OpenAI's model to create a KnowledgeGraph object from an input string.\n\nIt requests the model to interpret the input as a detailed knowledge graph and uses the response to form the KnowledgeGraph object.\n\nIn \\[8\\]:\n\nCopied!\n\n```\ndef generate_graph(input) -> KnowledgeGraph:\n    return client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"Help me understand the following by describing it as small knowledge graph: {input}\",\\\n            }\\\n        ],\n        response_model=KnowledgeGraph,\n    )\n\n```\n\ndef generate\\_graph(input) -> KnowledgeGraph: return client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[ { \"role\": \"user\", \"content\": f\"Help me understand the following by describing it as small knowledge graph: {input}\", } \\], response\\_model=KnowledgeGraph, )\n\nIn \\[9\\]:\n\nCopied!\n\n```\ngenerate_graph(\"Explain quantum mechanics\").visualize_knowledge_graph()\n\n```\n\ngenerate\\_graph(\"Explain quantum mechanics\").visualize\\_knowledge\\_graph()\n\n![No description has been provided for this image](<Base64-Image-Removed>)\n\n## Advanced: Accumulating Knowledge Graphs [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/\\#advanced-accumulating-knowledge-graphs)\n\nWhen dealing with larger datasets, or knowledge that grows over time, processing them all at once can be challenging due to limitations in prompt length or the complexity of the content. In such cases, an iterative approach to building the knowledge graph can be beneficial. This method involves processing the text in smaller, manageable chunks and updating the graph with new information from each chunk.\n\n### What are the benefits of this approach? [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/\\#what-are-the-benefits-of-this-approach)\n\n- Scalability: This approach can handle large datasets by breaking them down into smaller, more manageable pieces.\n\n- Flexibility: It allows for dynamic updates to the graph, accommodating new information as it becomes available.\n\n- Efficiency: Processing smaller chunks of text can be more efficient and less prone to errors or omissions.\n\n\n### What has changed? [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/\\#what-has-changed)\n\nThe previous example provided a basic structure, while this new example introduces additional complexity and functionality. The Node and Edge classes now have a **hash** method, allowing them to be used in sets and simplifying duplicate handling.\n\nThe KnowledgeGraph class has been enhanced with two new methods: `update` and `draw`.\n\nIn the KnowledgeGraph class, the nodes and edges fields are now optional, offering greater flexibility.\n\nThe `update` method enables the merging and removal of duplicates from two graphs.\n\nThe `draw` method includes a prefix parameter, making it easier to create different graph versions during iterations.\n\nIn \\[10\\]:\n\nCopied!\n\n```\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\n    def __hash__(self) -> int:\n        return hash((id, self.label))\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\n    def __hash__(self) -> int:\n        return hash((self.source, self.target, self.label))\n\n```\n\nclass Node(BaseModel): id: int label: str color: str def \\_\\_hash\\_\\_(self) -> int: return hash((id, self.label)) class Edge(BaseModel): source: int target: int label: str color: str = \"black\" def \\_\\_hash\\_\\_(self) -> int: return hash((self.source, self.target, self.label))\n\nIn \\[11\\]:\n\nCopied!\n\n```\nclass KnowledgeGraph(BaseModel):\n    # Optional list of nodes and edges in the knowledge graph\n    nodes: Optional[list[Node]] = Field(..., default_factory=list)\n    edges: Optional[list[Edge]] = Field(..., default_factory=list)\n\n    def update(self, other: \"KnowledgeGraph\") -> \"KnowledgeGraph\":\n        # This method updates the current graph with the other graph, deduplicating nodes and edges.\n        return KnowledgeGraph(\n            nodes=list(set(self.nodes + other.nodes)),  # Combine and deduplicate nodes\n            edges=list(set(self.edges + other.edges)),  # Combine and deduplicate edges\n        )\n\n\n    def visualize_knowledge_graph(self):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:\n            dot.node(str(node.id), node.label, color=node.color)\n        for edge in self.edges:\n            dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n\n        return display(dot)\n\n```\n\nclass KnowledgeGraph(BaseModel): # Optional list of nodes and edges in the knowledge graph nodes: Optional\\[list\\[Node\\]\\] = Field(..., default\\_factory=list) edges: Optional\\[list\\[Edge\\]\\] = Field(..., default\\_factory=list) def update(self, other: \"KnowledgeGraph\") -> \"KnowledgeGraph\": # This method updates the current graph with the other graph, deduplicating nodes and edges. return KnowledgeGraph( nodes=list(set(self.nodes + other.nodes)), # Combine and deduplicate nodes edges=list(set(self.edges + other.edges)), # Combine and deduplicate edges ) def visualize\\_knowledge\\_graph(self): dot = Digraph(comment=\"Knowledge Graph\") for node in self.nodes: dot.node(str(node.id), node.label, color=node.color) for edge in self.edges: dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color) return display(dot)\n\n### Generate iterative graphs [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/\\#generate-iterative-graphs)\n\nThe updated `generate_graph` function is specifically designed to handle a list of inputs iteratively. It updates the graph with each new piece of information.\n\nUpon closer inspection, this pattern resembles a common programming technique known as a \"reduce\" or \"fold\" function. A simple example of this would be iterating over a list to find the sum of all the elements squared.\n\nHere's an example in Python:\n\n```\ncur_state = 0\nfor i in [1, 2, 3, 4, 5]:\n    cur_state += i**2\nprint(cur_state)\n\n```\n\nIn \\[12\\]:\n\nCopied!\n\n```\ndef generate_graph(input: list[str]) -> KnowledgeGraph:\n    # Initialize an empty KnowledgeGraph\n    cur_state = KnowledgeGraph()\n\n    # Iterate over the input list\n    for i, inp in enumerate(input):\n        new_updates = client.chat.completions.create(\n            model=\"gpt-4-1106-preview\",\n            messages=[\\\n                {\\\n                    \"role\": \"system\",\\\n                    \"content\": \"\"\"You are an iterative knowledge graph builder.\\\n                    You are given the current state of the graph, and you must append the nodes and edges\\\n                    to it Do not procide any duplcates and try to reuse nodes as much as possible.\"\"\",\\\n                },\\\n                {\\\n                    \"role\": \"user\",\\\n                    \"content\": f\"\"\"Extract any new nodes and edges from the following:\\\n                    # Part {i}/{len(input)} of the input:\\\n\\\n                    {inp}\"\"\",\\\n                },\\\n                {\\\n                    \"role\": \"user\",\\\n                    \"content\": f\"\"\"Here is the current state of the graph:\\\n                    {cur_state.model_dump_json(indent=2)}\"\"\",\\\n                },\\\n            ],\n            response_model=KnowledgeGraph,\n        )  # type: ignore\n\n        # Update the current state with the new updates\n        cur_state = cur_state.update(new_updates)\n\n        # Draw the current state of the graph\n        cur_state.visualize_knowledge_graph()\n\n    # Return the final state of the KnowledgeGraph\n    return cur_state\n\n```\n\ndef generate\\_graph(input: list\\[str\\]) -> KnowledgeGraph: # Initialize an empty KnowledgeGraph cur\\_state = KnowledgeGraph() # Iterate over the input list for i, inp in enumerate(input): new\\_updates = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[ { \"role\": \"system\", \"content\": \"\"\"You are an iterative knowledge graph builder. You are given the current state of the graph, and you must append the nodes and edges to it Do not procide any duplcates and try to reuse nodes as much as possible.\"\"\", }, { \"role\": \"user\", \"content\": f\"\"\"Extract any new nodes and edges from the following: # Part {i}/{len(input)} of the input: {inp}\"\"\", }, { \"role\": \"user\", \"content\": f\"\"\"Here is the current state of the graph: {cur\\_state.model\\_dump\\_json(indent=2)}\"\"\", }, \\], response\\_model=KnowledgeGraph, ) # type: ignore # Update the current state with the new updates cur\\_state = cur\\_state.update(new\\_updates) # Draw the current state of the graph cur\\_state.visualize\\_knowledge\\_graph() # Return the final state of the KnowledgeGraph return cur\\_state\n\n### Examples Use Case [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/\\#examples-use-case)\n\nIn this approach, we process the text in manageable chunks, one at a time.\n\nThis method is particularly beneficial when dealing with extensive text that may not fit into a single prompt.\n\nIt is especially useful in scenarios such as constructing a knowledge graph for a complex topic, where the information is distributed across multiple documents or sections.\n\nIn \\[13\\]:\n\nCopied!\n\n```\ntext_chunks = [\\\n    \"Jason knows a lot about quantum mechanics. He is a physicist. He is a professor\",\\\n    \"Professors are smart.\",\\\n    \"Sarah knows Jason and is a student of his.\",\\\n    \"Sarah is a student at the University of Toronto. and UofT is in Canada.\",\\\n]\n\ngraph: KnowledgeGraph = generate_graph(text_chunks)\n\n```\n\ntext\\_chunks = \\[ \"Jason knows a lot about quantum mechanics. He is a physicist. He is a professor\", \"Professors are smart.\", \"Sarah knows Jason and is a student of his.\", \"Sarah is a student at the University of Toronto. and UofT is in Canada.\", \\] graph: KnowledgeGraph = generate\\_graph(text\\_chunks)\n\n![No description has been provided for this image](<Base64-Image-Removed>)\n\n![No description has been provided for this image](<Base64-Image-Removed>)\n\n![No description has been provided for this image](<Base64-Image-Removed>)\n\n![No description has been provided for this image](<Base64-Image-Removed>)\n\n## Conclusion [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/\\#conclusion)\n\nThis tutorial shows how to generate and visualize a knowledge graph for complex topics. It also demonstrates how to extract graphic knowledge from the language model or provided text. The tutorial highlights the iterative process of building the knowledge graph by processing text in smaller chunks and updating the graph with new information.\n\nUsing this approach, we can extract various things, including:\n\n1. People and their relationships in a story.\n\n```\nclass People(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Relationship(BaseModel):\n    id: str\n    source: str\n    target: str\n    label: str\n    description: str\n\nclass Story(BaseModel):\n    people: List[People]\n    relationships: List[Relationship]\n\n```\n\n2. Task dependencies and action items from a transcript.\n\n```\nclass Task(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Participant(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Assignment(BaseModel):\n    id: str\n    source: str\n    target: str\n    label: str\n    description: str\n\nclass Transcript(BaseModel):\n    tasks: List[Task]\n    participants: List[Participant]\n    assignments: List[Assignment]\n\n```\n\n3. Key concepts and their relationships from a research paper.\n4. Entities and their relationships from a news article.\n\nAs an exercise, try to implement one of the above examples.\n\nAll of them will follow an idea of iteratively extracting more and more information and accumulating it into some state.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/5-knowledge-graphs/",
      "ogUrl": "https://python.useinstructor.com/tutorials/5-knowledge-graphs/",
      "title": "Knowledge Graphs - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/5-knowledge-graphs/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/5-knowledge-graphs.png",
      "ogTitle": "Knowledge Graphs - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/5-knowledge-graphs.png",
      "og:title": "Knowledge Graphs - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/5-knowledge-graphs/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/5-knowledge-graphs.png",
      "twitter:title": "Knowledge Graphs - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/3-0-applications-rag/#applying-structured-output-to-rag-applications)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/3-0-applications-rag.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/3-0-applications-rag.ipynb \"View source of this page\")\n\n# Applying Structured Output to RAG applications [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/\\#applying-structured-output-to-rag-applications)\n\n**What is RAG?**\n\nRetrieval Augmented Generation (RAG) models are the bridge between large language models and external knowledge databases. They fetch the relevant data for a given query. For example, if you have some documents and want to ask questions related to the content of those documents, RAG models help by retrieving data from those documents and passing it to the LLM in queries.\n\n**How do RAG models work?**\n\nThe typical RAG process involves embedding a user query and searching a vector database to find the most relevant information to supplement the generated response. This approach is particularly effective when the database contains information closely matching the query but not more than that.\n\n![Image](https://jxnl.github.io/instructor/blog/img/dumb_rag.png)\n\n**Why is there a need for them?**\n\nPre-trained large language models do not learn over time. If you ask them a question they have not been trained on, they will often hallucinate. Therefore, we need to embed our own data to achieve a better output.\n\n## Simple RAG [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/\\#simple-rag)\n\n**What is it?**\n\nThe simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.\n\n- **Query-Document Mismatch:** It assumes that the query and document embeddings will align in the vector space, which is often not the case.\n- **Text Search Limitations:** The model is restricted to simple text queries without the nuances of advanced search features.\n- **Limited Planning Ability:** It fails to consider additional contextual information that could refine the search results.\n\n## Improving the RAG model [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/\\#improving-the-rag-model)\n\n**What's the solution?**\n\nEnhancing RAG requires a more sophisticated approach known as query understanding.\n\nThis process involves analyzing the user's query and transforming it to better match the backend's search capabilities.\n\nBy doing so, we can significantly improve both the precision and recall of the search results, providing more accurate and relevant responses.\n\n![Image](https://jxnl.github.io/instructor/blog/img/query_understanding.png)\n\n## Practical Examples [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/\\#practical-examples)\n\nIn the examples below, we're going to use the [`instructor`](https://github.com/jxnl/instructor) library to simplify the interaction between the programmer and language models via the function-calling API.\n\nIn \\[1\\]:\n\nCopied!\n\n```\nimport instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.patch(OpenAI())\n\n```\n\nimport instructor from openai import OpenAI from pydantic import BaseModel, Field client = instructor.patch(OpenAI())\n\n### Example 1) Improving Extractions [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/\\#example-1-improving-extractions)\n\nOne of the big limitations is that often times the query we embed and the text we are searching for may not have a direct match, leading to suboptimal results. A common method of using structured output is to extract information from a document and use it to answer a question. Directly, we can be creative in how we extract, summarize and generate potential questions in order for our embeddings to do better.\n\nFor example, instead of using just a text chunk we could try to:\n\n1. extract key words and themes\n2. extract hypothetical questions\n3. generate a summary of the text\n\nIn the example below, we use the `instructor` library to extract the key words and themes from a text chunk and use them to answer a question.\n\nIn \\[2\\]:\n\nCopied!\n\n```\nclass Extraction(BaseModel):\n    topic: str\n    summary: str\n    hypothetical_questions: list[str] = Field(\n        default_factory=list,\n        description=\"Hypothetical questions that this document could answer\",\n    )\n    keywords: list[str] = Field(\n        default_factory=list, description=\"Keywords that this document is about\"\n    )\n\n```\n\nclass Extraction(BaseModel): topic: str summary: str hypothetical\\_questions: list\\[str\\] = Field( default\\_factory=list, description=\"Hypothetical questions that this document could answer\", ) keywords: list\\[str\\] = Field( default\\_factory=list, description=\"Keywords that this document is about\" )\n\nIn \\[3\\]:\n\nCopied!\n\n```\nfrom pprint import pprint\nfrom collections.abc import Iterable\n\ntext_chunk = \"\"\"\n## Simple RAG\n\n**What is it?**\n\nThe simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.\n\n**What are the limitations?**\n\n- **Query-Document Mismatch:** It assumes that the query and document embeddings will align in the vector space, which is often not the case.\n    - Query: \"Tell me about climate change effects on marine life.\"\n    - Issue: The model might retrieve documents related to general climate change or marine life, missing the specific intersection of both topics.\n- **Monolithic Search Backend:** It relies on a single search method and backend, reducing flexibility and the ability to handle multiple data sources.\n    - Query: \"Latest research in quantum computing.\"\n    - Issue: The model might only search in a general science database, missing out on specialized quantum computing resources.\n- **Text Search Limitations:** The model is restricted to simple text queries without the nuances of advanced search features.\n    - Query: \"what problems did we fix last week\"\n    - Issue: cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week.\n- **Limited Planning Ability:** It fails to consider additional contextual information that could refine the search results.\n    - Query: \"Tips for first-time Europe travelers.\"\n    - Issue: The model might provide general travel advice, ignoring the specific context of first-time travelers or European destinations.\n\"\"\"\n\nextractions = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    stream=True,\n    response_model=Iterable[Extraction],\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"Your role is to extract chunks from the following and create a set of topics.\",\\\n        },\\\n        {\"role\": \"user\", \"content\": text_chunk},\\\n    ],\n)\n\nfor extraction in extractions:\n    pprint(extraction.model_dump())\n\n```\n\nfrom pprint import pprint from collections.abc import Iterable text\\_chunk = \"\"\" ## Simple RAG \\*\\*What is it?\\*\\* The simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources. \\*\\*What are the limitations?\\*\\* - \\*\\*Query-Document Mismatch:\\*\\* It assumes that the query and document embeddings will align in the vector space, which is often not the case. - Query: \"Tell me about climate change effects on marine life.\" - Issue: The model might retrieve documents related to general climate change or marine life, missing the specific intersection of both topics. - \\*\\*Monolithic Search Backend:\\*\\* It relies on a single search method and backend, reducing flexibility and the ability to handle multiple data sources. - Query: \"Latest research in quantum computing.\" - Issue: The model might only search in a general science database, missing out on specialized quantum computing resources. - \\*\\*Text Search Limitations:\\*\\* The model is restricted to simple text queries without the nuances of advanced search features. - Query: \"what problems did we fix last week\" - Issue: cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week. - \\*\\*Limited Planning Ability:\\*\\* It fails to consider additional contextual information that could refine the search results. - Query: \"Tips for first-time Europe travelers.\" - Issue: The model might provide general travel advice, ignoring the specific context of first-time travelers or European destinations. \"\"\" extractions = client.chat.completions.create( model=\"gpt-4-1106-preview\", stream=True, response\\_model=Iterable\\[Extraction\\], messages=\\[ { \"role\": \"system\", \"content\": \"Your role is to extract chunks from the following and create a set of topics.\", }, {\"role\": \"user\", \"content\": text\\_chunk}, \\], ) for extraction in extractions: pprint(extraction.model\\_dump())\n\n```\n{'hypothetical_questions': ['What is the basic concept behind simple RAG?',\\\n                            'How does simple RAG work for information '\\\n                            'retrieval?'],\n 'keywords': ['Simple RAG',\\\n              'Retrieval-Augmented Generation',\\\n              'user query',\\\n              'embedding search',\\\n              'vector database',\\\n              'Wikipedia articles',\\\n              'information retrieval'],\n 'summary': 'The simplest implementation of Retrieval-Augmented Generation '\n            '(RAG) involves embedding a user query and conducting a single '\n            'embedding search in a vector database, like a vector store of '\n            'Wikipedia articles, to retrieve relevant information. This method '\n            'may not be ideal for complex queries or varied data sources.',\n 'topic': 'Simple RAG'}\n{'hypothetical_questions': ['What are the drawbacks of using simple RAG '\\\n                            'systems?',\\\n                            'How does query-document mismatch affect the '\\\n                            'performance of RAG?',\\\n                            'Why is a monolithic search backend a limitation '\\\n                            'for RAG?'],\n 'keywords': ['limitations',\\\n              'query-document mismatch',\\\n              'simple RAG',\\\n              'monolithic search backend',\\\n              'text search',\\\n              'planning ability',\\\n              'contextual information'],\n 'summary': 'Key limitations of the simple RAG include query-document '\n            'mismatch, reliance on a single search backend, constraints of '\n            'text search capabilities, and limited planning ability to '\n            'leverage contextual information. These issues can result in '\n            'suboptimal search outcomes and retrieval of irrelevant or broad '\n            'information.',\n 'topic': 'Limitations of Simple RAG'}\n\n```\n\nNow you can imagine if you were to embed the summaries, hypothetical questions, and keywords in a vector database (i.e. in the metadata fields of a vector database), you can then use a vector search to find the best matching document for a given query. What you'll find is that the results are much better than if you were to just embed the text chunk!\n\n### Example 2) Understanding 'recent queries' to add temporal context [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/\\#example-2-understanding-recent-queries-to-add-temporal-context)\n\nOne common application of using structured outputs for query understanding is to identify the intent of a user's query. In this example we're going to use a simple schema to seperately process the query to add additional temporal context.\n\nIn \\[4\\]:\n\nCopied!\n\n```\nfrom datetime import date\n\nclass DateRange(BaseModel):\n    start: date\n    end: date\n\nclass Query(BaseModel):\n    rewritten_query: str\n    published_daterange: DateRange\n\n```\n\nfrom datetime import date class DateRange(BaseModel): start: date end: date class Query(BaseModel): rewritten\\_query: str published\\_daterange: DateRange\n\nIn this example, `DateRange` and `Query` are Pydantic models that structure the user's query with a date range and a list of domains to search within.\n\nThese models **restructure** the user's query by including a rewritten query, a range of published dates, and a list of domains to search in.\n\nUsing the new restructured query, we can apply this pattern to our function calls to obtain results that are optimized for our backend.\n\nIn \\[5\\]:\n\nCopied!\n\n```\ndef expand_query(q) -> Query:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Query,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\\\n            },\\\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\\\n        ],\n    )\n\nquery = expand_query(\"What are some recent developments in AI?\")\nquery\n\n```\n\ndef expand\\_query(q) -> Query: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\_model=Query, messages=\\[ { \"role\": \"system\", \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\", }, {\"role\": \"user\", \"content\": f\"query: {q}\"}, \\], ) query = expand\\_query(\"What are some recent developments in AI?\") query\n\nOut\\[5\\]:\n\n```\nQuery(rewritten_query='Recent developments in artificial intelligence', published_daterange=DateRange(start=datetime.date(2024, 1, 1), end=datetime.date(2024, 3, 31)))\n```\n\nThis isn't just about adding some date ranges. We can even use some chain of thought prompting to generate tailored searches that are deeply integrated with our backend.\n\nIn \\[6\\]:\n\nCopied!\n\n```\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Think step by step to plan what is the best time range to search in\"\n    )\n    start: date\n    end: date\n\nclass Query(BaseModel):\n    rewritten_query: str = Field(\n        description=\"Rewrite the query to make it more specific\"\n    )\n    published_daterange: DateRange = Field(\n        description=\"Effective date range to search in\"\n    )\n\ndef expand_query(q) -> Query:\n    return client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        response_model=Query,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\\\n            },\\\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\\\n        ],\n    )\n\nexpand_query(\"What are some recent developments in AI?\")\n\n```\n\nclass DateRange(BaseModel): chain\\_of\\_thought: str = Field( description=\"Think step by step to plan what is the best time range to search in\" ) start: date end: date class Query(BaseModel): rewritten\\_query: str = Field( description=\"Rewrite the query to make it more specific\" ) published\\_daterange: DateRange = Field( description=\"Effective date range to search in\" ) def expand\\_query(q) -> Query: return client.chat.completions.create( model=\"gpt-4-1106-preview\", response\\_model=Query, messages=\\[ { \"role\": \"system\", \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\", }, {\"role\": \"user\", \"content\": f\"query: {q}\"}, \\], ) expand\\_query(\"What are some recent developments in AI?\")\n\nOut\\[6\\]:\n\n```\nQuery(rewritten_query='latest advancements in artificial intelligence', published_daterange=DateRange(chain_of_thought='Since the user is asking for recent developments, it would be relevant to look for articles and papers published within the last year. Therefore, setting the start date to a year before today and the end date to today will cover the most recent advancements.', start=datetime.date(2023, 3, 31), end=datetime.date(2024, 3, 31)))\n```\n\n## Using Weights and Biases to track experiments [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/\\#using-weights-and-biases-to-track-experiments)\n\nWhile running a function like this production is quite simple, a lot of time will be spend on iterating and improving the model. To do this, we can use Weights and Biases to track our experiments.\n\nIn order to do so we wand manage a few things\n\n1. Save input and output pairs for later\n2. Save the JSON schema for the response\\_model\n3. Having snapshots of the model and data allow us to compare results over time, and as we make changes to the model we can see how the results change.\n\nThis is particularly useful when we might want to blend a mix of synthetic and real data to evaluate our model. We can use the `wandb` library to track our experiments and save the results to a dashboard.\n\nIn \\[7\\]:\n\nCopied!\n\n```\nimport json\nimport instructor\n\nfrom openai import AsyncOpenAI\nfrom datetime import date\nfrom pydantic import BaseModel, Field\n\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Think step by step to plan what is the best time range to search in\"\n    )\n    start: date\n    end: date\n\nclass Query(BaseModel):\n    rewritten_query: str = Field(\n        description=\"Rewrite the query to make it more specific\"\n    )\n    published_daterange: DateRange = Field(\n        description=\"Effective date range to search in\"\n    )\n\n    def report(self):\n        dct = self.model_dump()\n        dct[\"usage\"] = self._raw_response.usage.model_dump()\n        return dct\n\n# We'll use a different client for async calls\n# To highlight the difference and how we can use both\naclient = instructor.patch(AsyncOpenAI())\n\nasync def expand_query(\n    q, *, model: str = \"gpt-4-1106-preview\", temp: float = 0\n) -> Query:\n    return await aclient.chat.completions.create(\n        model=model,\n        temperature=temp,\n        response_model=Query,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\\\n            },\\\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\\\n        ],\n    )\n\n```\n\nimport json import instructor from openai import AsyncOpenAI from datetime import date from pydantic import BaseModel, Field class DateRange(BaseModel): chain\\_of\\_thought: str = Field( description=\"Think step by step to plan what is the best time range to search in\" ) start: date end: date class Query(BaseModel): rewritten\\_query: str = Field( description=\"Rewrite the query to make it more specific\" ) published\\_daterange: DateRange = Field( description=\"Effective date range to search in\" ) def report(self): dct = self.model\\_dump() dct\\[\"usage\"\\] = self.\\_raw\\_response.usage.model\\_dump() return dct # We'll use a different client for async calls # To highlight the difference and how we can use both aclient = instructor.patch(AsyncOpenAI()) async def expand\\_query( q, \\*, model: str = \"gpt-4-1106-preview\", temp: float = 0 ) -> Query: return await aclient.chat.completions.create( model=model, temperature=temp, response\\_model=Query, messages=\\[ { \"role\": \"system\", \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\", }, {\"role\": \"user\", \"content\": f\"query: {q}\"}, \\], )\n\nIn \\[8\\]:\n\nCopied!\n\n```\n# % pip install pandas wandb\nimport pandas as pd\nfrom typing import Any\n\ndef flatten_dict(d: dict[str, Any], parent_key: str = \"\", sep: str = \"_\") -> dict[str, Any]:\n    \"\"\"\n    Flatten a nested dictionary.\n\n    :param d: The nested dictionary to flatten.\n    :param parent_key: The base key to use for the flattened keys.\n    :param sep: Separator to use between keys.\n    :return: A flattened dictionary.\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\ndef dicts_to_df(list_of_dicts: list[dict[str, Any]]) -> pd.DataFrame:\n    \"\"\"\n    Convert a list of dictionaries to a pandas DataFrame.\n\n    :param list_of_dicts: List of dictionaries, potentially nested.\n    :return: A pandas DataFrame representing the flattened data.\n    \"\"\"\n    # Flatten each dictionary and create a DataFrame\n    flattened_data = [flatten_dict(d) for d in list_of_dicts]\n    return pd.DataFrame(flattened_data)\n\n```\n\n\\# % pip install pandas wandb import pandas as pd from typing import Any def flatten\\_dict(d: dict\\[str, Any\\], parent\\_key: str = \"\", sep: str = \"\\_\") -> dict\\[str, Any\\]: \"\"\" Flatten a nested dictionary. :param d: The nested dictionary to flatten. :param parent\\_key: The base key to use for the flattened keys. :param sep: Separator to use between keys. :return: A flattened dictionary. \"\"\" items = \\[\\] for k, v in d.items(): new\\_key = f\"{parent\\_key}{sep}{k}\" if parent\\_key else k if isinstance(v, dict): items.extend(flatten\\_dict(v, new\\_key, sep=sep).items()) else: items.append((new\\_key, v)) return dict(items) def dicts\\_to\\_df(list\\_of\\_dicts: list\\[dict\\[str, Any\\]\\]) -> pd.DataFrame: \"\"\" Convert a list of dictionaries to a pandas DataFrame. :param list\\_of\\_dicts: List of dictionaries, potentially nested. :return: A pandas DataFrame representing the flattened data. \"\"\" # Flatten each dictionary and create a DataFrame flattened\\_data = \\[flatten\\_dict(d) for d in list\\_of\\_dicts\\] return pd.DataFrame(flattened\\_data)\n\nIn \\[9\\]:\n\nCopied!\n\n```\nimport asyncio\nimport time\nimport pandas as pd\nimport wandb\n\nmodel = \"gpt-4-1106-preview\"\ntemp = 0\n\nrun = wandb.init(\n    project=\"query\",\n    config={\"model\": model, \"temp\": temp},\n)\n\ntest_queries = [\\\n    \"latest developments in artificial intelligence last 3 weeks\",\\\n    \"renewable energy trends past month\",\\\n    \"quantum computing advancements last 2 months\",\\\n    \"biotechnology updates last 10 days\",\\\n]\nstart = time.perf_counter()\nqueries = await asyncio.gather(\n    *[expand_query(q, model=model, temp=temp) for q in test_queries]\n)\nduration = time.perf_counter() - start\n\nwith open(\"schema.json\", \"w+\") as f:\n    schema = Query.model_json_schema()\n    json.dump(schema, f, indent=2)\n\nwith open(\"results.jsonlines\", \"w+\") as f:\n    for query in queries:\n        f.write(query.model_dump_json() + \"\\n\")\n\ndf = dicts_to_df([q.report() for q in queries])\ndf[\"input\"] = test_queries\ndf.to_csv(\"results.csv\")\n\nrun.log({\"schema\": wandb.Table(dataframe=pd.DataFrame([{\"schema\": schema}]))})\nrun.log(\n    {\n        \"usage_total_tokens\": df[\"usage_total_tokens\"].sum(),\n        \"usage_completion_tokens\": df[\"usage_completion_tokens\"].sum(),\n        \"usage_prompt_tokens\": df[\"usage_prompt_tokens\"].sum(),\n        \"duration (s)\": duration,\n        \"average duration (s)\": duration / len(queries),\n        \"n_queries\": len(queries),\n    }\n)\n\nrun.log(\n    {\n        \"results\": wandb.Table(dataframe=df),\n    }\n)\n\nfiles = wandb.Artifact(\"data\", type=\"dataset\")\nfiles.add_file(\"schema.json\")\nfiles.add_file(\"results.jsonlines\")\nfiles.add_file(\"results.csv\")\n\nrun.log_artifact(files)\nrun.finish()\n\n```\n\nimport asyncio import time import pandas as pd import wandb model = \"gpt-4-1106-preview\" temp = 0 run = wandb.init( project=\"query\", config={\"model\": model, \"temp\": temp}, ) test\\_queries = \\[ \"latest developments in artificial intelligence last 3 weeks\", \"renewable energy trends past month\", \"quantum computing advancements last 2 months\", \"biotechnology updates last 10 days\", \\] start = time.perf\\_counter() queries = await asyncio.gather( \\*\\[expand\\_query(q, model=model, temp=temp) for q in test\\_queries\\] ) duration = time.perf\\_counter() - start with open(\"schema.json\", \"w+\") as f: schema = Query.model\\_json\\_schema() json.dump(schema, f, indent=2) with open(\"results.jsonlines\", \"w+\") as f: for query in queries: f.write(query.model\\_dump\\_json() + \"\\\\n\") df = dicts\\_to\\_df(\\[q.report() for q in queries\\]) df\\[\"input\"\\] = test\\_queries df.to\\_csv(\"results.csv\") run.log({\"schema\": wandb.Table(dataframe=pd.DataFrame(\\[{\"schema\": schema}\\]))}) run.log( { \"usage\\_total\\_tokens\": df\\[\"usage\\_total\\_tokens\"\\].sum(), \"usage\\_completion\\_tokens\": df\\[\"usage\\_completion\\_tokens\"\\].sum(), \"usage\\_prompt\\_tokens\": df\\[\"usage\\_prompt\\_tokens\"\\].sum(), \"duration (s)\": duration, \"average duration (s)\": duration / len(queries), \"n\\_queries\": len(queries), } ) run.log( { \"results\": wandb.Table(dataframe=df), } ) files = wandb.Artifact(\"data\", type=\"dataset\") files.add\\_file(\"schema.json\") files.add\\_file(\"results.jsonlines\") files.add\\_file(\"results.csv\") run.log\\_artifact(files) run.finish()\n\nThe output of Weights and Biases would return something like the below table.\n\n| Metric | Value |\n| --- | --- |\n| average duration (s) | 1.5945 |\n| duration (s) | 6.37799 |\n| n\\_queries | 4 |\n| usage\\_completion\\_tokens | 376 |\n| usage\\_prompt\\_tokens | 780 |\n| usage\\_total\\_tokens | 1156 |\n\n### Example 3) Personal Assistants, parallel processing [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/\\#example-3-personal-assistants-parallel-processing)\n\nA personal assistant application needs to interpret vague queries and fetch information from multiple backends, such as emails and calendars. By modeling the assistant's capabilities using Pydantic, we can dispatch the query to the correct backend and retrieve a unified response.\n\nFor instance, when you ask, \"What's on my schedule today?\", the application needs to fetch data from various sources like events, emails, and reminders. This data is stored across different backends, but the goal is to provide a consolidated summary of results.\n\nIt's important to note that the data from these sources may not be embedded in a search backend. Instead, they could be accessed through different clients like a calendar or email, spanning both personal and professional accounts.\n\nIn \\[10\\]:\n\nCopied!\n\n```\nfrom typing import Literal\n\nclass SearchClient(BaseModel):\n    query: str = Field(description=\"The search query that will go into the search bar\")\n    keywords: list[str]\n    email: str\n    source: Literal[\"gmail\", \"calendar\"]\n    date_range: DateRange\n\nclass Retrieval(BaseModel):\n    queries: list[SearchClient]\n\n```\n\nfrom typing import Literal class SearchClient(BaseModel): query: str = Field(description=\"The search query that will go into the search bar\") keywords: list\\[str\\] email: str source: Literal\\[\"gmail\", \"calendar\"\\] date\\_range: DateRange class Retrieval(BaseModel): queries: list\\[SearchClient\\]\n\nNow, we can utilize this with a straightforward query such as \"What do I have today?\".\n\nThe system will attempt to asynchronously dispatch the query to the appropriate backend.\n\nHowever, it's still crucial to remember that effectively prompting the language model is still a key aspect.\n\nIn \\[11\\]:\n\nCopied!\n\n```\nretrieval = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Retrieval,\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": f\"\"\"You are Jason's personal assistant.\\\n                He has two emails jason@work.com jason@personal.com\\\n                Today is {date.today()}\"\"\",\\\n        },\\\n        {\"role\": \"user\", \"content\": \"What do I have today for work? any new emails?\"},\\\n    ],\n)\nprint(retrieval.model_dump_json(indent=4))\n\n```\n\nretrieval = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\_model=Retrieval, messages=\\[ { \"role\": \"system\", \"content\": f\"\"\"You are Jason's personal assistant. He has two emails jason@work.com jason@personal.com Today is {date.today()}\"\"\", }, {\"role\": \"user\", \"content\": \"What do I have today for work? any new emails?\"}, \\], ) print(retrieval.model\\_dump\\_json(indent=4))\n\n```\n{\n    \"queries\": [\\\n        {\\\n            \"query\": \"work\",\\\n            \"keywords\": [\\\n                \"work\",\\\n                \"today\"\\\n            ],\\\n            \"email\": \"jason@work.com\",\\\n            \"source\": \"gmail\",\\\n            \"date_range\": {\\\n                \"chain_of_thought\": \"Check today's work schedule\",\\\n                \"start\": \"2024-03-31\",\\\n                \"end\": \"2024-03-31\"\\\n            }\\\n        },\\\n        {\\\n            \"query\": \"new emails\",\\\n            \"keywords\": [\\\n                \"email\",\\\n                \"new\"\\\n            ],\\\n            \"email\": \"jason@work.com\",\\\n            \"source\": \"gmail\",\\\n            \"date_range\": {\\\n                \"chain_of_thought\": \"Check for new emails today\",\\\n                \"start\": \"2024-03-31\",\\\n                \"end\": \"2024-03-31\"\\\n            }\\\n        }\\\n    ]\n}\n\n```\n\nTo make it more challenging, we will assign it multiple tasks, followed by a list of queries that are routed to various search backends, such as email and calendar. Not only do we dispatch to different backends, over which we have no control, but we are also likely to render them to the user in different ways.\n\nIn \\[12\\]:\n\nCopied!\n\n```\nretrieval = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=Retrieval,\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": f\"\"\"You are Jason's personal assistant.\\\n                He has two emails jason@work.com jason@personal.com\\\n                Today is {date.today()}\"\"\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"What meetings do I have today and are there any important emails I should be aware of\",\\\n        },\\\n    ],\n)\nprint(retrieval.model_dump_json(indent=4))\n\n```\n\nretrieval = client.chat.completions.create( model=\"gpt-4-1106-preview\", response\\_model=Retrieval, messages=\\[ { \"role\": \"system\", \"content\": f\"\"\"You are Jason's personal assistant. He has two emails jason@work.com jason@personal.com Today is {date.today()}\"\"\", }, { \"role\": \"user\", \"content\": \"What meetings do I have today and are there any important emails I should be aware of\", }, \\], ) print(retrieval.model\\_dump\\_json(indent=4))\n\n```\n{\n    \"queries\": [\\\n        {\\\n            \"query\": \"Jason's meetings\",\\\n            \"keywords\": [\\\n                \"meeting\",\\\n                \"appointment\",\\\n                \"schedule\",\\\n                \"calendar\"\\\n            ],\\\n            \"email\": \"jason@work.com\",\\\n            \"source\": \"calendar\",\\\n            \"date_range\": {\\\n                \"chain_of_thought\": \"Since today's date is 2024-03-31, we should look for meetings scheduled for this exact date.\",\\\n                \"start\": \"2024-03-31\",\\\n                \"end\": \"2024-03-31\"\\\n            }\\\n        }\\\n    ]\n}\n\n```\n\n### Example 4) Decomposing questions [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/\\#example-4-decomposing-questions)\n\nLastly, a lightly more complex example of a problem that can be solved with structured output is decomposing questions. Where you ultimately want to decompose a question into a series of sub-questions that can be answered by a search backend. For example\n\n\"Whats the difference in populations of jason's home country and canada?\"\n\nYou'd ultimately need to know a few things\n\n1. Jason's home country\n2. The population of Jason's home country\n3. The population of Canada\n4. The difference between the two\n\nThis would not be done correctly as a single query, nor would it be done in parallel, however there are some opportunities try to be parallel since not all of the sub-questions are dependent on each other.\n\nIn \\[13\\]:\n\nCopied!\n\n```\nclass Question(BaseModel):\n    id: int = Field(..., description=\"A unique identifier for the question\")\n    query: str = Field(..., description=\"The question decomposited as much as possible\")\n    subquestions: list[int] = Field(\n        default_factory=list,\n        description=\"The subquestions that this question is composed of\",\n    )\n\nclass QueryPlan(BaseModel):\n    root_question: str = Field(..., description=\"The root question that the user asked\")\n    plan: list[Question] = Field(\n        ..., description=\"The plan to answer the root question and its subquestions\"\n    )\n\nretrieval = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=QueryPlan,\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"You are a query understanding system capable of decomposing a question into subquestions.\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"What is the difference between the population of jason's home country and canada?\",\\\n        },\\\n    ],\n)\n\nprint(retrieval.model_dump_json(indent=4))\n\n```\n\nclass Question(BaseModel): id: int = Field(..., description=\"A unique identifier for the question\") query: str = Field(..., description=\"The question decomposited as much as possible\") subquestions: list\\[int\\] = Field( default\\_factory=list, description=\"The subquestions that this question is composed of\", ) class QueryPlan(BaseModel): root\\_question: str = Field(..., description=\"The root question that the user asked\") plan: list\\[Question\\] = Field( ..., description=\"The plan to answer the root question and its subquestions\" ) retrieval = client.chat.completions.create( model=\"gpt-4-1106-preview\", response\\_model=QueryPlan, messages=\\[ { \"role\": \"system\", \"content\": \"You are a query understanding system capable of decomposing a question into subquestions.\", }, { \"role\": \"user\", \"content\": \"What is the difference between the population of jason's home country and canada?\", }, \\], ) print(retrieval.model\\_dump\\_json(indent=4))\n\n```\n{\n    \"root_question\": \"What is the difference between the population of Jason's home country and Canada?\",\n    \"plan\": [\\\n        {\\\n            \"id\": 1,\\\n            \"query\": \"What is the population of Jason's home country?\",\\\n            \"subquestions\": []\\\n        },\\\n        {\\\n            \"id\": 2,\\\n            \"query\": \"What is the population of Canada?\",\\\n            \"subquestions\": []\\\n        },\\\n        {\\\n            \"id\": 3,\\\n            \"query\": \"What is the difference between two population numbers?\",\\\n            \"subquestions\": [\\\n                1,\\\n                2\\\n            ]\\\n        }\\\n    ]\n}\n\n```\n\nI hope in this section I've exposed you to some ways we can be creative in modeling structured outputs to leverage LLMS in building some lightweight components for our systems.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/3-0-applications-rag/",
      "ogUrl": "https://python.useinstructor.com/tutorials/3-0-applications-rag/",
      "title": "Applications RAG - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/3-0-applications-rag/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/3-0-applications-rag.png",
      "ogTitle": "Applications RAG - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/3-0-applications-rag.png",
      "og:title": "Applications RAG - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/3-0-applications-rag/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/3-0-applications-rag.png",
      "twitter:title": "Applications RAG - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/6-chain-of-density/#chain-of-density-summarization)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/6-chain-of-density.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/6-chain-of-density.ipynb \"View source of this page\")\n\n# Chain Of Density Summarization [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/\\#chain-of-density-summarization)\n\n## Introduction [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/\\#introduction)\n\n**What is Chain Of Density summarization?**\n\nSummarizing extensive texts with AI can be challenging. Initially, an AI produces a summary, then refines it through multiple iterations, adding missing article entities. Each iteration adds new article entities to the summary, keeping length consistent, leading to an entity-dense, informative summary called Chain Of Density.\n\nIt was first introduced in the paper - From Sparse to Dense : GPT-4 Summarization with Chain of Density prompting.\n\nThis was done in the original paper by asking GPT-4 to generate all of the rewritten summaries in a single go with the following prompt below.\n\n> Article: {{ARTICLE}}\n>\n> You will generate increasingly concise, entity-dense summaries of the above Article.\n>\n> Repeat the following 2 steps 5 times.\n>\n> Step 1. Identify 1-3 informative Entities (\";\" delimited) from the Article which are missing from the previously generated summary. Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.\n>\n> A Missing Entity is:\n>\n> - Relevant: to the main story.\n> - Specific: descriptive yet concise (5 words or fewer).\n> - Novel; not in the previous summary.\n> - Faithful: present in the Article.\n> - Anywhere: located anywhere in the Article.\n>\n> Guidelines: phrases like \"the article discusses\"\n>\n> - The first summary should be long (4-5 sentences, -80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach -80 words.\n> - Make every word count: re-write the previous summary to improve flow and make space for additional entities.\n> - Make space with fusion, compression, and removal of uninformative\n>\n> - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n> - Missing entities can appear anywhere in the new summary.\n> - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n>\n> Remember, use the exact same number of words for each summary.\n>\n> Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing\\_Entities\" and \"Denser\\_Summary\"\n\nWhile the original paper used a single prompt to generate the iterative generations, we can go one step better with `Instructor` and break down the process into smaller API calls - with validation along the way.\n\nThe process can be broken down as seen below.\n\n![image.png](<Base64-Image-Removed>)\n\n### Setup and Dependencies [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/\\#setup-and-dependencies)\n\nWe'll be using two new libraries for our demonstration\n\n1. `spaCy` : This provides a handful of useful utilities to do generic NLP tasks with\n2. `nltk` : This was used by the original paper to count the number of tokens in our generated summaries\n\nWe'll need to install the tokenizer packages and the spacy english library before we can proceed with the rest of the lesson\n\nIn \\[1\\]:\n\nCopied!\n\n```\nimport nltk\nnltk.download('punkt')\n\n!python -m spacy download en_core_web_sm --quiet\n\n```\n\nimport nltk nltk.download('punkt') !python -m spacy download en\\_core\\_web\\_sm --quiet\n\n```\n[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n```\n\n```\n✔ Download and installation successful\nYou can now load the package via spacy.load('en_core_web_sm')\n\n```\n\nOnce that's done, let's now move on to writing some code.\n\n## Definitions [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/\\#definitions)\n\nThere are a few different definitions which we'll need to understand in the tutorial. They are\n\n1. Tokens and tokenizers\n2. Entities\n3. Entity-Dense\n\nOnce we've gotten a hang of these concepts, we'll walk through a simple implementation of a Chain Of Density summarizer\n\n### Tokens and Tokenizers [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/\\#tokens-and-tokenizers)\n\nIn the original paper, the authors used `NLTK` to split the generated summary into tokens. These represent the smallest units that each sentence could be broken into where each hold semantic meaning.\n\nLet's walk through a simple example to see how the `NLTK` tokenizer might work\n\nIn \\[2\\]:\n\nCopied!\n\n```\nimport nltk\nsentence = \"My favourite type of Sashimi is Toro\"\n\nnltk.word_tokenize(sentence)\n\n```\n\nimport nltk sentence = \"My favourite type of Sashimi is Toro\" nltk.word\\_tokenize(sentence)\n\nOut\\[2\\]:\n\n```\n['My', 'favourite', 'type', 'of', 'Sashimi', 'is', 'Toro']\n```\n\nNLTK's word tokenizer does more than just split by empty whitespace. It handles a lot of nice edge cases and contractions such as `don't` or `I'm`.\n\nIn \\[3\\]:\n\nCopied!\n\n```\nsentence = \"I'm fascinated by machine learning!\"\n\nnltk.word_tokenize(sentence)\n\n```\n\nsentence = \"I'm fascinated by machine learning!\" nltk.word\\_tokenize(sentence)\n\nOut\\[3\\]:\n\n```\n['I', \"'m\", 'fascinated', 'by', 'machine', 'learning', '!']\n```\n\nWe can then calculate the number of tokens by simply finding the `len` of the generated sequence.\n\nIn \\[4\\]:\n\nCopied!\n\n```\nsentence = \"I'm fascinated by machine learning!\"\ntokens = nltk.word_tokenize(sentence)\nprint(tokens)\nprint(len(tokens))\n\n```\n\nsentence = \"I'm fascinated by machine learning!\" tokens = nltk.word\\_tokenize(sentence) print(tokens) print(len(tokens))\n\n```\n['I', \"'m\", 'fascinated', 'by', 'machine', 'learning', '!']\n7\n\n```\n\n### Entities [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/\\#entities)\n\nA named entity is an object in the real-world that we identify using a name. Common examples include people, countries, products or even books that we know and love. We can use the `spaCy` library for us to be able to detect the number of entities in a given sentence.\n\nIn \\[5\\]:\n\nCopied!\n\n```\n# First we load in the library\nimport spacy\n\n# Then we initialise an NLP object.\nnlp = spacy.load(\"en_core_web_sm\")\n\n```\n\n\\# First we load in the library import spacy # Then we initialise an NLP object. nlp = spacy.load(\"en\\_core\\_web\\_sm\")\n\nIn \\[6\\]:\n\nCopied!\n\n```\nsentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n\ndoc = nlp(sentence)\ndoc.ents\n\n```\n\nsentence = \"Apple is looking at buying U.K. startup for $1 billion\" doc = nlp(sentence) doc.ents\n\nOut\\[6\\]:\n\n```\n(Apple, U.K., $1 billion)\n```\n\nWe can see that Spacy was able to identify unique and named entities that were present within the sentence using the `doc.ents` property. Let's see a few more examples.\n\nIn \\[7\\]:\n\nCopied!\n\n```\nsentence = \"A knowledge graph, also known as a semantic network\\\n, represents real-world entities and their relationships\"\n\ndoc = nlp(sentence)\ndoc.ents\n\n```\n\nsentence = \"A knowledge graph, also known as a semantic network\\ , represents real-world entities and their relationships\" doc = nlp(sentence) doc.ents\n\nOut\\[7\\]:\n\n```\n()\n```\n\nIn \\[8\\]:\n\nCopied!\n\n```\nsentence = \"For example, a node representing an author like 'J.K. Rowling'\\\ncan be connected to another node representing one of her books, 'Harry Potter'\\\n, with the edge 'author of'\"\n\ndoc = nlp(sentence)\ndoc.ents\n\n```\n\nsentence = \"For example, a node representing an author like 'J.K. Rowling'\\ can be connected to another node representing one of her books, 'Harry Potter'\\ , with the edge 'author of'\" doc = nlp(sentence) doc.ents\n\nOut\\[8\\]:\n\n```\n(J.K., one, Harry Potter')\n```\n\nAs we can see from the examples above, entities are not nouns. They're direct or indirect references to people, places, concepts.\n\n### Entity Density [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/\\#entity-density)\n\nNow that we know what tokens and tokens are, we can move on to our last concept - that of entity density. Entity density is simply the mean number of entities present per token within your string of text.\n\nIn \\[9\\]:\n\nCopied!\n\n```\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef calculate_entity_density(sentence:str):\n    tokens = nltk.word_tokenize(sentence)\n    entities = nlp(sentence).ents\n    entity_density = round(len(entities)/len(tokens),3)\n\n    return len(tokens),len(entities),entity_density\n\n```\n\nnlp = spacy.load(\"en\\_core\\_web\\_sm\") def calculate\\_entity\\_density(sentence:str): tokens = nltk.word\\_tokenize(sentence) entities = nlp(sentence).ents entity\\_density = round(len(entities)/len(tokens),3) return len(tokens),len(entities),entity\\_density\n\nIn \\[10\\]:\n\nCopied!\n\n```\nsentence_1 = \"A knowledge graph, also known as a semantic network\\\n, represents real-world entities and their relationships\"\n\ncalculate_entity_density(sentence_1)\n\n```\n\nsentence\\_1 = \"A knowledge graph, also known as a semantic network\\ , represents real-world entities and their relationships\" calculate\\_entity\\_density(sentence\\_1)\n\nOut\\[10\\]:\n\n```\n(17, 0, 0.0)\n```\n\nIn \\[11\\]:\n\nCopied!\n\n```\nsentence_2 = \"Apple is looking at buying U.K. startup for $1 billion\"\n\ncalculate_entity_density(sentence_2)\n\n```\n\nsentence\\_2 = \"Apple is looking at buying U.K. startup for $1 billion\" calculate\\_entity\\_density(sentence\\_2)\n\nOut\\[11\\]:\n\n```\n(11, 3, 0.273)\n```\n\nThis gives us a quantitative method to be able to understand and compare two different sentences/summaries.\n\nWe want summaries that are more entity-dense\n\nIn \\[12\\]:\n\nCopied!\n\n```\nsummary_1 = \"\"\"\nThis article discusses an incident that occurred during the Chinese Grand Prix\ninvolving two racing drivers, Jenson Button and Pastor Maldonado. The two were\ncompeting for the 13th place when Button collided with Maldonado's vehicle,\ncausing damage to both cars. The incident resulted in a penalty for Button,\nwho was demoted to 14th place. Maldonado, on the other hand, had to retire from\nthe race due to the damage his car sustained.\n\"\"\"\n\nsummary_2 = \"\"\"\nJenson Button's McLaren collided with Pastor Maldonado's Lotus during the Chinese\nGrand Prix, causing front wing damage to Button's car and rear-end damage to\nMaldonado's, forcing his retirement. Button received a five-second penalty and\ntwo superlicence points, dropping himto 14th. Fernando Alonso advanced two places,\nwhile Button was lapped by Nico Rosberg and Alonso by Sebastian Vettel and\nKimi Raikkonen.\n\"\"\"\n\ncalculate_entity_density(summary_1),calculate_entity_density(summary_2)\n\n```\n\nsummary\\_1 = \"\"\" This article discusses an incident that occurred during the Chinese Grand Prix involving two racing drivers, Jenson Button and Pastor Maldonado. The two were competing for the 13th place when Button collided with Maldonado's vehicle, causing damage to both cars. The incident resulted in a penalty for Button, who was demoted to 14th place. Maldonado, on the other hand, had to retire from the race due to the damage his car sustained. \"\"\" summary\\_2 = \"\"\" Jenson Button's McLaren collided with Pastor Maldonado's Lotus during the Chinese Grand Prix, causing front wing damage to Button's car and rear-end damage to Maldonado's, forcing his retirement. Button received a five-second penalty and two superlicence points, dropping himto 14th. Fernando Alonso advanced two places, while Button was lapped by Nico Rosberg and Alonso by Sebastian Vettel and Kimi Raikkonen. \"\"\" calculate\\_entity\\_density(summary\\_1),calculate\\_entity\\_density(summary\\_2)\n\nOut\\[12\\]:\n\n```\n((82, 11, 0.134), (71, 17, 0.239))\n```\n\nWe can see that the final summary is almost twice as dense as the first summary and is hence more _entity dense_.\n\n## Implementation [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/\\#implementation)\n\n### Data Classes [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/\\#data-classes)\n\nLet's start by walking through some of the data models that we'll be using as the response\\_model for our open ai function calls. We'll need a total of two different classes\n\n1. Initial Summary: which is the lengthy and overly verbose article\n2. Rewritten Summary : which represents\n\nIn \\[13\\]:\n\nCopied!\n\n```\nfrom pydantic import BaseModel,Field,field_validator\n\n```\n\nfrom pydantic import BaseModel,Field,field\\_validator\n\nIn \\[14\\]:\n\nCopied!\n\n```\nclass InitialSummary(BaseModel):\n    \"\"\"\n    This is an initial summary which should be long ( 4-5 sentences, ~80 words)\n    yet highly non-specific, containing little information beyond the entities marked as missing.\n    Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a summary of the article provided which is overly verbose and uses fillers. \\\n        It should be roughly 80 words in length\",\n    )\n\n```\n\nclass InitialSummary(BaseModel): \"\"\" This is an initial summary which should be long ( 4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words. \"\"\" summary: str = Field( ..., description=\"This is a summary of the article provided which is overly verbose and uses fillers. \\ It should be roughly 80 words in length\", )\n\nPydantic is extremely handy because it allows us to do two things\n\n1. We can validate that our generated outputs are consistent with what we want, **and write vanilla python to validate so**\n2. We can export the generated class definition into a simple schema that fits in perfectly with OpenAI's function calling\n\nIn \\[15\\]:\n\nCopied!\n\n```\nInitialSummary.model_json_schema()\n\n```\n\nInitialSummary.model\\_json\\_schema()\n\nOut\\[15\\]:\n\n```\n{'description': 'This is an initial summary which should be long ( 4-5 sentences, ~80 words)\\nyet highly non-specific, containing little information beyond the entities marked as missing.\\nUse overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.',\n 'properties': {'summary': {'description': 'This is a summary of the article provided which is overly verbose and uses fillers.         It should be roughly 80 words in length',\n   'title': 'Summary',\n   'type': 'string'}},\n 'required': ['summary'],\n 'title': 'InitialSummary',\n 'type': 'object'}\n```\n\nIt's important here to provide a good description of the overall class and the respective fields. This is because all of the descriptions that we write for the individual fields and the class itself **are directly used by the llm when generating outputs**.\n\nNow, as a quick recap, when we rewrite our summaries at each step, we're performing a few things\n\n1. We identify any entities from the original article that are relevant which are **missing from our current summary**\n2. We then rewrite our summary, making sure to include as many of these new entities as possible with the goal of increasing the entity density of the new summary\n3. We then make sure that we have included all of the entities in our previous summary in the new rewritten summary.\n\nWe can express this in the form of the data model seen below called `RewrittenSummary`.\n\nIn \\[16\\]:\n\nCopied!\n\n```\nclass RewrittenSummary(BaseModel):\n    \"\"\"\n    This is a new, denser summary of identical length which covers every entity\n    and detail from the previous summary plus the Missing Entities.\n\n    Guidelines\n    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    - Missing entities can appear anywhere in the new summary\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n    )\n    absent: list[str] = Field(\n        ...,\n        default_factory=list,\n        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n    )\n    missing: list[str] = Field(\n        default_factory=list,\n        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n    )\n\n```\n\nclass RewrittenSummary(BaseModel): \"\"\" This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. Guidelines - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" - Missing entities can appear anywhere in the new summary An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. \"\"\" summary: str = Field( ..., description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\", ) absent: list\\[str\\] = Field( ..., default\\_factory=list, description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\", ) missing: list\\[str\\] = Field( default\\_factory=list, description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\", )\n\nWe'd also want our rewritten summary to have\n\n1. No missing entities => `absent` should have a length of 0\n2. New entities to be added in the next rewrite -> `missing` should have at least 1 entry\n3. A minimum length of 60 tokens and to have a density of at least 0.08 ( **NOTE**: 60 tokens and the 0.08 cut off are chosen arbitrarily, feel free to adjust them even higher if you wish. However, this might require you to add more retries in your code )\n\nWe can do so using the `field_validator` that we learnt in the previous lesson. This allows us to add in a validator for a specific field to ensure it meets our requirements.\n\nThis gives us the final definition of our `RewrittenSummary` class as seen below\n\nIn \\[17\\]:\n\nCopied!\n\n```\nclass RewrittenSummary(BaseModel):\n    \"\"\"\n    This is a new, denser summary of identical length which covers every entity\n    and detail from the previous summary plus the Missing Entities.\n\n    Guidelines\n    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    - Missing entities can appear anywhere in the new summary\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n    )\n    absent: list[str] = Field(\n        ...,\n        default_factory=list,\n        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n    )\n    missing: list[str] = Field(\n        default_factory=list,\n        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n    )\n\n\n    @field_validator(\"summary\")\n    def min_length(cls, v: str):\n        tokens = nltk.word_tokenize(v)\n        num_tokens = len(tokens)\n        if num_tokens < 60:\n            raise ValueError(\n                \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\"\n            )\n        return v\n\n    @field_validator(\"missing\")\n    def has_missing_entities(cls, missing_entities: list[str]):\n        if len(missing_entities) == 0:\n            raise ValueError(\n                \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\"\n            )\n        return missing_entities\n\n    @field_validator(\"absent\")\n    def has_no_absent_entities(cls, absent_entities: list[str]):\n        absent_entity_string = \",\".join(absent_entities)\n        if len(absent_entities) > 0:\n            print(f\"Detected absent entities of {absent_entity_string}\")\n            raise ValueError(\n                f\"Do not omit the following Entities {absent_entity_string} from the new summary\"\n            )\n        return absent_entities\n\n    @field_validator(\"summary\")\n    def min_entity_density(cls, v: str):\n        tokens = nltk.word_tokenize(v)\n        num_tokens = len(tokens)\n\n        # Extract Entities\n        doc = nlp(v)\n        num_entities = len(doc.ents)\n\n        density = num_entities / num_tokens\n        if density < 0.08:\n            raise ValueError(\n                f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\"\n            )\n\n        return v\n\n```\n\nclass RewrittenSummary(BaseModel): \"\"\" This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. Guidelines - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" - Missing entities can appear anywhere in the new summary An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. \"\"\" summary: str = Field( ..., description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\", ) absent: list\\[str\\] = Field( ..., default\\_factory=list, description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\", ) missing: list\\[str\\] = Field( default\\_factory=list, description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\", ) @field\\_validator(\"summary\") def min\\_length(cls, v: str): tokens = nltk.word\\_tokenize(v) num\\_tokens = len(tokens) if num\\_tokens < 60: raise ValueError( \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\" ) return v @field\\_validator(\"missing\") def has\\_missing\\_entities(cls, missing\\_entities: list\\[str\\]): if len(missing\\_entities) == 0: raise ValueError( \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\" ) return missing\\_entities @field\\_validator(\"absent\") def has\\_no\\_absent\\_entities(cls, absent\\_entities: list\\[str\\]): absent\\_entity\\_string = \",\".join(absent\\_entities) if len(absent\\_entities) > 0: print(f\"Detected absent entities of {absent\\_entity\\_string}\") raise ValueError( f\"Do not omit the following Entities {absent\\_entity\\_string} from the new summary\" ) return absent\\_entities @field\\_validator(\"summary\") def min\\_entity\\_density(cls, v: str): tokens = nltk.word\\_tokenize(v) num\\_tokens = len(tokens) # Extract Entities doc = nlp(v) num\\_entities = len(doc.ents) density = num\\_entities / num\\_tokens if density < 0.08: raise ValueError( f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\" ) return v\n\n### Putting it all together [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/\\#putting-it-all-together)\n\nNow that we have our models, let's implement a function to summarize a piece of text using a Chain Of Density summarization\n\nIn \\[18\\]:\n\nCopied!\n\n```\nfrom openai import OpenAI\nimport instructor\n\nclient = instructor.patch(OpenAI())\n\ndef summarize_article(article: str, summary_steps: int = 3):\n    summary_chain = []\n    # We first generate an initial summary\n    summary: InitialSummary = client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        response_model=InitialSummary,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\",\\\n            },\\\n            {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": \"The generated summary should be about 80 words.\",\\\n            },\\\n        ],\n        max_retries=2,\n    )\n    prev_summary = None\n    summary_chain.append(summary.summary)\n    for _i in range(summary_steps):\n        missing_entity_message = (\n            []\n            if prev_summary is None\n            else [\\\n                {\\\n                    \"role\": \"user\",\\\n                    \"content\": f\"Please include these Missing Entities: {','.join(prev_summary.missing)}\",\\\n                },\\\n            ]\n        )\n        new_summary: RewrittenSummary = client.chat.completions.create(\n            model=\"gpt-4-1106-preview\",\n            messages=[\\\n                {\\\n                    \"role\": \"system\",\\\n                    \"content\": \"\"\"\\\n                You are going to generate an increasingly concise,entity-dense summary of the following article.\\\n\\\n                Perform the following two tasks\\\n                - Identify 1-3 informative entities from the following article which is missing from the previous summary\\\n                - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities\\\n\\\n                Guidelines\\\n                - Make every word count: re-write the previous summary to improve flow and make space for additional entities\\\n                - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\\\n                - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\\\n                - Missing entities can appear anywhere in the new summary\\\n                - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\\\n                \"\"\",\\\n                },\\\n                {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\\\n                {\\\n                    \"role\": \"user\",\\\n                    \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",\\\n                },\\\n                *missing_entity_message,\\\n            ],\n            max_retries=3,\n            max_tokens=1000,\n            response_model=RewrittenSummary,\n        )\n        summary_chain.append(new_summary.summary)\n        prev_summary = new_summary\n\n    return summary_chain\n\n```\n\nfrom openai import OpenAI import instructor client = instructor.patch(OpenAI()) def summarize\\_article(article: str, summary\\_steps: int = 3): summary\\_chain = \\[\\] # We first generate an initial summary summary: InitialSummary = client.chat.completions.create( model=\"gpt-4-1106-preview\", response\\_model=InitialSummary, messages=\\[ { \"role\": \"system\", \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": \"The generated summary should be about 80 words.\", }, \\], max\\_retries=2, ) prev\\_summary = None summary\\_chain.append(summary.summary) for \\_i in range(summary\\_steps): missing\\_entity\\_message = ( \\[\\] if prev\\_summary is None else \\[ { \"role\": \"user\", \"content\": f\"Please include these Missing Entities: {','.join(prev\\_summary.missing)}\", }, \\] ) new\\_summary: RewrittenSummary = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[ { \"role\": \"system\", \"content\": \"\"\" You are going to generate an increasingly concise,entity-dense summary of the following article. Perform the following two tasks - Identify 1-3 informative entities from the following article which is missing from the previous summary - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities Guidelines - Make every word count: re-write the previous summary to improve flow and make space for additional entities - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\". - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article. - Missing entities can appear anywhere in the new summary - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \"\"\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": f\"Here is the previous summary: {summary\\_chain\\[-1\\]}\", }, \\*missing\\_entity\\_message, \\], max\\_retries=3, max\\_tokens=1000, response\\_model=RewrittenSummary, ) summary\\_chain.append(new\\_summary.summary) prev\\_summary = new\\_summary return summary\\_chain\n\n### Trial Run [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/\\#trial-run)\n\nLet's try running this on some sample text which we can import in from our repository. We've provided a sample article in a file called `article.txt`\n\nIn \\[19\\]:\n\nCopied!\n\n```\nwith open(\"./assets/article.txt\",\"r+\") as file:\n    article = file.readline()\n\n```\n\nwith open(\"./assets/article.txt\",\"r+\") as file: article = file.readline()\n\nIn \\[ \\]:\n\nCopied!\n\n```\n%%time\n\nsummaries = summarize_article(article)\n\n```\n\n%%time summaries = summarize\\_article(article)\n\nWe can see that it took roughly 40 seconds to do an iterative chain of density using this article. But does our approach increase the density of each individual summary? We can check by calculating the entity density of each summary in our list of summaries using the `calculate_entity_density` function we defined above.\n\nIn \\[ \\]:\n\nCopied!\n\n```\nfor index,summary in enumerate(summaries):\n    tokens,entity,density = calculate_entity_density(summary)\n    print(f\"Article {index+1} -> Results (Tokens: {tokens}, Entity Count: {entity}, Density: {density})\")\n\n```\n\nfor index,summary in enumerate(summaries): tokens,entity,density = calculate\\_entity\\_density(summary) print(f\"Article {index+1} -> Results (Tokens: {tokens}, Entity Count: {entity}, Density: {density})\")\n\nWe can take a look at the articles themselves to see if they qualitatively show improvement\n\nIn \\[ \\]:\n\nCopied!\n\n```\nfor summary in summaries:\n    print(f\"\\n{summary}\\n\")\n\n```\n\nfor summary in summaries: print(f\"\\\\n{summary}\\\\n\")\n\nAs we can see, the articles progressively introduce more entities and become more entity dense. We've performed 4 rounds of summarization here but you could definitely do with maybe 2-3 if latency is a significant issue.\n\n## Future Steps [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/\\#future-steps)\n\nThis guide showed how to to generate complex summaries using chain of density summarization. We spent some time covering how to apply more complex validators - using `spaCy` and `NLTK` to ensure we had a minimum number of tokens and entity density as well as how you might apply instructor in a multi-stage process.\n\nBy building in validation at each step of the proccess, this helps to improve the performance of your LLM across various tasks.\n\nFor those looking to delve deeper, here are some to-do lists to explore.\n\n- **Validate Increasing Entity Density**: `Pydantic` exposes a more complex validator that can take in an arbitrary python dictionary. Use the validation context to check the entity density of the previous summary and the new summary to validate that our model has generated a more entity-dense rewrite\n- **Fine-Tuning** : `Instructor` comes with a simple to use interface to help you fine-tune other OpenAI models for your needs. This can be accomplished by capturing the outputs of LLMs using the `Instructions` module to generate training data for fine-tuning. In this specific case, finetuning a model to generate dense summaries could decrease latency and cost significantly by replacing the iterative LLM calls that we make .\n\nBy accomplishing these tasks, you'll gain practical experience in tuning your models to suit your specific tasks as well as build in more complex validation processes when working with LLMs to ensure more reliable, accurate and consistent outputs.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/6-chain-of-density/",
      "ogUrl": "https://python.useinstructor.com/tutorials/6-chain-of-density/",
      "title": "Chain of Density - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/6-chain-of-density/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/6-chain-of-density.png",
      "ogTitle": "Chain of Density - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/6-chain-of-density.png",
      "og:title": "Chain of Density - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/6-chain-of-density/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/6-chain-of-density.png",
      "twitter:title": "Chain of Density - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/4-validation/#validators)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/4-validation.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/4-validation.ipynb \"View source of this page\")\n\n# Validators [¶](https://python.useinstructor.com/tutorials/4-validation/\\#validators)\n\nInstead of framing \"self-critique\" or \"self-reflection\" in AI as new concepts, we can view them as validation errors with clear error messages that the system can use to self correct.\n\nPydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic [docs](https://docs.pydantic.dev/latest/) on validators.\n\nNote: For the majority of this notebook we won't be calling openai, just using validators to see how we can control the validation of the objects.\n\nValidators will enable us to control outputs by defining a function like so:\n\n```\ndef validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n\n```\n\nBefore we get started lets go over the general shape of a validator:\n\nIn \\[61\\]:\n\nCopied!\n\n```\nfrom pydantic import BaseModel\nfrom typing import Annotated\nfrom pydantic import AfterValidator\n\ndef name_must_contain_space(v: str) -> str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\nperson = UserDetail(age=29, name=\"Jason\")\n\n```\n\nfrom pydantic import BaseModel from typing import Annotated from pydantic import AfterValidator def name\\_must\\_contain\\_space(v: str) -> str: if \" \" not in v: raise ValueError(\"Name must contain a space.\") return v.lower() class UserDetail(BaseModel): age: int name: Annotated\\[str, AfterValidator(name\\_must\\_contain\\_space)\\] person = UserDetail(age=29, name=\"Jason\")\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 4 line 1\n     <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>     age: int\n     <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>     name: Annotated[str, AfterValidator(name_must_contain_space)]\n---> <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a> person = UserDetail(age=29, name=\"Jason\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for UserDetail\nname\n  Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n```\n\n**Validation Applications**\n\nValidators are essential in tackling the unpredictabile nature of LLMs.\n\nStraightforward examples include:\n\n- Flagging outputs containing blacklisted words.\n- Identifying outputs with tones like racism or violence.\n\nFor more complex tasks:\n\n- Ensuring citations directly come from provided content.\n- Checking that the model's responses align with given context.\n- Validating the syntax of SQL queries before execution.\n\n## Setup and Dependencies [¶](https://python.useinstructor.com/tutorials/4-validation/\\#setup-and-dependencies)\n\nUsing the [instructor](https://github.com/jxnl/instructor) library, we streamline the integration of these validators. `instructor` manages the parsing and validation of outputs and automates retries for compliant responses. This simplifies the process for developers to implement new validation logic, minimizing extra overhead.\n\nTo use instructor in our api calls, we just need to patch the openai client:\n\nIn \\[5\\]:\n\nCopied!\n\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\n```\n\nimport instructor from openai import OpenAI client = instructor.patch(OpenAI())\n\n## Software 2.0: Rule-based validators [¶](https://python.useinstructor.com/tutorials/4-validation/\\#software-20-rule-based-validators)\n\nDeterministic validation, characterized by its rule-based logic, ensures consistent outcomes for the same input. Let's explore how we can apply this concept through some examples.\n\n### Flagging bad keywords [¶](https://python.useinstructor.com/tutorials/4-validation/\\#flagging-bad-keywords)\n\nTo begin with, we aim to prevent engagement in topics involving explicit violence.\n\nWe will define a blacklist of violent words that cannot be mentioned in any messages:\n\nIn \\[63\\]:\n\nCopied!\n\n```\nblacklist = {\n    \"rob\",\n    \"steal\",\n    \"hurt\",\n    \"kill\",\n    \"attack\",\n}\n\n```\n\nblacklist = { \"rob\", \"steal\", \"hurt\", \"kill\", \"attack\", }\n\nTo validate if the message contains a blacklisted word we will use a [field\\_validator](https://jxnl.github.io/instructor/blog/2023/10/23/good-llm-validation-is-just-good-validation/#using-field_validator-decorator) over the 'message' field:\n\nIn \\[64\\]:\n\nCopied!\n\n```\nfrom pydantic import BaseModel, field_validator\nfrom pydantic.fields import Field\n\nclass Response(BaseModel):\n    message: str\n\n    @field_validator('message')\n    def message_cannot_have_blacklisted_words(cls, v: str) -> str:\n        for word in v.split():\n            if word.lower() in blacklist:\n                raise ValueError(f\"`{word}` was found in the message `{v}`\")\n        return v\n\nResponse(message=\"I will hurt him\")\n\n```\n\nfrom pydantic import BaseModel, field\\_validator from pydantic.fields import Field class Response(BaseModel): message: str @field\\_validator('message') def message\\_cannot\\_have\\_blacklisted\\_words(cls, v: str) -> str: for word in v.split(): if word.lower() in blacklist: raise ValueError(f\"\\`{word}\\` was found in the message \\`{v}\\`\") return v Response(message=\"I will hurt him\")\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 17 line 1\n     <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>                 raise ValueError(f\"`{word}` was found in the message `{v}`\")\n     <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>         return v\n---> <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a> Response(message=\"I will hurt him\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for Response\nmessage\n  Value error, `hurt` was found in the message `I will hurt him` [type=value_error, input_value='I will hurt him', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n```\n\n### Flagging using OpenAI Moderation [¶](https://python.useinstructor.com/tutorials/4-validation/\\#flagging-using-openai-moderation)\n\nTo enhance our validation measures, we'll extend the scope to flag any answer that contains hateful content, harassment, or similar issues. OpenAI offers a moderation endpoint that addresses these concerns, and it's freely available when using OpenAI models.\n\nWith the `instructor` library, this is just one function edit away:\n\nIn \\[1\\]:\n\nCopied!\n\n```\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\n```\n\nfrom typing import Annotated from pydantic.functional\\_validators import AfterValidator\n\nIn \\[6\\]:\n\nCopied!\n\n```\nfrom instructor import openai_moderation\n\nclass Response(BaseModel):\n    message: Annotated[str, AfterValidator(openai_moderation(client=client))]\n\n```\n\nfrom instructor import openai\\_moderation class Response(BaseModel): message: Annotated\\[str, AfterValidator(openai\\_moderation(client=client))\\]\n\nNow we have a more comprehensive flagging for violence and we can outsource the moderation of our messages.\n\nIn \\[7\\]:\n\nCopied!\n\n```\nResponse(message=\"I want to make them suffer the consequences\")\n\n```\n\nResponse(message=\"I want to make them suffer the consequences\")\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[7], line 1\n----> 1 Response(message=\"I want to make them suffer the consequences\")\n\nFile ~/.virtualenvs/pampa-labs/lib/python3.10/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for Response\nmessage\n  Value error, `I want to make them suffer the consequences` was flagged for harassment, harassment_threatening, violence, harassment/threatening [type=value_error, input_value='I want to make them suffer the consequences', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n```\n\nAnd as an extra, we get flagging for other topics like religion, race etc.\n\nIn \\[26\\]:\n\nCopied!\n\n```\nResponse(message=\"I will mock their religion\")\n\n```\n\nResponse(message=\"I will mock their religion\")\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[26], line 1\n----> 1 Response(message=\"I will mock their religion\")\n\nFile ~/.virtualenvs/pampa-labs/lib/python3.10/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for Response\nmessage\n  Value error, `I will mock their religion` was flagged for ['harassment'] [type=value_error, input_value='I will mock their religion', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n```\n\n### Filtering very long messages [¶](https://python.useinstructor.com/tutorials/4-validation/\\#filtering-very-long-messages)\n\nIn addition to content-based flags, we can also set criteria based on other aspects of the input text. For instance, to maintain user engagement, we might want to prevent the assistant from returning excessively long texts.\n\nHere, noticed that `Field` has built-in validators for `min_length` and `max_length`. to learn more checkout [Field Contraints](https://docs.pydantic.dev/latest/concepts/fields)\n\nIn \\[68\\]:\n\nCopied!\n\n```\nclass AssistantMessage(BaseModel):\n    message: str = Field(..., max_length=100)\n\n```\n\nclass AssistantMessage(BaseModel): message: str = Field(..., max\\_length=100)\n\nIn \\[69\\]:\n\nCopied!\n\n```\nAssistantMessage(message=\"Certainly! Lorem ipsum is a placeholder text commonly used in the printing and typesetting industry. Here's a sample of Lorem ipsum text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam euismod velit vel tellus tempor, non viverra eros iaculis. Sed vel nisl nec mauris bibendum tincidunt. Vestibulum sed libero euismod, eleifend tellus id, laoreet elit. Donec auctor arcu ac mi feugiat, vel lobortis justo efficitur. Fusce vel odio vitae justo varius dignissim. Integer sollicitudin mi a justo bibendum ultrices. Quisque id nisl a lectus venenatis luctus. Please note that Lorem ipsum text is a nonsensical Latin-like text used as a placeholder for content, and it has no specific meaning. It's often used in design and publishing to demonstrate the visual aspects of a document without focusing on the actual content.\")\n\n```\n\nAssistantMessage(message=\"Certainly! Lorem ipsum is a placeholder text commonly used in the printing and typesetting industry. Here's a sample of Lorem ipsum text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam euismod velit vel tellus tempor, non viverra eros iaculis. Sed vel nisl nec mauris bibendum tincidunt. Vestibulum sed libero euismod, eleifend tellus id, laoreet elit. Donec auctor arcu ac mi feugiat, vel lobortis justo efficitur. Fusce vel odio vitae justo varius dignissim. Integer sollicitudin mi a justo bibendum ultrices. Quisque id nisl a lectus venenatis luctus. Please note that Lorem ipsum text is a nonsensical Latin-like text used as a placeholder for content, and it has no specific meaning. It's often used in design and publishing to demonstrate the visual aspects of a document without focusing on the actual content.\")\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 29 line 1\n----> <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a> AssistantMessage(message=\"Certainly! Lorem ipsum is a placeholder text commonly used in the printing and typesetting industry. Here's a sample of Lorem ipsum text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam euismod velit vel tellus tempor, non viverra eros iaculis. Sed vel nisl nec mauris bibendum tincidunt. Vestibulum sed libero euismod, eleifend tellus id, laoreet elit. Donec auctor arcu ac mi feugiat, vel lobortis justo efficitur. Fusce vel odio vitae justo varius dignissim. Integer sollicitudin mi a justo bibendum ultrices. Quisque id nisl a lectus venenatis luctus. Please note that Lorem ipsum text is a nonsensical Latin-like text used as a placeholder for content, and it has no specific meaning. It's often used in design and publishing to demonstrate the visual aspects of a document without focusing on the actual content.\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for AssistantMessage\nmessage\n  String should have at most 100 characters [type=string_too_long, input_value=\"Certainly! Lorem ipsum i... on the actual content.\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/string_too_long\n```\n\n### Avoiding hallucination with citations [¶](https://python.useinstructor.com/tutorials/4-validation/\\#avoiding-hallucination-with-citations)\n\nWhen incorporating external knowledge bases, it's crucial to ensure that the agent uses the provided context accurately and doesn't fabricate responses. Validators can be effectively used for this purpose. We can illustrate this with an example where we validate that a provided citation is actually included in the referenced text chunk:\n\nIn \\[70\\]:\n\nCopied!\n\n```\nfrom pydantic import ValidationInfo\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: str\n\n    @field_validator('citation')\n    @classmethod\n    def citation_exists(cls, v: str, info: ValidationInfo):\n        context = info.context\n        if context:\n            context = context.get('text_chunk')\n            if v not in context:\n                raise ValueError(f\"Citation `{v}` not found in text\")\n        return v\n\n```\n\nfrom pydantic import ValidationInfo class AnswerWithCitation(BaseModel): answer: str citation: str @field\\_validator('citation') @classmethod def citation\\_exists(cls, v: str, info: ValidationInfo): context = info.context if context: context = context.get('text\\_chunk') if v not in context: raise ValueError(f\"Citation \\`{v}\\` not found in text\") return v\n\nHere we assume that there is a \"text\\_chunk\" field that contains the text that the model is supposed to use as context. We then use the `field_validator` decorator to define a validator that checks if the citation is included in the text chunk. If it's not, we raise a `ValueError` with a message that will be returned to the user.\n\nIn \\[71\\]:\n\nCopied!\n\n```\nAnswerWithCitation.model_validate(\n    {\n        \"answer\": \"Blueberries are packed with protein\",\n        \"citation\": \"Blueberries contain high levels of protein\"\n    },\n    context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"},\n)\n\n```\n\nAnswerWithCitation.model\\_validate( { \"answer\": \"Blueberries are packed with protein\", \"citation\": \"Blueberries contain high levels of protein\" }, context={\"text\\_chunk\": \"Blueberries are very rich in antioxidants\"}, )\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 34 line 1\n----> <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a> AnswerWithCitation.model_validate(\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>     {\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>         \"answer\": \"Blueberries are packed with protein\",\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>         \"citation\": \"Blueberries contain high levels of protein\"\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>     },\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=5'>6</a>     context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"},\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a> )\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:503, in BaseModel.model_validate(cls, obj, strict, from_attributes, context)\n    501 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    502 __tracebackhide__ = True\n--> 503 return cls.__pydantic_validator__.validate_python(\n    504     obj, strict=strict, from_attributes=from_attributes, context=context\n    505 )\n\nValidationError: 1 validation error for AnswerWithCitation\ncitation\n  Value error, Citation `Blueberries contain high levels of protein` not found in text [type=value_error, input_value='Blueberries contain high levels of protein', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n```\n\n## Software 3.0: Probabilistic validators [¶](https://python.useinstructor.com/tutorials/4-validation/\\#software-30-probabilistic-validators)\n\nFor scenarios requiring more nuanced validation than rule-based methods, we use probabilistic validation. This approach incorporates LLMs into the validation workflow for a sophisticated assessment of outputs.\n\nThe `instructor` library offers the `llm_validator` utility for this purpose. By specifying the desired directive, we can use LLMs for complex validation tasks. Let's explore some intriguing use cases enabled by LLMs.\n\n### Keeping an agent on topic [¶](https://python.useinstructor.com/tutorials/4-validation/\\#keeping-an-agent-on-topic)\n\nWhen creating an agent focused on health improvement, providing answers and daily practice suggestions, it's crucial to ensure strict adherence to health-related topics. This is important because the knowledge base is limited to health topics, and veering off-topic could result in fabricated responses.\n\nTo achieve this focus, we'll follow a similar process as before, but with an important addition: integrating an LLM into our validator.\n\nThis LLM will be tasked with determining whether the agent's responses are exclusively related to health topics. For this, we will use the `llm_validator` from `instructor` like so:\n\nIn \\[73\\]:\n\nCopied!\n\n```\nfrom instructor import llm_validator\n\nclass AssistantMessage(BaseModel):\n    message: Annotated[str,\\\n                       AfterValidator(\\\n                           llm_validator(\"don't talk about any other topic except health best practices and topics\",\\\n                                         client=client))]\n\nAssistantMessage(message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\")\n\n```\n\nfrom instructor import llm\\_validator class AssistantMessage(BaseModel): message: Annotated\\[str, AfterValidator( llm\\_validator(\"don't talk about any other topic except health best practices and topics\", client=client))\\] AssistantMessage(message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\")\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 38 line 1\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a> class AssistantMessage(BaseModel):\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>     message: Annotated[str,\\\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>                        AfterValidator(\\\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>                            llm_validator(\"don't talk about any other topic except health best practices and topics\",\\\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>                                          openai_client=client))]\n---> <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a> AssistantMessage(message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for AssistantMessage\nmessage\n  Assertion failed, The statement is not related to health best practices or topics. [type=assertion_error, input_value='I would suggest you to v...is very nice in winter.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/assertion_error\n```\n\nImportant that for these examples we're not waiting for the messages, to get this message we would need to call the openai with `response_model=AssistantMessage`.\n\n### Validating agent thinking with CoT [¶](https://python.useinstructor.com/tutorials/4-validation/\\#validating-agent-thinking-with-cot)\n\nUsing probabilistic validation, we can also assess the agent's reasoning process to ensure it's logical before providing a response. With [chain of thought](https://learnprompting.org/docs/intermediate/chain_of_thought) prompting, the model is expected to think in steps and arrive at an answer following its logical progression. If there are errors in this logic, the final response may be incorrect.\n\nHere we will use Pydantic's [model\\_validator](https://docs.pydantic.dev/latest/concepts/validators/#model-validators) which allows us to apply validation over all the properties of the `AIResponse` at once.\n\nTo make this easier we'll make a simple validation class that we can reuse for all our validation:\n\nIn \\[74\\]:\n\nCopied!\n\n```\nfrom typing import Optional\n\nclass Validation(BaseModel):\n    is_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\")\n    error_message: Optional[str] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\")\n\n```\n\nfrom typing import Optional class Validation(BaseModel): is\\_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\") error\\_message: Optional\\[str\\] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\")\n\nThe function we will call will integrate an LLM and will ask it to determine whether the answer the model provided follows from the chain of thought:\n\nIn \\[75\\]:\n\nCopied!\n\n```\ndef validate_chain_of_thought(values):\n    chain_of_thought = values[\"chain_of_thought\"]\n    answer = values[\"answer\"]\n    resp = client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"You are a validator. Determine if the value follows from the statement. If it is not, explain why.\",\\\n            },\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"Verify that `{answer}` follows the chain of thought: {chain_of_thought}\",\\\n            },\\\n        ],\n        response_model=Validation,\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return values\n\n```\n\ndef validate\\_chain\\_of\\_thought(values): chain\\_of\\_thought = values\\[\"chain\\_of\\_thought\"\\] answer = values\\[\"answer\"\\] resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[ { \"role\": \"system\", \"content\": \"You are a validator. Determine if the value follows from the statement. If it is not, explain why.\", }, { \"role\": \"user\", \"content\": f\"Verify that \\`{answer}\\` follows the chain of thought: {chain\\_of\\_thought}\", }, \\], response\\_model=Validation, ) if not resp.is\\_valid: raise ValueError(resp.error\\_message) return values\n\nThe use of the 'before' argument in this context is significant. It means that the validator will receive the complete dictionary of inputs in their raw form, before any parsing by Pydantic.\n\nIn \\[76\\]:\n\nCopied!\n\n```\nfrom typing import Any\nfrom pydantic import model_validator\n\nclass AIResponse(BaseModel):\n    chain_of_thought: str\n    answer: str\n\n    @model_validator(mode='before')\n    @classmethod\n    def chain_of_thought_makes_sense(cls, data: Any) -> Any:\n        # here we assume data is the dict representation of the model\n        # since we use 'before' mode.\n        return validate_chain_of_thought(data)\n\n```\n\nfrom typing import Any from pydantic import model\\_validator class AIResponse(BaseModel): chain\\_of\\_thought: str answer: str @model\\_validator(mode='before') @classmethod def chain\\_of\\_thought\\_makes\\_sense(cls, data: Any) -> Any: # here we assume data is the dict representation of the model # since we use 'before' mode. return validate\\_chain\\_of\\_thought(data)\n\nIn \\[77\\]:\n\nCopied!\n\n```\nAIResponse(chain_of_thought=\"The user suffers from diabetes.\", answer=\"The user has a broken leg.\")\n\n```\n\nAIResponse(chain\\_of\\_thought=\"The user suffers from diabetes.\", answer=\"The user has a broken leg.\")\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 47 line 1\n----> <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a> AIResponse(chain_of_thought=\"The user suffers from diabetes.\", answer=\"The user has a broken leg.\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for AIResponse\n  Value error, The statement about the user having a broken leg does not logically follow from the information provided about the user suffering from diabetes. These are two separate health conditions and one does not imply the other. [type=value_error, input_value={'chain_of_thought': 'The...user has a broken leg.'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n```\n\n## Reasking with validators [¶](https://python.useinstructor.com/tutorials/4-validation/\\#reasking-with-validators)\n\nFor most of these examples all we've done we've mostly only defined the validation logic.\n\nWe'eve covered field validators and model validators and even used LLMs to validate our outputs. But we haven't actually used the validators to reask the model! One of the most powerful features of `instructor` is that it will automatically reask the model when it receives a validation error. This means that we can use the same validation logic for both code-based and LLM-based validation.\n\nThis also means that our 'prompt' is not only the prompt we send, but the code that runs the validator, and the error message we send back to the model.\n\nIntegrating these validation examples with the OpenAI API is streamlined using `instructor`. After patching the OpenAI client with `instructor`, you simply need to specify a `response_model` for your requests. This setup ensures that all the validation processes occur automatically.\n\nTo enable reasking you can set a maximum number of retries. When calling the OpenAI client, the system can re-attempt to generate a correct answer. It does this by resending the original query along with feedback on why the previous response was rejected, guiding the LLM towards a more accurate answer in subsequent attempts.\n\nIn \\[79\\]:\n\nCopied!\n\n```\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n\nquestion = \"What is the meaning of life?\"\ncontext = \"The according to the devil the meaning of life is a life of sin and debauchery.\"\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=QuestionAnswer,\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\\\n        },\\\n    ],\n)\n\nresp.answer\n\n```\n\nclass QuestionAnswer(BaseModel): question: str answer: str question = \"What is the meaning of life?\" context = \"The according to the devil the meaning of life is a life of sin and debauchery.\" resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", response\\_model=QuestionAnswer, messages=\\[ { \"role\": \"system\", \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\", }, { \"role\": \"user\", \"content\": f\"using the context: \\`{context}\\`\\\\n\\\\nAnswer the following question: \\`{question}\\`\", }, \\], ) resp.answer\n\nOut\\[79\\]:\n\n```\n'a life of sin and debauchery'\n```\n\nIn \\[80\\]:\n\nCopied!\n\n```\nfrom pydantic import BeforeValidator\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\\\n        str,\\\n        BeforeValidator(\\\n            llm_validator(\"don't say objectionable things\", client=client)\\\n        ),\\\n    ]\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    max_retries=2,\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\\\n        },\\\n    ],\n)\n\nresp.answer\n\n```\n\nfrom pydantic import BeforeValidator class QuestionAnswer(BaseModel): question: str answer: Annotated\\[ str, BeforeValidator( llm\\_validator(\"don't say objectionable things\", client=client) ), \\] resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\_model=QuestionAnswer, max\\_retries=2, messages=\\[ { \"role\": \"system\", \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\", }, { \"role\": \"user\", \"content\": f\"using the context: \\`{context}\\`\\\\n\\\\nAnswer the following question: \\`{question}\\`\", }, \\], ) resp.answer\n\nOut\\[80\\]:\n\n```\n'The meaning of life is a concept that varies depending on individual perspectives and beliefs.'\n```\n\n# Conclusion [¶](https://python.useinstructor.com/tutorials/4-validation/\\#conclusion)\n\nThis guide explains how to use deterministic and probabilistic validation techniques with Large Language Models (LLMs). We discussed using an instructor to establish validation processes for content filtering, context relevance maintenance, and model reasoning verification. These methods enhance the performance of LLMs across different tasks.\n\nFor those interested in further exploration, here's a to-do list:\n\n1. **SQL Syntax Checker**: Create a validator to check the syntax of SQL queries before executing them.\n2. **Context-Based Response Validation**: Design a method to flag responses based on the model's own knowledge rather than the provided context.\n3. **PII Detection**: Implement a mechanism to identify and handle Personally Identifiable Information in responses while prioritizing user privacy.\n4. **Targeted Rule-Based Filtering**: Develop filters to remove specific content types, such as responses mentioning named entities.\n\nCompleting these tasks will enable users to acquire practical skills in improving LLMs through advanced validation methods.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/4-validation/",
      "ogUrl": "https://python.useinstructor.com/tutorials/4-validation/",
      "title": "Validation - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/4-validation/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/4-validation.png",
      "ogTitle": "Validation - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/4-validation.png",
      "og:title": "Validation - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/4-validation/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/4-validation.png",
      "twitter:title": "Validation - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/#synthetic-data-generation)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/7-synthetic-data-generation.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/7-synthetic-data-generation.ipynb \"View source of this page\")\n\n# Synthetic Data Generation [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/\\#synthetic-data-generation)\n\nRAG Applications are often tricky to evaluate, especially when you haven't obtained any user queries to begin. In this notebook, we'll see how we can use `instructor` to quickly generate synthetic questions from a dataset to benchmark your retrieval systems using some simple metrics.\n\n## Data Ingestion [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/\\#data-ingestion)\n\nLet's first start by installing the required packages and ingesting the first 200 rows of the `ms-marco` dataset into our local database.\n\nIn \\[91\\]:\n\nCopied!\n\n```\n!uv pip install instructor openai datasets lancedb tantivy tenacity tqdm\n\n```\n\n!uv pip install instructor openai datasets lancedb tantivy tenacity tqdm\n\n```\nAudited 7 packages in 301ms\n\n```\n\nWe're using `lancedb` here to easily ingest large amounts of data. This is preferable since we can define our table schema using a `Pydantic` Schema and also have LanceDB automatically handle the generation of the embeddings using their `get_registry()` method that we can define as an object property.\n\nIn \\[6\\]:\n\nCopied!\n\n```\nfrom lancedb import connect\n\nDB_PATH = \"./db\"\nDB_TABLE = \"ms_marco\"\n\n# Create a db at the path `./db`\ndb = connect(DB_PATH)\n\n```\n\nfrom lancedb import connect DB\\_PATH = \"./db\" DB\\_TABLE = \"ms\\_marco\" # Create a db at the path \\`./db\\` db = connect(DB\\_PATH)\n\nIn \\[31\\]:\n\nCopied!\n\n```\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\nfunc = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n\nclass Chunk(LanceModel):\n    passage:str = func.SourceField()\n    chunk_id:str\n    embedding:Vector(func.ndims()) = func.VectorField()\n\ntable = db.create_table(DB_TABLE, schema=Chunk, exist_ok=True, mode=\"overwrite\")\n\n```\n\nfrom lancedb.pydantic import LanceModel, Vector from lancedb.embeddings import get\\_registry func = get\\_registry().get(\"openai\").create(name=\"text-embedding-3-small\") class Chunk(LanceModel): passage:str = func.SourceField() chunk\\_id:str embedding:Vector(func.ndims()) = func.VectorField() table = db.create\\_table(DB\\_TABLE, schema=Chunk, exist\\_ok=True, mode=\"overwrite\")\n\nIn \\[32\\]:\n\nCopied!\n\n```\nfrom datasets import load_dataset\n\nN_ROWS = 200\n\ndataset = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\", streaming=True).take(N_ROWS)\n\n```\n\nfrom datasets import load\\_dataset N\\_ROWS = 200 dataset = load\\_dataset(\"ms\\_marco\", \"v1.1\", split=\"train\", streaming=True).take(N\\_ROWS)\n\nIn \\[33\\]:\n\nCopied!\n\n```\n# from itertools import islice\nfirst_item = next(iter(dataset))\nfirst_item.keys()\n\n```\n\n\\# from itertools import islice first\\_item = next(iter(dataset)) first\\_item.keys()\n\nOut\\[33\\]:\n\n```\ndict_keys(['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'])\n```\n\nIn \\[36\\]:\n\nCopied!\n\n```\nfirst_item['passages']['passage_text'][:3]\n\n```\n\nfirst\\_item\\['passages'\\]\\['passage\\_text'\\]\\[:3\\]\n\nOut\\[36\\]:\n\n```\n[\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\\\n \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\\\n 'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ']\n```\n\nIn \\[34\\]:\n\nCopied!\n\n```\nimport hashlib\nfrom itertools import batched\n\ndef get_passages(dataset):\n    for row in dataset:\n        for passage in row['passages']['passage_text']:\n            yield {\n                \"passage\":passage,\n                \"chunk_id\":hashlib.md5(passage.encode()).hexdigest()\n            }\n\npassages = batched(get_passages(dataset),10)\n\nfor passage_batch in passages:\n    # print(passage_batch)\n    table.add(list(passage_batch))\n\n```\n\nimport hashlib from itertools import batched def get\\_passages(dataset): for row in dataset: for passage in row\\['passages'\\]\\['passage\\_text'\\]: yield { \"passage\":passage, \"chunk\\_id\":hashlib.md5(passage.encode()).hexdigest() } passages = batched(get\\_passages(dataset),10) for passage\\_batch in passages: # print(passage\\_batch) table.add(list(passage\\_batch))\n\n## Synthetic Questions [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/\\#synthetic-questions)\n\nNow that we have the first ~2000 passages from the MS-Marco dataset ingested into our database. Let's start generating some synthetic questions using the chunks we've ingested.\n\nLet's see how we might do so using `instructor` by defining a datamodel that can help support this use-case.\n\nIn \\[35\\]:\n\nCopied!\n\n```\nfrom pydantic import BaseModel,Field\n\nclass QuestionAnswerPair(BaseModel):\n    \"\"\"\n    This model represents a pair of a question generated from a text chunk, its corresponding answer,\n    and the chain of thought leading to the answer. The chain of thought provides insight into how the answer\n    was derived from the question.\n    \"\"\"\n\n    chain_of_thought: str = Field(\n        description=\"The reasoning process leading to the answer.\"\n    )\n    question: str = Field(\n        description=\"The generated question from the text chunk.\"\n    )\n    answer: str = Field(description=\"The answer to the generated question.\")\n\n```\n\nfrom pydantic import BaseModel,Field class QuestionAnswerPair(BaseModel): \"\"\" This model represents a pair of a question generated from a text chunk, its corresponding answer, and the chain of thought leading to the answer. The chain of thought provides insight into how the answer was derived from the question. \"\"\" chain\\_of\\_thought: str = Field( description=\"The reasoning process leading to the answer.\" ) question: str = Field( description=\"The generated question from the text chunk.\" ) answer: str = Field(description=\"The answer to the generated question.\")\n\nOnce we've defined this data-model, we can then use it in an instructor call to generate a synthetic question.\n\nIn \\[42\\]:\n\nCopied!\n\n```\nfrom openai import OpenAI\nfrom instructor import from_openai\n\nclient = from_openai(OpenAI())\n\ndef generate_question(chunk:str)->QuestionAnswerPair:\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\",\\\n            },\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"Here is the text chunk: {chunk}\"\\\n            }\\\n        ],\n        response_model=QuestionAnswerPair\n    )\n\ntext_chunk = \"\"\"\nThe Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\n\"\"\"\nprint(generate_question(text_chunk).model_dump_json(indent=2))\n\n```\n\nfrom openai import OpenAI from instructor import from\\_openai client = from\\_openai(OpenAI()) def generate\\_question(chunk:str)->QuestionAnswerPair: return client.chat.completions.create( model=\"gpt-4o\", messages=\\[ { \"role\": \"system\", \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\", }, { \"role\": \"user\", \"content\": f\"Here is the text chunk: {chunk}\" } \\], response\\_model=QuestionAnswerPair ) text\\_chunk = \"\"\" The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site. \"\"\" print(generate\\_question(text\\_chunk).model\\_dump\\_json(indent=2))\n\n```\n{\n  \"chain_of_thought\": \"To form a specific question from the given text chunk, I should focus on the unique details provided about the Reserve Bank of Australia, such as its creation, functions, and assets.\",\n  \"question\": \"When was the Reserve Bank of Australia established as Australia's central bank and banknote issuing authority?\",\n  \"answer\": \"The Reserve Bank of Australia was established as Australia's central bank and banknote issuing authority on 14 January 1960.\"\n}\n\n```\n\nNow that we've seen how to generate a single question, let's see how we might be able to scale this up. We can do so by taking advantage of the `asyncio` library and `tenacity` to handle retries.\n\nIn \\[56\\]:\n\nCopied!\n\n```\nchunks = table.to_pandas()\nchunks = [item for item in chunks['passage']]\nchunks[:2]\n\n```\n\nchunks = table.to\\_pandas() chunks = \\[item for item in chunks\\['passage'\\]\\] chunks\\[:2\\]\n\nOut\\[56\\]:\n\n```\n[\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\\\n \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\"]\n```\n\nIn \\[98\\]:\n\nCopied!\n\n```\nfrom asyncio import Semaphore\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom openai import AsyncOpenAI\nimport asyncio\n\nclient = from_openai(AsyncOpenAI())\n\nasync def generate_questions(chunks:list[str],max_queries:int):\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    async def generate_question(chunk:str,sem:Semaphore)->tuple[QuestionAnswerPair,str]:\n        async with sem:\n            return (await client.chat.completions.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[\\\n                    {\\\n                        \"role\": \"system\",\\\n                        \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\",\\\n                    },\\\n                    {\\\n                        \"role\": \"user\",\\\n                        \"content\": f\"Here is the text chunk: {chunk}\"\\\n                    }\\\n                ],\n                response_model=QuestionAnswerPair\n            ), chunk)\n    sem = Semaphore(max_queries)\n    coros = [\\\n        generate_question(chunk,sem)\\\n        for chunk in\\\n        chunks\\\n    ]\n    return await asyncio.gather(*coros)\n\nquestions = await generate_questions(chunks[:300],10)\n\n```\n\nfrom asyncio import Semaphore from tenacity import retry, stop\\_after\\_attempt, wait\\_exponential from openai import AsyncOpenAI import asyncio client = from\\_openai(AsyncOpenAI()) async def generate\\_questions(chunks:list\\[str\\],max\\_queries:int): @retry(stop=stop\\_after\\_attempt(3), wait=wait\\_exponential(multiplier=1, min=4, max=10)) async def generate\\_question(chunk:str,sem:Semaphore)->tuple\\[QuestionAnswerPair,str\\]: async with sem: return (await client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\[ { \"role\": \"system\", \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\", }, { \"role\": \"user\", \"content\": f\"Here is the text chunk: {chunk}\" } \\], response\\_model=QuestionAnswerPair ), chunk) sem = Semaphore(max\\_queries) coros = \\[ generate\\_question(chunk,sem) for chunk in chunks \\] return await asyncio.gather(\\*coros) questions = await generate\\_questions(chunks\\[:300\\],10)\n\n## Benchmarking Retrieval [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/\\#benchmarking-retrieval)\n\nNow that we've generated a list of questions to query our database with, let's do a quick benchmark to see how full text search compares against that of hybrid search. We'll use two simple metrics here - Mean Reciprocal Rank ( MRR ) and Recall.\n\nLet's start by making sure we have an inverted index created on our table above that we can perform full text search on\n\nIn \\[64\\]:\n\nCopied!\n\n```\ntable.create_fts_index(\"passage\",replace=True)\n\n```\n\ntable.create\\_fts\\_index(\"passage\",replace=True)\n\nThis allows us to then use the `.search` function on each table to query it using full text search. Let's see an example below.\n\nIn \\[67\\]:\n\nCopied!\n\n```\nfor entry in table.search(\"RBA\",query_type=\"fts\").limit(2).to_list():\n    print(entry['passage'])\n\n```\n\nfor entry in table.search(\"RBA\",query\\_type=\"fts\").limit(2).to\\_list(): print(entry\\['passage'\\])\n\n```\nA rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.\nResults-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;\n\n```\n\n### Metrics [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/\\#metrics)\n\nNow that we've figured out how we might be able to query our table using full text search. Let's take a step back and see how we can implement some metrics to quantiatively evaluate the retrieved items. It's important to note that when we want to evalute the quality of our listings, we always take it at some subset of k.\n\nThis is important because k is often constrained by a business outcome and can help us determine how well our solution works\n\nEg. Here are some hypothetical scenarios\n\n- k=5 : We'd like to display some recomended items based of a user query (Eg. Help me plan out a dinner with Jonathan next week -> Display 5 possible actions)\n- k=10 : We have a small carousel with recomended items for a user to buy\n- k=25 : We're using a re-ranker, is it filtering out the irrelevant chunks from the relevant chunks well?\n- k=50 : We have a pipeline that fetches information for a model to respond with, are we fetching all relevant bits of information\n\n#### Reciprocal Rank [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/\\#reciprocal-rank)\n\nReciprocal Rank Imagine we're spotify and we want to suggest a couple of songs to the user. Which is a better result among the two lists of retrieved songs below? ( Note that 2 is the answer we want )\n\n- \\[0,1,2,3,4\\]\n- \\[0,1,3,4,2\\]\n\nObviously if we're suggesting songs to the user, we want the first relevant song to be listed as early as possible! Therefore we'd prefer 1 over 2 in the example above because 2 is ordered earlier in the first case. A metric that works well for this is the Reciprocal Rank (RR).\n\n![](https://python.useinstructor.com/tutorials/img/mrr_eqn.png)\n\nIn \\[84\\]:\n\nCopied!\n\n```\ndef rr(results,labels):\n    return max(\n        [round(1/(results.index(label)+1),2) if label in results else 0\\\n        for label in labels]\n\n    )\n\n```\n\ndef rr(results,labels): return max( \\[round(1/(results.index(label)+1),2) if label in results else 0 for label in labels\\] )\n\nThis is an aggressive metric and once we get to an position of > 10, the value doesn't change much anymore. Most of the big changes happen at indexes < 10.\n\n#### Recall [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/\\#recall)\n\nAnother metric that we can track is recall which measures how many of our retrieved items were retrieved.\n\n![](https://python.useinstructor.com/tutorials/img/recall_eqn.png)\n\nIn \\[69\\]:\n\nCopied!\n\n```\ndef recall(results,relevant_chunks):\n    return sum([1 if chunk in results else 0 for chunk in relevant_chunks])/len(relevant_chunks)\n\n```\n\ndef recall(results,relevant\\_chunks): return sum(\\[1 if chunk in results else 0 for chunk in relevant\\_chunks\\])/len(relevant\\_chunks)\n\n## Using Our Questions [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/\\#using-our-questions)\n\nNow that we've seen two metrics that we can use and how we might be able to generate some synthetic questions, let's try it out on an actual question.\n\nTo do so, we'll first generate a unique chunk id for our original passage that we generated the question from.\n\nWe'll then compare the chunk\\_ids of the retrieved chunks and then compute the `mrr` and the `recall` of the retrieved results.\n\nIn \\[86\\]:\n\nCopied!\n\n```\nimport hashlib\nsample_question,chunk = questions[0]\n\nchunk_id = hashlib.md5(chunk.encode()).hexdigest()\nchunk_id, sample_question.question, chunk\n\n```\n\nimport hashlib sample\\_question,chunk = questions\\[0\\] chunk\\_id = hashlib.md5(chunk.encode()).hexdigest() chunk\\_id, sample\\_question.question, chunk\n\nOut\\[86\\]:\n\n```\n('b6d9bf888fd53590ee69a913bd9bf8a4',\n \"What factors influence the average salary for people with a bachelor's degree?\",\n \"However, the average salary for people with a bachelor's degree varies widely based upon several factors, including their major, job position, location and years of experience. The National Association of Colleges and Employers conducted a salary survey that determined the average starting salary for graduates of various bachelor's degree programs.\")\n```\n\nIn \\[81\\]:\n\nCopied!\n\n```\nretrieved_results = table.search(sample_question.question,query_type='fts').limit(25).to_list()\nretrieved_chunk_ids = [item['chunk_id'] for item in retrieved_results]\n\nretrieved_chunk_ids[:3]\n\n```\n\nretrieved\\_results = table.search(sample\\_question.question,query\\_type='fts').limit(25).to\\_list() retrieved\\_chunk\\_ids = \\[item\\['chunk\\_id'\\] for item in retrieved\\_results\\] retrieved\\_chunk\\_ids\\[:3\\]\n\nOut\\[81\\]:\n\n```\n['b6d9bf888fd53590ee69a913bd9bf8a4',\\\n '7a0254c9dc709220367857dcb67f2c8d',\\\n '04e7e6f91463033aa87b4104ea16b477']\n```\n\nWe can now compute the results for the retrieved items that we've obtained using full text search relative to the ground truth label that we have - the original chunk that we generated it from\n\nIn \\[85\\]:\n\nCopied!\n\n```\nrecall(retrieved_chunk_ids,[chunk_id]), rr(retrieved_chunk_ids,[chunk_id])\n\n```\n\nrecall(retrieved\\_chunk\\_ids,\\[chunk\\_id\\]), rr(retrieved\\_chunk\\_ids,\\[chunk\\_id\\])\n\nOut\\[85\\]:\n\n```\n(1.0, 1.0)\n```\n\nScaling it up for different values of `k`, where we can see how this value changes for different subsets of the retrieved items is relatively simple.\n\nWe can generate this mapping automatically using `itertools.product`\n\nIn \\[112\\]:\n\nCopied!\n\n```\nfrom itertools import product\n\nSIZES = [3,5,10,15,25]\nMETRICS = [\\\n    [\"mrr\",rr],\\\n    [\"recall\",recall]\\\n]\n\nscore_fns = {}\n\nfor metric,size in product(METRICS,SIZES):\n    metric_name, score_fn = metric\n    score_fns[f\"{metric_name}@{size}\"] = lambda predictions,labels, fn=score_fn, k=size: fn(predictions[:k],labels) # type: ignore\n\n```\n\nfrom itertools import product SIZES = \\[3,5,10,15,25\\] METRICS = \\[ \\[\"mrr\",rr\\], \\[\"recall\",recall\\] \\] score\\_fns = {} for metric,size in product(METRICS,SIZES): metric\\_name, score\\_fn = metric score\\_fns\\[f\"{metric\\_name}@{size}\"\\] = lambda predictions,labels, fn=score\\_fn, k=size: fn(predictions\\[:k\\],labels) # type: ignore\n\n## Running an Evaluation [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/\\#running-an-evaluation)\n\nWe can now use the code above to run a test to see how our full text search performs for our synthetic questions.\n\nIn \\[114\\]:\n\nCopied!\n\n```\nimport hashlib\nfrom tqdm import tqdm\n\nfts_results = []\n\nfor sample_qn, chunk in tqdm(questions):\n    chunk_id = hashlib.md5(chunk.encode()).hexdigest()\n    cleaned_question = ''.join(char for char in sample_qn.question if char.isalnum() or char.isspace())\n    retrieved_results = table.search(cleaned_question, query_type='fts').limit(25).to_list()\n    retrieved_chunk_ids = [item['chunk_id'] for item in retrieved_results]\n\n    fts_results.append(\n        {\n            metric: score_fn(retrieved_chunk_ids,[chunk_id])\n            for metric,score_fn\n            in score_fns.items()\n        }\n    )\n\n```\n\nimport hashlib from tqdm import tqdm fts\\_results = \\[\\] for sample\\_qn, chunk in tqdm(questions): chunk\\_id = hashlib.md5(chunk.encode()).hexdigest() cleaned\\_question = ''.join(char for char in sample\\_qn.question if char.isalnum() or char.isspace()) retrieved\\_results = table.search(cleaned\\_question, query\\_type='fts').limit(25).to\\_list() retrieved\\_chunk\\_ids = \\[item\\['chunk\\_id'\\] for item in retrieved\\_results\\] fts\\_results.append( { metric: score\\_fn(retrieved\\_chunk\\_ids,\\[chunk\\_id\\]) for metric,score\\_fn in score\\_fns.items() } )\n\n```\n100%|██████████| 300/300 [00:07<00:00, 41.64it/s]\n\n```\n\nIn \\[115\\]:\n\nCopied!\n\n```\nimport pandas as pd\n\ndf = pd.DataFrame(fts_results)\ndf.mean()\n\n```\n\nimport pandas as pd df = pd.DataFrame(fts\\_results) df.mean()\n\nOut\\[115\\]:\n\n```\nmrr@3        0.784267\nmrr@5        0.791267\nmrr@10       0.797633\nmrr@15       0.798133\nmrr@25       0.798433\nrecall@3     0.896667\nrecall@5     0.926667\nrecall@10    0.973333\nrecall@15    0.980000\nrecall@25    0.986667\ndtype: float64\n```\n\nWe can see that on average full text search is able to surface the relevant item 97-98% of the time if we take `k=10` and that we have the relevant item in between the first and second item here.\n\nNow, because these are synthetic question, there's likely to be a large amount of overlap in the phrases used in the questions and the original source text, leading to the high values.\n\nIn actual production applications and your domain specific dataset, it's useful to do these experiments and see what works best for your needs.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/7-synthetic-data-generation/",
      "ogUrl": "https://python.useinstructor.com/tutorials/7-synthetic-data-generation/",
      "title": "Synthetic Data Generation - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/7-synthetic-data-generation/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/7-synthetic-data-generation.png",
      "ogTitle": "Synthetic Data Generation - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/7-synthetic-data-generation.png",
      "og:title": "Synthetic Data Generation - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/7-synthetic-data-generation/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/7-synthetic-data-generation.png",
      "twitter:title": "Synthetic Data Generation - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/2-tips/#general-tips-on-prompting)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/2-tips.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/2-tips.ipynb \"View source of this page\")\n\n# General Tips on Prompting [¶](https://python.useinstructor.com/tutorials/2-tips/\\#general-tips-on-prompting)\n\nBefore we get into some big applications of schema engineering I want to equip you with the tools for success. This notebook is to share some general advice when using prompts to get the most of your models.\n\nBefore you might think of prompt engineering as massaging this wall of text, almost like coding in a notepad. But with schema engineering you can get a lot more out of your prompts with a lot less work.\n\n## Classification [¶](https://python.useinstructor.com/tutorials/2-tips/\\#classification)\n\nFor classification we've found theres generally two methods of modeling.\n\n1. using Enums\n2. using Literals\n\nUse an enum in Python when you need a set of named constants that are related and you want to ensure type safety, readability, and prevent invalid values. Enums are helpful for grouping and iterating over these constants.\n\nUse literals when you have a small, unchanging set of values that you don't need to group or iterate over, and when type safety and preventing invalid values is less of a concern. Literals are simpler and more direct for basic, one-off values.\n\nIn \\[1\\]:\n\nCopied!\n\n```\nimport instructor\nfrom openai import OpenAI\n\nfrom enum import Enum\nfrom pydantic import BaseModel, Field\nfrom typing_extensions import Literal\n\nclient = instructor.patch(OpenAI())\n\n# Tip: Do not use auto() as they cast to 1,2,3,4\nclass House(Enum):\n    Gryffindor = \"gryffindor\"\n    Hufflepuff = \"hufflepuff\"\n    Ravenclaw = \"ravenclaw\"\n    Slytherin = \"slytherin\"\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: House\n\n    def say_hello(self):\n        print(\n            f\"Hello, I'm {self.name}, I'm {self.age} years old and I'm from {self.house.value.title()}\"\n        )\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n\n```\n\nimport instructor from openai import OpenAI from enum import Enum from pydantic import BaseModel, Field from typing\\_extensions import Literal client = instructor.patch(OpenAI()) # Tip: Do not use auto() as they cast to 1,2,3,4 class House(Enum): Gryffindor = \"gryffindor\" Hufflepuff = \"hufflepuff\" Ravenclaw = \"ravenclaw\" Slytherin = \"slytherin\" class Character(BaseModel): age: int name: str house: House def say\\_hello(self): print( f\"Hello, I'm {self.name}, I'm {self.age} years old and I'm from {self.house.value.title()}\" ) resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[{\"role\": \"user\", \"content\": \"Harry Potter\"}\\], response\\_model=Character, ) resp.model\\_dump()\n\nOut\\[1\\]:\n\n```\n{'age': 17, 'name': 'Harry Potter', 'house': <House.Gryffindor: 'gryffindor'>}\n```\n\nIn \\[2\\]:\n\nCopied!\n\n```\nresp.say_hello()\n\n```\n\nresp.say\\_hello()\n\n```\nHello, I'm Harry Potter, I'm 17 years old and I'm from Gryffindor\n\n```\n\nIn \\[3\\]:\n\nCopied!\n\n```\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n\n```\n\nclass Character(BaseModel): age: int name: str house: Literal\\[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"\\] resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[{\"role\": \"user\", \"content\": \"Harry Potter\"}\\], response\\_model=Character, ) resp.model\\_dump()\n\nOut\\[3\\]:\n\n```\n{'age': 11, 'name': 'Harry Potter', 'house': 'Gryffindor'}\n```\n\n## Arbitrary properties [¶](https://python.useinstructor.com/tutorials/2-tips/\\#arbitrary-properties)\n\nOften times there are long properties that you might want to extract from data that we can not specify in advanced. We can get around this by defining an arbitrary key value store like so:\n\nIn \\[4\\]:\n\nCopied!\n\n```\nclass Property(BaseModel):\n    key: str = Field(description=\"Must be snake case\")\n    value: str\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n    properties: list[Property]\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n\n```\n\nclass Property(BaseModel): key: str = Field(description=\"Must be snake case\") value: str class Character(BaseModel): age: int name: str house: Literal\\[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"\\] properties: list\\[Property\\] resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}\\], response\\_model=Character, ) resp.model\\_dump()\n\nOut\\[4\\]:\n\n```\n{'age': 38,\n 'name': 'Severus Snape',\n 'house': 'Slytherin',\n 'properties': [{'key': 'role', 'value': 'Potions Master'},\\\n  {'key': 'patronus', 'value': 'Doe'},\\\n  {'key': 'loyalty', 'value': 'Dumbledore'},\\\n  {'key': 'played_by', 'value': 'Alan Rickman'}]}\n```\n\n## Limiting the length of lists [¶](https://python.useinstructor.com/tutorials/2-tips/\\#limiting-the-length-of-lists)\n\nIn later chapters we'll talk about how to use validators to assert the length of lists but we can also use prompting tricks to enumerate values. Here we'll define a index to count the properties.\n\nIn this following example instead of extraction we're going to work on generation instead.\n\nIn \\[5\\]:\n\nCopied!\n\n```\nclass Property(BaseModel):\n    index: str = Field(..., description=\"Monotonically increasing ID\")\n    key: str = Field(description=\"Must be snake case\")\n    value: str\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n    properties: list[Property] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be exactly 5\",\n    )\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n\n```\n\nclass Property(BaseModel): index: str = Field(..., description=\"Monotonically increasing ID\") key: str = Field(description=\"Must be snake case\") value: str class Character(BaseModel): age: int name: str house: Literal\\[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"\\] properties: list\\[Property\\] = Field( ..., description=\"Numbered list of arbitrary extracted properties, should be exactly 5\", ) resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}\\], response\\_model=Character, ) resp.model\\_dump()\n\nOut\\[5\\]:\n\n```\n{'age': 38,\n 'name': 'Severus Snape',\n 'house': 'Slytherin',\n 'properties': [{'index': '1',\\\n   'key': 'position_at_hogwarts',\\\n   'value': 'Potions Master'},\\\n  {'index': '2', 'key': 'patronus_form', 'value': 'Doe'},\\\n  {'index': '3', 'key': 'loyalty', 'value': 'Albus Dumbledore'},\\\n  {'index': '4', 'key': 'played_by', 'value': 'Alan Rickman'},\\\n  {'index': '5', 'key': 'final_act', 'value': 'Protecting Harry Potter'}]}\n```\n\n## Defining Multiple Entities [¶](https://python.useinstructor.com/tutorials/2-tips/\\#defining-multiple-entities)\n\nNow that we see a single entity with many properties we can continue to nest them into many users\n\nIn \\[6\\]:\n\nCopied!\n\n```\nfrom collections.abc import Iterable\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n\n```\n\nfrom collections.abc import Iterable class Character(BaseModel): age: int name: str house: Literal\\[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"\\] resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}\\], response\\_model=Iterable\\[Character\\], ) for character in resp: print(character)\n\n```\nage=11 name='Harry Potter' house='Gryffindor'\nage=11 name='Hermione Granger' house='Gryffindor'\nage=11 name='Ron Weasley' house='Gryffindor'\nage=11 name='Draco Malfoy' house='Slytherin'\nage=11 name='Neville Longbottom' house='Gryffindor'\n\n```\n\nIn \\[7\\]:\n\nCopied!\n\n```\nfrom collections.abc import Iterable\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],\n    stream=True,\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n\n```\n\nfrom collections.abc import Iterable class Character(BaseModel): age: int name: str house: Literal\\[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"\\] resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}\\], stream=True, response\\_model=Iterable\\[Character\\], ) for character in resp: print(character)\n\n```\nage=11 name='Harry Potter' house='Gryffindor'\nage=11 name='Hermione Granger' house='Gryffindor'\nage=11 name='Ron Weasley' house='Gryffindor'\nage=17 name='Draco Malfoy' house='Slytherin'\nage=11 name='Luna Lovegood' house='Ravenclaw'\n\n```\n\n## Defining Relationships [¶](https://python.useinstructor.com/tutorials/2-tips/\\#defining-relationships)\n\nNot only can we define lists of users, but with lists of properties we can also easily define lists of references. It's one of the more interesting things I've learned about prompting.\n\nIn \\[8\\]:\n\nCopied!\n\n```\nclass Character(BaseModel):\n    id: int\n    name: str\n    friends_array: list[int] = Field(description=\"Relationships to their friends using the id\")\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"5 kids from Harry Potter\"}],\n    stream=True,\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n\n```\n\nclass Character(BaseModel): id: int name: str friends\\_array: list\\[int\\] = Field(description=\"Relationships to their friends using the id\") resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[{\"role\": \"user\", \"content\": \"5 kids from Harry Potter\"}\\], stream=True, response\\_model=Iterable\\[Character\\], ) for character in resp: print(character)\n\n```\nid=1 name='Harry Potter' friends_array=[2, 3, 4, 5, 6]\nid=2 name='Hermione Granger' friends_array=[1, 3, 4, 5]\nid=3 name='Ron Weasley' friends_array=[1, 2, 4, 6]\nid=4 name='Neville Longbottom' friends_array=[1, 2, 3, 5]\nid=5 name='Luna Lovegood' friends_array=[1, 2, 4, 6]\nid=6 name='Draco Malfoy' friends_array=[1, 3, 5]\n\n```\n\nWith the tools we've discussed, we can find numerous real-world applications in production settings. These include extracting action items from transcripts, generating fake data, filling out forms, and creating objects that correspond to generative UI. These simple tricks will be highly useful.\n\n# Missing Data [¶](https://python.useinstructor.com/tutorials/2-tips/\\#missing-data)\n\nThe Maybe pattern is a concept in functional programming used for error handling. Instead of raising exceptions or returning None, you can use a Maybe type to encapsulate both the result and potential errors.\n\nThis pattern is particularly useful when making LLM calls, as providing language models with an escape hatch can effectively reduce hallucinations.\n\nIn \\[9\\]:\n\nCopied!\n\n```\nfrom typing import Optional\n\nclass Character(BaseModel):\n    age: int\n    name: str\n\nclass MaybeCharacter(BaseModel):\n    result: Optional[Character] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n```\n\nfrom typing import Optional class Character(BaseModel): age: int name: str class MaybeCharacter(BaseModel): result: Optional\\[Character\\] = Field(default=None) error: bool = Field(default=False) message: Optional\\[str\\]\n\nIn \\[10\\]:\n\nCopied!\n\n```\ndef extract(content: str) -> MaybeCharacter:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MaybeCharacter,\n        messages=[\\\n            {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},\\\n        ],\n    )\n\n```\n\ndef extract(content: str) -> MaybeCharacter: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\_model=MaybeCharacter, messages=\\[ {\"role\": \"user\", \"content\": f\"Extract \\`{content}\\`\"}, \\], )\n\nIn \\[11\\]:\n\nCopied!\n\n```\nextract(\"Harry Potter\")\n\n```\n\nextract(\"Harry Potter\")\n\nOut\\[11\\]:\n\n```\nMaybeCharacter(result=Character(age=17, name='Harry Potter'), error=False, message=None)\n```\n\nIn \\[12\\]:\n\nCopied!\n\n```\nuser = extract(\"404 Error\")\n\nif user.error:\n    raise ValueError(user.message)\n\n```\n\nuser = extract(\"404 Error\") if user.error: raise ValueError(user.message)\n\n```\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb Cell 20 line 4\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a> user = extract(\"404 Error\")\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a> if user.error:\n----> <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>     raise ValueError(user.message)\n\nValueError: 404 Error\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/2-tips/",
      "ogUrl": "https://python.useinstructor.com/tutorials/2-tips/",
      "title": "Tips and Tricks - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/2-tips/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/2-tips.png",
      "ogTitle": "Tips and Tricks - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/2-tips.png",
      "og:title": "Tips and Tricks - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/2-tips/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/2-tips.png",
      "twitter:title": "Tips and Tricks - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/index.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/index.md \"View source of this page\")\n\n# Overview\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/?q=",
      "ogUrl": "https://python.useinstructor.com/tutorials/",
      "title": "SEO-Friendly Guide to Content Optimization - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/index.png",
      "ogTitle": "SEO-Friendly Guide to Content Optimization - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/index.png",
      "og:title": "SEO-Friendly Guide to Content Optimization - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/?q=",
      "statusCode": 200,
      "description": "Learn effective strategies for optimizing your content for search engines, enhancing visibility, and attracting organic traffic.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn effective strategies for optimizing your content for search engines, enhancing visibility, and attracting organic traffic.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/index.png",
      "twitter:title": "SEO-Friendly Guide to Content Optimization - Instructor",
      "og:description": "Learn effective strategies for optimizing your content for search engines, enhancing visibility, and attracting organic traffic.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn effective strategies for optimizing your content for search engines, enhancing visibility, and attracting organic traffic."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/1-introduction/?q=#working-with-structured-outputs)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/1-introduction.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/1-introduction.ipynb \"View source of this page\")\n\n# Working with structured outputs [¶](https://python.useinstructor.com/tutorials/1-introduction/?q=\\#working-with-structured-outputs)\n\nIf you've seen my [talk](https://www.youtube.com/watch?v=yj-wSRJwrrc&t=1s) on this topic, you can skip this chapter.\n\ntl;dr\n\nWhen we work with LLMs you find that many times we are not building chatbots, instead we're working with structured outputs in order to solve a problem by returning machine readable data. However the way we think about the problem is still very much influenced by the way we think about chatbots. This is a problem because it leads to a lot of confusion and frustration. In this chapter we'll try to understand why this happens and how we can fix it.\n\nIn \\[1\\]:\n\nCopied!\n\n```\nimport traceback\n\n```\n\nimport traceback\n\nIn \\[2\\]:\n\nCopied!\n\n```\nRED = \"\\033[91m\"\\\nRESET = \"\\033[0m\"\\\n\\\n```\\\n\\\nRED = \"\\\\033\\[91m\" RESET = \"\\\\033\\[0m\"\\\n\\\n## The fundamental problem with JSON and Dictionaries [¶](https://python.useinstructor.com/tutorials/1-introduction/?q=\\#the-fundamental-problem-with-json-and-dictionaries)\\\n\\\nLets say we have a simple JSON object, and we want to work with it. We can use the `json` module to load it into a dictionary, and then work with it. However, this is a bit of a pain, because we have to manually check the types of the data, and we have to manually check if the data is valid. For example, lets say we have a JSON object that looks like this:\\\n\\\nIn \\[3\\]:\\\n\\\nCopied!\\\n\\\n```\\\ndata = [{\"first_name\": \"Jason\", \"age\": 10}, {\"firstName\": \"Jason\", \"age\": \"10\"}]\\\n\\\n```\\\n\\\ndata = \\[{\"first\\_name\": \"Jason\", \"age\": 10}, {\"firstName\": \"Jason\", \"age\": \"10\"}\\]\\\n\\\nWe have a `name` field, which is a string, and an `age` field, which is an integer. However, if we were to load this into a dictionary, we would have no way of knowing if the data is valid. For example, we could have a string for the age, or we could have a float for the age. We could also have a string for the name, or we could have a list for the name.\\\n\\\nIn \\[4\\]:\\\n\\\nCopied!\\\n\\\n```\\\nfor obj in data:\\\n    name = obj.get(\"first_name\")\\\n    age = obj.get(\"age\")\\\n    print(f\"{name} is {age}\")\\\n\\\nfor obj in data:\\\n    name = obj.get(\"first_name\")\\\n    age = obj.get(\"age\")\\\n    try:\\\n        age_next_year = age + 1\\\n        print(f\"Next year {name} will be {age_next_year} years old\")\\\n    except TypeError:\\\n        traceback.print_exc()\\\n\\\n```\\\n\\\nfor obj in data: name = obj.get(\"first\\_name\") age = obj.get(\"age\") print(f\"{name} is {age}\") for obj in data: name = obj.get(\"first\\_name\") age = obj.get(\"age\") try: age\\_next\\_year = age + 1 print(f\"Next year {name} will be {age\\_next\\_year} years old\") except TypeError: traceback.print\\_exc()\\\n\\\n```\\\nJason is 10\\\nNone is 10\\\nNext year Jason will be 11 years old\\\n\\\n```\\\n\\\n```\\\nTraceback (most recent call last):\\\n  File \"/var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_24047/2607506000.py\", line 10, in <module>\\\n    age_next_year = age + 1\\\n                    ~~~~^~~\\\nTypeError: can only concatenate str (not \"int\") to str\\\n\\\n```\\\n\\\nYou see that while we were able to program with a dictionary, we had issues with the data being valid. We would have had to manually check the types of the data, and we had to manually check if the data was valid. This is a pain, and we can do better.\\\n\\\n## Pydantic to the rescue [¶](https://python.useinstructor.com/tutorials/1-introduction/?q=\\#pydantic-to-the-rescue)\\\n\\\nPydantic is a library that allows us to define data structures, and then validate them.\\\n\\\nIn \\[5\\]:\\\n\\\nCopied!\\\n\\\n```\\\nfrom pydantic import BaseModel, Field, ValidationError\\\n\\\nclass Person(BaseModel):\\\n    name: str\\\n    age: int\\\n\\\nperson = Person(name=\"Sam\", age=30)\\\nperson\\\n\\\n```\\\n\\\nfrom pydantic import BaseModel, Field, ValidationError class Person(BaseModel): name: str age: int person = Person(name=\"Sam\", age=30) person\\\n\\\nOut\\[5\\]:\\\n\\\n```\\\nPerson(name='Sam', age=30)\\\n```\\\n\\\nIn \\[6\\]:\\\n\\\nCopied!\\\n\\\n```\\\n# Data is correctly casted to the right type\\\nperson = Person.model_validate({\"name\": \"Sam\", \"age\": \"30\"})\\\nperson\\\n\\\n```\\\n\\\n\\# Data is correctly casted to the right type person = Person.model\\_validate({\"name\": \"Sam\", \"age\": \"30\"}) person\\\n\\\nOut\\[6\\]:\\\n\\\n```\\\nPerson(name='Sam', age=30)\\\n```\\\n\\\nIn \\[7\\]:\\\n\\\nCopied!\\\n\\\n```\\\nassert person.name == \"Sam\"\\\nassert person.age == 30\\\n\\\ntry:\\\n    assert person.age == 20\\\nexcept AssertionError:\\\n    traceback.print_exc()\\\n\\\n```\\\n\\\nassert person.name == \"Sam\" assert person.age == 30 try: assert person.age == 20 except AssertionError: traceback.print\\_exc()\\\n\\\n```\\\nTraceback (most recent call last):\\\n  File \"/var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_24047/3040264600.py\", line 5, in <module>\\\n    assert person.age == 20\\\n           ^^^^^^^^^^^^^^^^\\\nAssertionError\\\n\\\n```\\\n\\\nIn \\[8\\]:\\\n\\\nCopied!\\\n\\\n```\\\n# Data is validated to get better error messages\\\ntry:\\\n    person = Person.model_validate({\"first_name\": \"Sam\", \"age\": \"30.2\"})\\\nexcept ValidationError as e:\\\n    print(\"Validation Error:\")\\\n    for error in e.errors():\\\n        print(f\"Field: {error['loc'][0]}, Error: {error['msg']}\")\\\n\\\n    print(f\"{RED}\\nOriginal Traceback Below{RESET}\")\\\n    traceback.print_exc()\\\n\\\n```\\\n\\\n\\# Data is validated to get better error messages try: person = Person.model\\_validate({\"first\\_name\": \"Sam\", \"age\": \"30.2\"}) except ValidationError as e: print(\"Validation Error:\") for error in e.errors(): print(f\"Field: {error\\['loc'\\]\\[0\\]}, Error: {error\\['msg'\\]}\") print(f\"{RED}\\\\nOriginal Traceback Below{RESET}\") traceback.print\\_exc()\\\n\\\n```\\\nValidation Error:\\\nField: name, Error: Field required\\\nField: age, Error: Input should be a valid integer, unable to parse string as an integer\\\n\\\nOriginal Traceback Below\\\n\\\n```\\\n\\\n```\\\nTraceback (most recent call last):\\\n  File \"/var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_24047/621989455.py\", line 3, in <module>\\\n    person = Person.model_validate({\"first_name\": \"Sam\", \"age\": \"30.2\"})\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/instructor/lib/python3.11/site-packages/pydantic/main.py\", line 509, in model_validate\\\n    return cls.__pydantic_validator__.validate_python(\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\npydantic_core._pydantic_core.ValidationError: 2 validation errors for Person\\\nname\\\n  Field required [type=missing, input_value={'first_name': 'Sam', 'age': '30.2'}, input_type=dict]\\\n    For further information visit https://errors.pydantic.dev/2.6/v/missing\\\nage\\\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='30.2', input_type=str]\\\n    For further information visit https://errors.pydantic.dev/2.6/v/int_parsing\\\n\\\n```\\\n\\\nBy introducing pydantic into any python codebase you can get a lot of benefits. You can get type checking, you can get validation, and you can get autocomplete. This is a huge win, because it means you can catch errors before they happen. This is even more useful when we rely on language models to generate data for us.\\\n\\\nYou can also define validators that are run on the data. This is useful because it means you can catch errors before they happen. For example, you can define a validator that checks if the age is greater than 0. This is useful because it means you can catch errors before they happen.\\\n\\\n## Fundamental problem with asking for JSON from OpenAI [¶](https://python.useinstructor.com/tutorials/1-introduction/?q=\\#fundamental-problem-with-asking-for-json-from-openai)\\\n\\\nAs we shall see below, the correct json format would be something of the format below:\\\n\\\n```\\\n{\\\n    \"name\": \"Jason\",\\\n    \"age\": 10\\\n}\\\n\\\n```\\\n\\\nHowever, we get errorenous outputs like:\\\n\\\n```\\\n{\\\n  \"jason\": 10\\\n}\\\n\\\n```\\\n\\\nIn \\[9\\]:\\\n\\\nCopied!\\\n\\\n````\\\nfrom openai import OpenAI\\\n\\\nclient = OpenAI()\\\n\\\nresp = client.chat.completions.create(\\\n    model=\"gpt-3.5-turbo\",\\\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Please give me jason is 10 as a json object ```json\\n\"},\\\n    ],\\\n    n=10,\\\n    temperature=1,\\\n)\\\n\\\nprint(\"json that we want:\")\\\nprint(\"\"\"\\\n{\\\n    \"name\": \"Jason\",\\\n    \"age\": 10\\\n}\\\n\"\"\")\\\n\\\nfor choice in resp.choices:\\\n    json = choice.message.content\\\n    try:\\\n        person = Person.model_validate_json(json)\\\n        print(f\"correctly parsed {person=}\")\\\n    except Exception as e:\\\n        print(\"error!!\")\\\n        print(json)\\\n\\\n````\\\n\\\nfrom openai import OpenAI client = OpenAI() resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\[ {\"role\": \"user\", \"content\": \"Please give me jason is 10 as a json object \\`\\`\\`json\\\\n\"}, \\], n=10, temperature=1, ) print(\"json that we want:\") print(\"\"\" { \"name\": \"Jason\", \"age\": 10 } \"\"\") for choice in resp.choices: json = choice.message.content try: person = Person.model\\_validate\\_json(json) print(f\"correctly parsed {person=}\") except Exception as e: print(\"error!!\") print(json)\\\n\\\n```\\\njson that we want:\\\n\\\n{\\\n    \"name\": \"Jason\",\\\n    \"age\": 10\\\n}\\\n\\\nerror!!\\\n{\\\n  \"jason\": 10\\\n}\\\ncorrectly parsed person=Person(name='Jason', age=10)\\\ncorrectly parsed person=Person(name='jason', age=10)\\\nerror!!\\\n{\\\n  \"Jason\": {\\\n    \"age\": 10\\\n  }\\\n}\\\nerror!!\\\n{\\\n  \"Jason\": {\\\n    \"age\": 10\\\n  }\\\n}\\\nerror!!\\\n{\\\n  \"Jason\": {\\\n    \"age\": 10\\\n  }\\\n}\\\nerror!!\\\n{\\\n  \"Jason\": {\\\n    \"age\": 10\\\n  }\\\n}\\\ncorrectly parsed person=Person(name='Jason', age=10)\\\ncorrectly parsed person=Person(name='Jason', age=10)\\\nerror!!\\\n{\\\n  \"jason\": 10\\\n}\\\n\\\n```\\\n\\\n## Introduction to Function Calling [¶](https://python.useinstructor.com/tutorials/1-introduction/?q=\\#introduction-to-function-calling)\\\n\\\nThe json could be anything! We could add more and more into a prompt and hope it works, or we can use something called [function calling](https://platform.openai.com/docs/guides/function-calling) to directly specify the schema we want.\\\n\\\n**Function Calling**\\\n\\\nIn an API call, you can describe _functions_ and have the model intelligently choose to output a _JSON object_ containing _arguments_ to call one or many functions. The Chat Completions API does **not** call the function; instead, the model generates _JSON_ that you can use to call the function in **your code**.\\\n\\\nIn \\[10\\]:\\\n\\\nCopied!\\\n\\\n```\\\nimport datetime\\\n\\\nclass PersonBirthday(BaseModel):\\\n    name: str\\\n    age: int\\\n    birthday: datetime.date\\\n\\\nschema = {\\\n    \"properties\": {\\\n        \"name\": {\"type\": \"string\"},\\\n        \"age\": {\"type\": \"integer\"},\\\n        \"birthday\": {\"type\": \"string\", \"format\": \"YYYY-MM-DD\"},\\\n    },\\\n    \"required\": [\"name\", \"age\"],\\\n    \"type\": \"object\",\\\n}\\\n\\\nresp = client.chat.completions.create(\\\n    model=\"gpt-3.5-turbo\",\\\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": f\"Extract `Jason Liu is thirty years old his birthday is yesturday` into json today is {datetime.date.today()}\",\\\n        },\\\n    ],\\\n    functions=[{\"name\": \"Person\", \"parameters\": schema}],\\\n    function_call=\"auto\",\\\n)\\\n\\\nPersonBirthday.model_validate_json(resp.choices[0].message.function_call.arguments)\\\n\\\n```\\\n\\\nimport datetime class PersonBirthday(BaseModel): name: str age: int birthday: datetime.date schema = { \"properties\": { \"name\": {\"type\": \"string\"}, \"age\": {\"type\": \"integer\"}, \"birthday\": {\"type\": \"string\", \"format\": \"YYYY-MM-DD\"}, }, \"required\": \\[\"name\", \"age\"\\], \"type\": \"object\", } resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\[ { \"role\": \"user\", \"content\": f\"Extract \\`Jason Liu is thirty years old his birthday is yesturday\\` into json today is {datetime.date.today()}\", }, \\], functions=\\[{\"name\": \"Person\", \"parameters\": schema}\\], function\\_call=\"auto\", ) PersonBirthday.model\\_validate\\_json(resp.choices\\[0\\].message.function\\_call.arguments)\\\n\\\nOut\\[10\\]:\\\n\\\n```\\\nPersonBirthday(name='Jason Liu', age=30, birthday=datetime.date(1994, 3, 26))\\\n```\\\n\\\nBut it turns out, pydantic actually not only does our serialization, we can define the schema as well as add additional documentation!\\\n\\\nIn \\[11\\]:\\\n\\\nCopied!\\\n\\\n```\\\nPersonBirthday.model_json_schema()\\\n\\\n```\\\n\\\nPersonBirthday.model\\_json\\_schema()\\\n\\\nOut\\[11\\]:\\\n\\\n```\\\n{'properties': {'name': {'title': 'Name', 'type': 'string'},\\\n  'age': {'title': 'Age', 'type': 'integer'},\\\n  'birthday': {'format': 'date', 'title': 'Birthday', 'type': 'string'}},\\\n 'required': ['name', 'age', 'birthday'],\\\n 'title': 'PersonBirthday',\\\n 'type': 'object'}\\\n```\\\n\\\nWe can even define nested complex schemas, and documentation with ease.\\\n\\\nIn \\[12\\]:\\\n\\\nCopied!\\\n\\\n```\\\nclass Address(BaseModel):\\\n    address: str = Field(description=\"Full street address\")\\\n    city: str\\\n    state: str\\\n\\\nclass PersonAddress(Person):\\\n    \"\"\"A Person with an address\"\"\"\\\n\\\n    address: Address\\\n\\\nPersonAddress.model_json_schema()\\\n\\\n```\\\n\\\nclass Address(BaseModel): address: str = Field(description=\"Full street address\") city: str state: str class PersonAddress(Person): \"\"\"A Person with an address\"\"\" address: Address PersonAddress.model\\_json\\_schema()\\\n\\\nOut\\[12\\]:\\\n\\\n```\\\n{'$defs': {'Address': {'properties': {'address': {'description': 'Full street address',\\\n     'title': 'Address',\\\n     'type': 'string'},\\\n    'city': {'title': 'City', 'type': 'string'},\\\n    'state': {'title': 'State', 'type': 'string'}},\\\n   'required': ['address', 'city', 'state'],\\\n   'title': 'Address',\\\n   'type': 'object'}},\\\n 'description': 'A Person with an address',\\\n 'properties': {'name': {'title': 'Name', 'type': 'string'},\\\n  'age': {'title': 'Age', 'type': 'integer'},\\\n  'address': {'$ref': '#/$defs/Address'}},\\\n 'required': ['name', 'age', 'address'],\\\n 'title': 'PersonAddress',\\\n 'type': 'object'}\\\n```\\\n\\\nThese simple concepts become what we built into `instructor` and most of the work has been around documenting how we can leverage schema engineering. Except now we use `instructor.patch()` to add a bunch more capabilities to the OpenAI SDK.\\\n\\\n# The core idea around Instructor [¶](https://python.useinstructor.com/tutorials/1-introduction/?q=\\#the-core-idea-around-instructor)\\\n\\\n1. Using function calling allows us use a llm that is finetuned to use json\\_schema and output json.\\\n2. Pydantic can be used to define the object, schema, and validation in one single class, allow us to encapsulate everything neatly\\\n3. As a library with 100M downloads, we can leverage pydantic to do all the heavy lifting for us and fit nicely with the python ecosystem\\\n\\\nIn \\[13\\]:\\\n\\\nCopied!\\\n\\\n```\\\nimport instructor\\\nimport datetime\\\n\\\n# patch the client to add `response_model` to the `create` method\\\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\\\n\\\nresp = client.chat.completions.create(\\\n    model=\"gpt-3.5-turbo-1106\",\\\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": f\"\"\"\\\n            Today is {datetime.date.today()}\\\n\\\n            Extract `Jason Liu is thirty years old his birthday is yesturday`\\\n            he lives at 123 Main St, San Francisco, CA\"\"\",\\\n        },\\\n    ],\\\n    response_model=PersonAddress,\\\n)\\\nresp\\\n\\\n```\\\n\\\nimport instructor import datetime # patch the client to add \\`response\\_model\\` to the \\`create\\` method client = instructor.patch(OpenAI(), mode=instructor.Mode.MD\\_JSON) resp = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", messages=\\[ { \"role\": \"user\", \"content\": f\"\"\" Today is {datetime.date.today()} Extract \\`Jason Liu is thirty years old his birthday is yesturday\\` he lives at 123 Main St, San Francisco, CA\"\"\", }, \\], response\\_model=PersonAddress, ) resp\\\n\\\nOut\\[13\\]:\\\n\\\n```\\\nPersonAddress(name='Jason Liu', age=30, address=Address(address='123 Main St', city='San Francisco', state='CA'))\\\n```\\\n\\\nBy defining `response_model` we can leverage pydantic to do all the heavy lifting. Later we'll introduce the other features that `instructor.patch()` adds to the OpenAI SDK. but for now, this small change allows us to do a lot more with the API.\\\n\\\n## Is instructor the only way to do this? [¶](https://python.useinstructor.com/tutorials/1-introduction/?q=\\#is-instructor-the-only-way-to-do-this)\\\n\\\nNo. Libraries like Marvin, Langchain, and Llamaindex all now leverage the Pydantic object in similar ways. The goal is to be as light weight as possible, get you as close as possible to the openai api, and then get out of your way.\\\n\\\nMore importantly, we've also added straight forward validation and reasking to the mix.\\\n\\\nThe goal of instructor is to show you how to think about structured prompting and provide examples and documentation that you can take with you to any framework.\\\n\\\nFor further exploration:\\\n\\\n- [Marvin](https://www.askmarvin.ai/)\\\n- [Langchain](https://python.langchain.com/docs/modules/model_io/output_parsers/pydantic)\\\n- [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/examples/output_parsing/openai_pydantic_program.html)\\\n\\\nWas this page helpful?\\\n\\\nThanks for your feedback!\\\n\\\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\\\n\\\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/1-introduction/?q=",
      "ogUrl": "https://python.useinstructor.com/tutorials/1-introduction/",
      "title": "Tutorials (Notebooks) - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/1-introduction/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/1-introduction.png",
      "ogTitle": "Tutorials (Notebooks) - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/1-introduction.png",
      "og:title": "Tutorials (Notebooks) - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/1-introduction/?q=",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/1-introduction.png",
      "twitter:title": "Tutorials (Notebooks) - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/3-0-applications-rag/?q=#applying-structured-output-to-rag-applications)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/3-0-applications-rag.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/3-0-applications-rag.ipynb \"View source of this page\")\n\n# Applying Structured Output to RAG applications [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/?q=\\#applying-structured-output-to-rag-applications)\n\n**What is RAG?**\n\nRetrieval Augmented Generation (RAG) models are the bridge between large language models and external knowledge databases. They fetch the relevant data for a given query. For example, if you have some documents and want to ask questions related to the content of those documents, RAG models help by retrieving data from those documents and passing it to the LLM in queries.\n\n**How do RAG models work?**\n\nThe typical RAG process involves embedding a user query and searching a vector database to find the most relevant information to supplement the generated response. This approach is particularly effective when the database contains information closely matching the query but not more than that.\n\n![Image](https://jxnl.github.io/instructor/blog/img/dumb_rag.png)\n\n**Why is there a need for them?**\n\nPre-trained large language models do not learn over time. If you ask them a question they have not been trained on, they will often hallucinate. Therefore, we need to embed our own data to achieve a better output.\n\n## Simple RAG [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/?q=\\#simple-rag)\n\n**What is it?**\n\nThe simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.\n\n- **Query-Document Mismatch:** It assumes that the query and document embeddings will align in the vector space, which is often not the case.\n- **Text Search Limitations:** The model is restricted to simple text queries without the nuances of advanced search features.\n- **Limited Planning Ability:** It fails to consider additional contextual information that could refine the search results.\n\n## Improving the RAG model [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/?q=\\#improving-the-rag-model)\n\n**What's the solution?**\n\nEnhancing RAG requires a more sophisticated approach known as query understanding.\n\nThis process involves analyzing the user's query and transforming it to better match the backend's search capabilities.\n\nBy doing so, we can significantly improve both the precision and recall of the search results, providing more accurate and relevant responses.\n\n![Image](https://jxnl.github.io/instructor/blog/img/query_understanding.png)\n\n## Practical Examples [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/?q=\\#practical-examples)\n\nIn the examples below, we're going to use the [`instructor`](https://github.com/jxnl/instructor) library to simplify the interaction between the programmer and language models via the function-calling API.\n\nIn \\[1\\]:\n\nCopied!\n\n```\nimport instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.patch(OpenAI())\n\n```\n\nimport instructor from openai import OpenAI from pydantic import BaseModel, Field client = instructor.patch(OpenAI())\n\n### Example 1) Improving Extractions [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/?q=\\#example-1-improving-extractions)\n\nOne of the big limitations is that often times the query we embed and the text we are searching for may not have a direct match, leading to suboptimal results. A common method of using structured output is to extract information from a document and use it to answer a question. Directly, we can be creative in how we extract, summarize and generate potential questions in order for our embeddings to do better.\n\nFor example, instead of using just a text chunk we could try to:\n\n1. extract key words and themes\n2. extract hypothetical questions\n3. generate a summary of the text\n\nIn the example below, we use the `instructor` library to extract the key words and themes from a text chunk and use them to answer a question.\n\nIn \\[2\\]:\n\nCopied!\n\n```\nclass Extraction(BaseModel):\n    topic: str\n    summary: str\n    hypothetical_questions: list[str] = Field(\n        default_factory=list,\n        description=\"Hypothetical questions that this document could answer\",\n    )\n    keywords: list[str] = Field(\n        default_factory=list, description=\"Keywords that this document is about\"\n    )\n\n```\n\nclass Extraction(BaseModel): topic: str summary: str hypothetical\\_questions: list\\[str\\] = Field( default\\_factory=list, description=\"Hypothetical questions that this document could answer\", ) keywords: list\\[str\\] = Field( default\\_factory=list, description=\"Keywords that this document is about\" )\n\nIn \\[3\\]:\n\nCopied!\n\n```\nfrom pprint import pprint\nfrom collections.abc import Iterable\n\ntext_chunk = \"\"\"\n## Simple RAG\n\n**What is it?**\n\nThe simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.\n\n**What are the limitations?**\n\n- **Query-Document Mismatch:** It assumes that the query and document embeddings will align in the vector space, which is often not the case.\n    - Query: \"Tell me about climate change effects on marine life.\"\n    - Issue: The model might retrieve documents related to general climate change or marine life, missing the specific intersection of both topics.\n- **Monolithic Search Backend:** It relies on a single search method and backend, reducing flexibility and the ability to handle multiple data sources.\n    - Query: \"Latest research in quantum computing.\"\n    - Issue: The model might only search in a general science database, missing out on specialized quantum computing resources.\n- **Text Search Limitations:** The model is restricted to simple text queries without the nuances of advanced search features.\n    - Query: \"what problems did we fix last week\"\n    - Issue: cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week.\n- **Limited Planning Ability:** It fails to consider additional contextual information that could refine the search results.\n    - Query: \"Tips for first-time Europe travelers.\"\n    - Issue: The model might provide general travel advice, ignoring the specific context of first-time travelers or European destinations.\n\"\"\"\n\nextractions = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    stream=True,\n    response_model=Iterable[Extraction],\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"Your role is to extract chunks from the following and create a set of topics.\",\\\n        },\\\n        {\"role\": \"user\", \"content\": text_chunk},\\\n    ],\n)\n\nfor extraction in extractions:\n    pprint(extraction.model_dump())\n\n```\n\nfrom pprint import pprint from collections.abc import Iterable text\\_chunk = \"\"\" ## Simple RAG \\*\\*What is it?\\*\\* The simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources. \\*\\*What are the limitations?\\*\\* - \\*\\*Query-Document Mismatch:\\*\\* It assumes that the query and document embeddings will align in the vector space, which is often not the case. - Query: \"Tell me about climate change effects on marine life.\" - Issue: The model might retrieve documents related to general climate change or marine life, missing the specific intersection of both topics. - \\*\\*Monolithic Search Backend:\\*\\* It relies on a single search method and backend, reducing flexibility and the ability to handle multiple data sources. - Query: \"Latest research in quantum computing.\" - Issue: The model might only search in a general science database, missing out on specialized quantum computing resources. - \\*\\*Text Search Limitations:\\*\\* The model is restricted to simple text queries without the nuances of advanced search features. - Query: \"what problems did we fix last week\" - Issue: cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week. - \\*\\*Limited Planning Ability:\\*\\* It fails to consider additional contextual information that could refine the search results. - Query: \"Tips for first-time Europe travelers.\" - Issue: The model might provide general travel advice, ignoring the specific context of first-time travelers or European destinations. \"\"\" extractions = client.chat.completions.create( model=\"gpt-4-1106-preview\", stream=True, response\\_model=Iterable\\[Extraction\\], messages=\\[ { \"role\": \"system\", \"content\": \"Your role is to extract chunks from the following and create a set of topics.\", }, {\"role\": \"user\", \"content\": text\\_chunk}, \\], ) for extraction in extractions: pprint(extraction.model\\_dump())\n\n```\n{'hypothetical_questions': ['What is the basic concept behind simple RAG?',\\\n                            'How does simple RAG work for information '\\\n                            'retrieval?'],\n 'keywords': ['Simple RAG',\\\n              'Retrieval-Augmented Generation',\\\n              'user query',\\\n              'embedding search',\\\n              'vector database',\\\n              'Wikipedia articles',\\\n              'information retrieval'],\n 'summary': 'The simplest implementation of Retrieval-Augmented Generation '\n            '(RAG) involves embedding a user query and conducting a single '\n            'embedding search in a vector database, like a vector store of '\n            'Wikipedia articles, to retrieve relevant information. This method '\n            'may not be ideal for complex queries or varied data sources.',\n 'topic': 'Simple RAG'}\n{'hypothetical_questions': ['What are the drawbacks of using simple RAG '\\\n                            'systems?',\\\n                            'How does query-document mismatch affect the '\\\n                            'performance of RAG?',\\\n                            'Why is a monolithic search backend a limitation '\\\n                            'for RAG?'],\n 'keywords': ['limitations',\\\n              'query-document mismatch',\\\n              'simple RAG',\\\n              'monolithic search backend',\\\n              'text search',\\\n              'planning ability',\\\n              'contextual information'],\n 'summary': 'Key limitations of the simple RAG include query-document '\n            'mismatch, reliance on a single search backend, constraints of '\n            'text search capabilities, and limited planning ability to '\n            'leverage contextual information. These issues can result in '\n            'suboptimal search outcomes and retrieval of irrelevant or broad '\n            'information.',\n 'topic': 'Limitations of Simple RAG'}\n\n```\n\nNow you can imagine if you were to embed the summaries, hypothetical questions, and keywords in a vector database (i.e. in the metadata fields of a vector database), you can then use a vector search to find the best matching document for a given query. What you'll find is that the results are much better than if you were to just embed the text chunk!\n\n### Example 2) Understanding 'recent queries' to add temporal context [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/?q=\\#example-2-understanding-recent-queries-to-add-temporal-context)\n\nOne common application of using structured outputs for query understanding is to identify the intent of a user's query. In this example we're going to use a simple schema to seperately process the query to add additional temporal context.\n\nIn \\[4\\]:\n\nCopied!\n\n```\nfrom datetime import date\n\nclass DateRange(BaseModel):\n    start: date\n    end: date\n\nclass Query(BaseModel):\n    rewritten_query: str\n    published_daterange: DateRange\n\n```\n\nfrom datetime import date class DateRange(BaseModel): start: date end: date class Query(BaseModel): rewritten\\_query: str published\\_daterange: DateRange\n\nIn this example, `DateRange` and `Query` are Pydantic models that structure the user's query with a date range and a list of domains to search within.\n\nThese models **restructure** the user's query by including a rewritten query, a range of published dates, and a list of domains to search in.\n\nUsing the new restructured query, we can apply this pattern to our function calls to obtain results that are optimized for our backend.\n\nIn \\[5\\]:\n\nCopied!\n\n```\ndef expand_query(q) -> Query:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Query,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\\\n            },\\\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\\\n        ],\n    )\n\nquery = expand_query(\"What are some recent developments in AI?\")\nquery\n\n```\n\ndef expand\\_query(q) -> Query: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\_model=Query, messages=\\[ { \"role\": \"system\", \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\", }, {\"role\": \"user\", \"content\": f\"query: {q}\"}, \\], ) query = expand\\_query(\"What are some recent developments in AI?\") query\n\nOut\\[5\\]:\n\n```\nQuery(rewritten_query='Recent developments in artificial intelligence', published_daterange=DateRange(start=datetime.date(2024, 1, 1), end=datetime.date(2024, 3, 31)))\n```\n\nThis isn't just about adding some date ranges. We can even use some chain of thought prompting to generate tailored searches that are deeply integrated with our backend.\n\nIn \\[6\\]:\n\nCopied!\n\n```\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Think step by step to plan what is the best time range to search in\"\n    )\n    start: date\n    end: date\n\nclass Query(BaseModel):\n    rewritten_query: str = Field(\n        description=\"Rewrite the query to make it more specific\"\n    )\n    published_daterange: DateRange = Field(\n        description=\"Effective date range to search in\"\n    )\n\ndef expand_query(q) -> Query:\n    return client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        response_model=Query,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\\\n            },\\\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\\\n        ],\n    )\n\nexpand_query(\"What are some recent developments in AI?\")\n\n```\n\nclass DateRange(BaseModel): chain\\_of\\_thought: str = Field( description=\"Think step by step to plan what is the best time range to search in\" ) start: date end: date class Query(BaseModel): rewritten\\_query: str = Field( description=\"Rewrite the query to make it more specific\" ) published\\_daterange: DateRange = Field( description=\"Effective date range to search in\" ) def expand\\_query(q) -> Query: return client.chat.completions.create( model=\"gpt-4-1106-preview\", response\\_model=Query, messages=\\[ { \"role\": \"system\", \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\", }, {\"role\": \"user\", \"content\": f\"query: {q}\"}, \\], ) expand\\_query(\"What are some recent developments in AI?\")\n\nOut\\[6\\]:\n\n```\nQuery(rewritten_query='latest advancements in artificial intelligence', published_daterange=DateRange(chain_of_thought='Since the user is asking for recent developments, it would be relevant to look for articles and papers published within the last year. Therefore, setting the start date to a year before today and the end date to today will cover the most recent advancements.', start=datetime.date(2023, 3, 31), end=datetime.date(2024, 3, 31)))\n```\n\n## Using Weights and Biases to track experiments [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/?q=\\#using-weights-and-biases-to-track-experiments)\n\nWhile running a function like this production is quite simple, a lot of time will be spend on iterating and improving the model. To do this, we can use Weights and Biases to track our experiments.\n\nIn order to do so we wand manage a few things\n\n1. Save input and output pairs for later\n2. Save the JSON schema for the response\\_model\n3. Having snapshots of the model and data allow us to compare results over time, and as we make changes to the model we can see how the results change.\n\nThis is particularly useful when we might want to blend a mix of synthetic and real data to evaluate our model. We can use the `wandb` library to track our experiments and save the results to a dashboard.\n\nIn \\[7\\]:\n\nCopied!\n\n```\nimport json\nimport instructor\n\nfrom openai import AsyncOpenAI\nfrom datetime import date\nfrom pydantic import BaseModel, Field\n\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Think step by step to plan what is the best time range to search in\"\n    )\n    start: date\n    end: date\n\nclass Query(BaseModel):\n    rewritten_query: str = Field(\n        description=\"Rewrite the query to make it more specific\"\n    )\n    published_daterange: DateRange = Field(\n        description=\"Effective date range to search in\"\n    )\n\n    def report(self):\n        dct = self.model_dump()\n        dct[\"usage\"] = self._raw_response.usage.model_dump()\n        return dct\n\n# We'll use a different client for async calls\n# To highlight the difference and how we can use both\naclient = instructor.patch(AsyncOpenAI())\n\nasync def expand_query(\n    q, *, model: str = \"gpt-4-1106-preview\", temp: float = 0\n) -> Query:\n    return await aclient.chat.completions.create(\n        model=model,\n        temperature=temp,\n        response_model=Query,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\\\n            },\\\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\\\n        ],\n    )\n\n```\n\nimport json import instructor from openai import AsyncOpenAI from datetime import date from pydantic import BaseModel, Field class DateRange(BaseModel): chain\\_of\\_thought: str = Field( description=\"Think step by step to plan what is the best time range to search in\" ) start: date end: date class Query(BaseModel): rewritten\\_query: str = Field( description=\"Rewrite the query to make it more specific\" ) published\\_daterange: DateRange = Field( description=\"Effective date range to search in\" ) def report(self): dct = self.model\\_dump() dct\\[\"usage\"\\] = self.\\_raw\\_response.usage.model\\_dump() return dct # We'll use a different client for async calls # To highlight the difference and how we can use both aclient = instructor.patch(AsyncOpenAI()) async def expand\\_query( q, \\*, model: str = \"gpt-4-1106-preview\", temp: float = 0 ) -> Query: return await aclient.chat.completions.create( model=model, temperature=temp, response\\_model=Query, messages=\\[ { \"role\": \"system\", \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\", }, {\"role\": \"user\", \"content\": f\"query: {q}\"}, \\], )\n\nIn \\[8\\]:\n\nCopied!\n\n```\n# % pip install pandas wandb\nimport pandas as pd\nfrom typing import Any\n\ndef flatten_dict(d: dict[str, Any], parent_key: str = \"\", sep: str = \"_\") -> dict[str, Any]:\n    \"\"\"\n    Flatten a nested dictionary.\n\n    :param d: The nested dictionary to flatten.\n    :param parent_key: The base key to use for the flattened keys.\n    :param sep: Separator to use between keys.\n    :return: A flattened dictionary.\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\ndef dicts_to_df(list_of_dicts: list[dict[str, Any]]) -> pd.DataFrame:\n    \"\"\"\n    Convert a list of dictionaries to a pandas DataFrame.\n\n    :param list_of_dicts: List of dictionaries, potentially nested.\n    :return: A pandas DataFrame representing the flattened data.\n    \"\"\"\n    # Flatten each dictionary and create a DataFrame\n    flattened_data = [flatten_dict(d) for d in list_of_dicts]\n    return pd.DataFrame(flattened_data)\n\n```\n\n\\# % pip install pandas wandb import pandas as pd from typing import Any def flatten\\_dict(d: dict\\[str, Any\\], parent\\_key: str = \"\", sep: str = \"\\_\") -> dict\\[str, Any\\]: \"\"\" Flatten a nested dictionary. :param d: The nested dictionary to flatten. :param parent\\_key: The base key to use for the flattened keys. :param sep: Separator to use between keys. :return: A flattened dictionary. \"\"\" items = \\[\\] for k, v in d.items(): new\\_key = f\"{parent\\_key}{sep}{k}\" if parent\\_key else k if isinstance(v, dict): items.extend(flatten\\_dict(v, new\\_key, sep=sep).items()) else: items.append((new\\_key, v)) return dict(items) def dicts\\_to\\_df(list\\_of\\_dicts: list\\[dict\\[str, Any\\]\\]) -> pd.DataFrame: \"\"\" Convert a list of dictionaries to a pandas DataFrame. :param list\\_of\\_dicts: List of dictionaries, potentially nested. :return: A pandas DataFrame representing the flattened data. \"\"\" # Flatten each dictionary and create a DataFrame flattened\\_data = \\[flatten\\_dict(d) for d in list\\_of\\_dicts\\] return pd.DataFrame(flattened\\_data)\n\nIn \\[9\\]:\n\nCopied!\n\n```\nimport asyncio\nimport time\nimport pandas as pd\nimport wandb\n\nmodel = \"gpt-4-1106-preview\"\ntemp = 0\n\nrun = wandb.init(\n    project=\"query\",\n    config={\"model\": model, \"temp\": temp},\n)\n\ntest_queries = [\\\n    \"latest developments in artificial intelligence last 3 weeks\",\\\n    \"renewable energy trends past month\",\\\n    \"quantum computing advancements last 2 months\",\\\n    \"biotechnology updates last 10 days\",\\\n]\nstart = time.perf_counter()\nqueries = await asyncio.gather(\n    *[expand_query(q, model=model, temp=temp) for q in test_queries]\n)\nduration = time.perf_counter() - start\n\nwith open(\"schema.json\", \"w+\") as f:\n    schema = Query.model_json_schema()\n    json.dump(schema, f, indent=2)\n\nwith open(\"results.jsonlines\", \"w+\") as f:\n    for query in queries:\n        f.write(query.model_dump_json() + \"\\n\")\n\ndf = dicts_to_df([q.report() for q in queries])\ndf[\"input\"] = test_queries\ndf.to_csv(\"results.csv\")\n\nrun.log({\"schema\": wandb.Table(dataframe=pd.DataFrame([{\"schema\": schema}]))})\nrun.log(\n    {\n        \"usage_total_tokens\": df[\"usage_total_tokens\"].sum(),\n        \"usage_completion_tokens\": df[\"usage_completion_tokens\"].sum(),\n        \"usage_prompt_tokens\": df[\"usage_prompt_tokens\"].sum(),\n        \"duration (s)\": duration,\n        \"average duration (s)\": duration / len(queries),\n        \"n_queries\": len(queries),\n    }\n)\n\nrun.log(\n    {\n        \"results\": wandb.Table(dataframe=df),\n    }\n)\n\nfiles = wandb.Artifact(\"data\", type=\"dataset\")\nfiles.add_file(\"schema.json\")\nfiles.add_file(\"results.jsonlines\")\nfiles.add_file(\"results.csv\")\n\nrun.log_artifact(files)\nrun.finish()\n\n```\n\nimport asyncio import time import pandas as pd import wandb model = \"gpt-4-1106-preview\" temp = 0 run = wandb.init( project=\"query\", config={\"model\": model, \"temp\": temp}, ) test\\_queries = \\[ \"latest developments in artificial intelligence last 3 weeks\", \"renewable energy trends past month\", \"quantum computing advancements last 2 months\", \"biotechnology updates last 10 days\", \\] start = time.perf\\_counter() queries = await asyncio.gather( \\*\\[expand\\_query(q, model=model, temp=temp) for q in test\\_queries\\] ) duration = time.perf\\_counter() - start with open(\"schema.json\", \"w+\") as f: schema = Query.model\\_json\\_schema() json.dump(schema, f, indent=2) with open(\"results.jsonlines\", \"w+\") as f: for query in queries: f.write(query.model\\_dump\\_json() + \"\\\\n\") df = dicts\\_to\\_df(\\[q.report() for q in queries\\]) df\\[\"input\"\\] = test\\_queries df.to\\_csv(\"results.csv\") run.log({\"schema\": wandb.Table(dataframe=pd.DataFrame(\\[{\"schema\": schema}\\]))}) run.log( { \"usage\\_total\\_tokens\": df\\[\"usage\\_total\\_tokens\"\\].sum(), \"usage\\_completion\\_tokens\": df\\[\"usage\\_completion\\_tokens\"\\].sum(), \"usage\\_prompt\\_tokens\": df\\[\"usage\\_prompt\\_tokens\"\\].sum(), \"duration (s)\": duration, \"average duration (s)\": duration / len(queries), \"n\\_queries\": len(queries), } ) run.log( { \"results\": wandb.Table(dataframe=df), } ) files = wandb.Artifact(\"data\", type=\"dataset\") files.add\\_file(\"schema.json\") files.add\\_file(\"results.jsonlines\") files.add\\_file(\"results.csv\") run.log\\_artifact(files) run.finish()\n\nThe output of Weights and Biases would return something like the below table.\n\n| Metric | Value |\n| --- | --- |\n| average duration (s) | 1.5945 |\n| duration (s) | 6.37799 |\n| n\\_queries | 4 |\n| usage\\_completion\\_tokens | 376 |\n| usage\\_prompt\\_tokens | 780 |\n| usage\\_total\\_tokens | 1156 |\n\n### Example 3) Personal Assistants, parallel processing [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/?q=\\#example-3-personal-assistants-parallel-processing)\n\nA personal assistant application needs to interpret vague queries and fetch information from multiple backends, such as emails and calendars. By modeling the assistant's capabilities using Pydantic, we can dispatch the query to the correct backend and retrieve a unified response.\n\nFor instance, when you ask, \"What's on my schedule today?\", the application needs to fetch data from various sources like events, emails, and reminders. This data is stored across different backends, but the goal is to provide a consolidated summary of results.\n\nIt's important to note that the data from these sources may not be embedded in a search backend. Instead, they could be accessed through different clients like a calendar or email, spanning both personal and professional accounts.\n\nIn \\[10\\]:\n\nCopied!\n\n```\nfrom typing import Literal\n\nclass SearchClient(BaseModel):\n    query: str = Field(description=\"The search query that will go into the search bar\")\n    keywords: list[str]\n    email: str\n    source: Literal[\"gmail\", \"calendar\"]\n    date_range: DateRange\n\nclass Retrieval(BaseModel):\n    queries: list[SearchClient]\n\n```\n\nfrom typing import Literal class SearchClient(BaseModel): query: str = Field(description=\"The search query that will go into the search bar\") keywords: list\\[str\\] email: str source: Literal\\[\"gmail\", \"calendar\"\\] date\\_range: DateRange class Retrieval(BaseModel): queries: list\\[SearchClient\\]\n\nNow, we can utilize this with a straightforward query such as \"What do I have today?\".\n\nThe system will attempt to asynchronously dispatch the query to the appropriate backend.\n\nHowever, it's still crucial to remember that effectively prompting the language model is still a key aspect.\n\nIn \\[11\\]:\n\nCopied!\n\n```\nretrieval = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Retrieval,\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": f\"\"\"You are Jason's personal assistant.\\\n                He has two emails jason@work.com jason@personal.com\\\n                Today is {date.today()}\"\"\",\\\n        },\\\n        {\"role\": \"user\", \"content\": \"What do I have today for work? any new emails?\"},\\\n    ],\n)\nprint(retrieval.model_dump_json(indent=4))\n\n```\n\nretrieval = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\_model=Retrieval, messages=\\[ { \"role\": \"system\", \"content\": f\"\"\"You are Jason's personal assistant. He has two emails jason@work.com jason@personal.com Today is {date.today()}\"\"\", }, {\"role\": \"user\", \"content\": \"What do I have today for work? any new emails?\"}, \\], ) print(retrieval.model\\_dump\\_json(indent=4))\n\n```\n{\n    \"queries\": [\\\n        {\\\n            \"query\": \"work\",\\\n            \"keywords\": [\\\n                \"work\",\\\n                \"today\"\\\n            ],\\\n            \"email\": \"jason@work.com\",\\\n            \"source\": \"gmail\",\\\n            \"date_range\": {\\\n                \"chain_of_thought\": \"Check today's work schedule\",\\\n                \"start\": \"2024-03-31\",\\\n                \"end\": \"2024-03-31\"\\\n            }\\\n        },\\\n        {\\\n            \"query\": \"new emails\",\\\n            \"keywords\": [\\\n                \"email\",\\\n                \"new\"\\\n            ],\\\n            \"email\": \"jason@work.com\",\\\n            \"source\": \"gmail\",\\\n            \"date_range\": {\\\n                \"chain_of_thought\": \"Check for new emails today\",\\\n                \"start\": \"2024-03-31\",\\\n                \"end\": \"2024-03-31\"\\\n            }\\\n        }\\\n    ]\n}\n\n```\n\nTo make it more challenging, we will assign it multiple tasks, followed by a list of queries that are routed to various search backends, such as email and calendar. Not only do we dispatch to different backends, over which we have no control, but we are also likely to render them to the user in different ways.\n\nIn \\[12\\]:\n\nCopied!\n\n```\nretrieval = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=Retrieval,\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": f\"\"\"You are Jason's personal assistant.\\\n                He has two emails jason@work.com jason@personal.com\\\n                Today is {date.today()}\"\"\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"What meetings do I have today and are there any important emails I should be aware of\",\\\n        },\\\n    ],\n)\nprint(retrieval.model_dump_json(indent=4))\n\n```\n\nretrieval = client.chat.completions.create( model=\"gpt-4-1106-preview\", response\\_model=Retrieval, messages=\\[ { \"role\": \"system\", \"content\": f\"\"\"You are Jason's personal assistant. He has two emails jason@work.com jason@personal.com Today is {date.today()}\"\"\", }, { \"role\": \"user\", \"content\": \"What meetings do I have today and are there any important emails I should be aware of\", }, \\], ) print(retrieval.model\\_dump\\_json(indent=4))\n\n```\n{\n    \"queries\": [\\\n        {\\\n            \"query\": \"Jason's meetings\",\\\n            \"keywords\": [\\\n                \"meeting\",\\\n                \"appointment\",\\\n                \"schedule\",\\\n                \"calendar\"\\\n            ],\\\n            \"email\": \"jason@work.com\",\\\n            \"source\": \"calendar\",\\\n            \"date_range\": {\\\n                \"chain_of_thought\": \"Since today's date is 2024-03-31, we should look for meetings scheduled for this exact date.\",\\\n                \"start\": \"2024-03-31\",\\\n                \"end\": \"2024-03-31\"\\\n            }\\\n        }\\\n    ]\n}\n\n```\n\n### Example 4) Decomposing questions [¶](https://python.useinstructor.com/tutorials/3-0-applications-rag/?q=\\#example-4-decomposing-questions)\n\nLastly, a lightly more complex example of a problem that can be solved with structured output is decomposing questions. Where you ultimately want to decompose a question into a series of sub-questions that can be answered by a search backend. For example\n\n\"Whats the difference in populations of jason's home country and canada?\"\n\nYou'd ultimately need to know a few things\n\n1. Jason's home country\n2. The population of Jason's home country\n3. The population of Canada\n4. The difference between the two\n\nThis would not be done correctly as a single query, nor would it be done in parallel, however there are some opportunities try to be parallel since not all of the sub-questions are dependent on each other.\n\nIn \\[13\\]:\n\nCopied!\n\n```\nclass Question(BaseModel):\n    id: int = Field(..., description=\"A unique identifier for the question\")\n    query: str = Field(..., description=\"The question decomposited as much as possible\")\n    subquestions: list[int] = Field(\n        default_factory=list,\n        description=\"The subquestions that this question is composed of\",\n    )\n\nclass QueryPlan(BaseModel):\n    root_question: str = Field(..., description=\"The root question that the user asked\")\n    plan: list[Question] = Field(\n        ..., description=\"The plan to answer the root question and its subquestions\"\n    )\n\nretrieval = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=QueryPlan,\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"You are a query understanding system capable of decomposing a question into subquestions.\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"What is the difference between the population of jason's home country and canada?\",\\\n        },\\\n    ],\n)\n\nprint(retrieval.model_dump_json(indent=4))\n\n```\n\nclass Question(BaseModel): id: int = Field(..., description=\"A unique identifier for the question\") query: str = Field(..., description=\"The question decomposited as much as possible\") subquestions: list\\[int\\] = Field( default\\_factory=list, description=\"The subquestions that this question is composed of\", ) class QueryPlan(BaseModel): root\\_question: str = Field(..., description=\"The root question that the user asked\") plan: list\\[Question\\] = Field( ..., description=\"The plan to answer the root question and its subquestions\" ) retrieval = client.chat.completions.create( model=\"gpt-4-1106-preview\", response\\_model=QueryPlan, messages=\\[ { \"role\": \"system\", \"content\": \"You are a query understanding system capable of decomposing a question into subquestions.\", }, { \"role\": \"user\", \"content\": \"What is the difference between the population of jason's home country and canada?\", }, \\], ) print(retrieval.model\\_dump\\_json(indent=4))\n\n```\n{\n    \"root_question\": \"What is the difference between the population of Jason's home country and Canada?\",\n    \"plan\": [\\\n        {\\\n            \"id\": 1,\\\n            \"query\": \"What is the population of Jason's home country?\",\\\n            \"subquestions\": []\\\n        },\\\n        {\\\n            \"id\": 2,\\\n            \"query\": \"What is the population of Canada?\",\\\n            \"subquestions\": []\\\n        },\\\n        {\\\n            \"id\": 3,\\\n            \"query\": \"What is the difference between two population numbers?\",\\\n            \"subquestions\": [\\\n                1,\\\n                2\\\n            ]\\\n        }\\\n    ]\n}\n\n```\n\nI hope in this section I've exposed you to some ways we can be creative in modeling structured outputs to leverage LLMS in building some lightweight components for our systems.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/3-0-applications-rag/?q=",
      "ogUrl": "https://python.useinstructor.com/tutorials/3-0-applications-rag/",
      "title": "Applications RAG - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/3-0-applications-rag/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/3-0-applications-rag.png",
      "ogTitle": "Applications RAG - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/3-0-applications-rag.png",
      "og:title": "Applications RAG - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/3-0-applications-rag/?q=",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/3-0-applications-rag.png",
      "twitter:title": "Applications RAG - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/?q=#synthetic-data-generation)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/7-synthetic-data-generation.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/7-synthetic-data-generation.ipynb \"View source of this page\")\n\n# Synthetic Data Generation [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/?q=\\#synthetic-data-generation)\n\nRAG Applications are often tricky to evaluate, especially when you haven't obtained any user queries to begin. In this notebook, we'll see how we can use `instructor` to quickly generate synthetic questions from a dataset to benchmark your retrieval systems using some simple metrics.\n\n## Data Ingestion [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/?q=\\#data-ingestion)\n\nLet's first start by installing the required packages and ingesting the first 200 rows of the `ms-marco` dataset into our local database.\n\nIn \\[91\\]:\n\nCopied!\n\n```\n!uv pip install instructor openai datasets lancedb tantivy tenacity tqdm\n\n```\n\n!uv pip install instructor openai datasets lancedb tantivy tenacity tqdm\n\n```\nAudited 7 packages in 301ms\n\n```\n\nWe're using `lancedb` here to easily ingest large amounts of data. This is preferable since we can define our table schema using a `Pydantic` Schema and also have LanceDB automatically handle the generation of the embeddings using their `get_registry()` method that we can define as an object property.\n\nIn \\[6\\]:\n\nCopied!\n\n```\nfrom lancedb import connect\n\nDB_PATH = \"./db\"\nDB_TABLE = \"ms_marco\"\n\n# Create a db at the path `./db`\ndb = connect(DB_PATH)\n\n```\n\nfrom lancedb import connect DB\\_PATH = \"./db\" DB\\_TABLE = \"ms\\_marco\" # Create a db at the path \\`./db\\` db = connect(DB\\_PATH)\n\nIn \\[31\\]:\n\nCopied!\n\n```\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\nfunc = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n\nclass Chunk(LanceModel):\n    passage:str = func.SourceField()\n    chunk_id:str\n    embedding:Vector(func.ndims()) = func.VectorField()\n\ntable = db.create_table(DB_TABLE, schema=Chunk, exist_ok=True, mode=\"overwrite\")\n\n```\n\nfrom lancedb.pydantic import LanceModel, Vector from lancedb.embeddings import get\\_registry func = get\\_registry().get(\"openai\").create(name=\"text-embedding-3-small\") class Chunk(LanceModel): passage:str = func.SourceField() chunk\\_id:str embedding:Vector(func.ndims()) = func.VectorField() table = db.create\\_table(DB\\_TABLE, schema=Chunk, exist\\_ok=True, mode=\"overwrite\")\n\nIn \\[32\\]:\n\nCopied!\n\n```\nfrom datasets import load_dataset\n\nN_ROWS = 200\n\ndataset = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\", streaming=True).take(N_ROWS)\n\n```\n\nfrom datasets import load\\_dataset N\\_ROWS = 200 dataset = load\\_dataset(\"ms\\_marco\", \"v1.1\", split=\"train\", streaming=True).take(N\\_ROWS)\n\nIn \\[33\\]:\n\nCopied!\n\n```\n# from itertools import islice\nfirst_item = next(iter(dataset))\nfirst_item.keys()\n\n```\n\n\\# from itertools import islice first\\_item = next(iter(dataset)) first\\_item.keys()\n\nOut\\[33\\]:\n\n```\ndict_keys(['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'])\n```\n\nIn \\[36\\]:\n\nCopied!\n\n```\nfirst_item['passages']['passage_text'][:3]\n\n```\n\nfirst\\_item\\['passages'\\]\\['passage\\_text'\\]\\[:3\\]\n\nOut\\[36\\]:\n\n```\n[\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\\\n \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\\\n 'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ']\n```\n\nIn \\[34\\]:\n\nCopied!\n\n```\nimport hashlib\nfrom itertools import batched\n\ndef get_passages(dataset):\n    for row in dataset:\n        for passage in row['passages']['passage_text']:\n            yield {\n                \"passage\":passage,\n                \"chunk_id\":hashlib.md5(passage.encode()).hexdigest()\n            }\n\npassages = batched(get_passages(dataset),10)\n\nfor passage_batch in passages:\n    # print(passage_batch)\n    table.add(list(passage_batch))\n\n```\n\nimport hashlib from itertools import batched def get\\_passages(dataset): for row in dataset: for passage in row\\['passages'\\]\\['passage\\_text'\\]: yield { \"passage\":passage, \"chunk\\_id\":hashlib.md5(passage.encode()).hexdigest() } passages = batched(get\\_passages(dataset),10) for passage\\_batch in passages: # print(passage\\_batch) table.add(list(passage\\_batch))\n\n## Synthetic Questions [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/?q=\\#synthetic-questions)\n\nNow that we have the first ~2000 passages from the MS-Marco dataset ingested into our database. Let's start generating some synthetic questions using the chunks we've ingested.\n\nLet's see how we might do so using `instructor` by defining a datamodel that can help support this use-case.\n\nIn \\[35\\]:\n\nCopied!\n\n```\nfrom pydantic import BaseModel,Field\n\nclass QuestionAnswerPair(BaseModel):\n    \"\"\"\n    This model represents a pair of a question generated from a text chunk, its corresponding answer,\n    and the chain of thought leading to the answer. The chain of thought provides insight into how the answer\n    was derived from the question.\n    \"\"\"\n\n    chain_of_thought: str = Field(\n        description=\"The reasoning process leading to the answer.\"\n    )\n    question: str = Field(\n        description=\"The generated question from the text chunk.\"\n    )\n    answer: str = Field(description=\"The answer to the generated question.\")\n\n```\n\nfrom pydantic import BaseModel,Field class QuestionAnswerPair(BaseModel): \"\"\" This model represents a pair of a question generated from a text chunk, its corresponding answer, and the chain of thought leading to the answer. The chain of thought provides insight into how the answer was derived from the question. \"\"\" chain\\_of\\_thought: str = Field( description=\"The reasoning process leading to the answer.\" ) question: str = Field( description=\"The generated question from the text chunk.\" ) answer: str = Field(description=\"The answer to the generated question.\")\n\nOnce we've defined this data-model, we can then use it in an instructor call to generate a synthetic question.\n\nIn \\[42\\]:\n\nCopied!\n\n```\nfrom openai import OpenAI\nfrom instructor import from_openai\n\nclient = from_openai(OpenAI())\n\ndef generate_question(chunk:str)->QuestionAnswerPair:\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\",\\\n            },\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"Here is the text chunk: {chunk}\"\\\n            }\\\n        ],\n        response_model=QuestionAnswerPair\n    )\n\ntext_chunk = \"\"\"\nThe Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\n\"\"\"\nprint(generate_question(text_chunk).model_dump_json(indent=2))\n\n```\n\nfrom openai import OpenAI from instructor import from\\_openai client = from\\_openai(OpenAI()) def generate\\_question(chunk:str)->QuestionAnswerPair: return client.chat.completions.create( model=\"gpt-4o\", messages=\\[ { \"role\": \"system\", \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\", }, { \"role\": \"user\", \"content\": f\"Here is the text chunk: {chunk}\" } \\], response\\_model=QuestionAnswerPair ) text\\_chunk = \"\"\" The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site. \"\"\" print(generate\\_question(text\\_chunk).model\\_dump\\_json(indent=2))\n\n```\n{\n  \"chain_of_thought\": \"To form a specific question from the given text chunk, I should focus on the unique details provided about the Reserve Bank of Australia, such as its creation, functions, and assets.\",\n  \"question\": \"When was the Reserve Bank of Australia established as Australia's central bank and banknote issuing authority?\",\n  \"answer\": \"The Reserve Bank of Australia was established as Australia's central bank and banknote issuing authority on 14 January 1960.\"\n}\n\n```\n\nNow that we've seen how to generate a single question, let's see how we might be able to scale this up. We can do so by taking advantage of the `asyncio` library and `tenacity` to handle retries.\n\nIn \\[56\\]:\n\nCopied!\n\n```\nchunks = table.to_pandas()\nchunks = [item for item in chunks['passage']]\nchunks[:2]\n\n```\n\nchunks = table.to\\_pandas() chunks = \\[item for item in chunks\\['passage'\\]\\] chunks\\[:2\\]\n\nOut\\[56\\]:\n\n```\n[\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\\\n \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\"]\n```\n\nIn \\[98\\]:\n\nCopied!\n\n```\nfrom asyncio import Semaphore\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom openai import AsyncOpenAI\nimport asyncio\n\nclient = from_openai(AsyncOpenAI())\n\nasync def generate_questions(chunks:list[str],max_queries:int):\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    async def generate_question(chunk:str,sem:Semaphore)->tuple[QuestionAnswerPair,str]:\n        async with sem:\n            return (await client.chat.completions.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[\\\n                    {\\\n                        \"role\": \"system\",\\\n                        \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\",\\\n                    },\\\n                    {\\\n                        \"role\": \"user\",\\\n                        \"content\": f\"Here is the text chunk: {chunk}\"\\\n                    }\\\n                ],\n                response_model=QuestionAnswerPair\n            ), chunk)\n    sem = Semaphore(max_queries)\n    coros = [\\\n        generate_question(chunk,sem)\\\n        for chunk in\\\n        chunks\\\n    ]\n    return await asyncio.gather(*coros)\n\nquestions = await generate_questions(chunks[:300],10)\n\n```\n\nfrom asyncio import Semaphore from tenacity import retry, stop\\_after\\_attempt, wait\\_exponential from openai import AsyncOpenAI import asyncio client = from\\_openai(AsyncOpenAI()) async def generate\\_questions(chunks:list\\[str\\],max\\_queries:int): @retry(stop=stop\\_after\\_attempt(3), wait=wait\\_exponential(multiplier=1, min=4, max=10)) async def generate\\_question(chunk:str,sem:Semaphore)->tuple\\[QuestionAnswerPair,str\\]: async with sem: return (await client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\[ { \"role\": \"system\", \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\", }, { \"role\": \"user\", \"content\": f\"Here is the text chunk: {chunk}\" } \\], response\\_model=QuestionAnswerPair ), chunk) sem = Semaphore(max\\_queries) coros = \\[ generate\\_question(chunk,sem) for chunk in chunks \\] return await asyncio.gather(\\*coros) questions = await generate\\_questions(chunks\\[:300\\],10)\n\n## Benchmarking Retrieval [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/?q=\\#benchmarking-retrieval)\n\nNow that we've generated a list of questions to query our database with, let's do a quick benchmark to see how full text search compares against that of hybrid search. We'll use two simple metrics here - Mean Reciprocal Rank ( MRR ) and Recall.\n\nLet's start by making sure we have an inverted index created on our table above that we can perform full text search on\n\nIn \\[64\\]:\n\nCopied!\n\n```\ntable.create_fts_index(\"passage\",replace=True)\n\n```\n\ntable.create\\_fts\\_index(\"passage\",replace=True)\n\nThis allows us to then use the `.search` function on each table to query it using full text search. Let's see an example below.\n\nIn \\[67\\]:\n\nCopied!\n\n```\nfor entry in table.search(\"RBA\",query_type=\"fts\").limit(2).to_list():\n    print(entry['passage'])\n\n```\n\nfor entry in table.search(\"RBA\",query\\_type=\"fts\").limit(2).to\\_list(): print(entry\\['passage'\\])\n\n```\nA rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.\nResults-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;\n\n```\n\n### Metrics [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/?q=\\#metrics)\n\nNow that we've figured out how we might be able to query our table using full text search. Let's take a step back and see how we can implement some metrics to quantiatively evaluate the retrieved items. It's important to note that when we want to evalute the quality of our listings, we always take it at some subset of k.\n\nThis is important because k is often constrained by a business outcome and can help us determine how well our solution works\n\nEg. Here are some hypothetical scenarios\n\n- k=5 : We'd like to display some recomended items based of a user query (Eg. Help me plan out a dinner with Jonathan next week -> Display 5 possible actions)\n- k=10 : We have a small carousel with recomended items for a user to buy\n- k=25 : We're using a re-ranker, is it filtering out the irrelevant chunks from the relevant chunks well?\n- k=50 : We have a pipeline that fetches information for a model to respond with, are we fetching all relevant bits of information\n\n#### Reciprocal Rank [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/?q=\\#reciprocal-rank)\n\nReciprocal Rank Imagine we're spotify and we want to suggest a couple of songs to the user. Which is a better result among the two lists of retrieved songs below? ( Note that 2 is the answer we want )\n\n- \\[0,1,2,3,4\\]\n- \\[0,1,3,4,2\\]\n\nObviously if we're suggesting songs to the user, we want the first relevant song to be listed as early as possible! Therefore we'd prefer 1 over 2 in the example above because 2 is ordered earlier in the first case. A metric that works well for this is the Reciprocal Rank (RR).\n\n![](https://python.useinstructor.com/tutorials/img/mrr_eqn.png)\n\nIn \\[84\\]:\n\nCopied!\n\n```\ndef rr(results,labels):\n    return max(\n        [round(1/(results.index(label)+1),2) if label in results else 0\\\n        for label in labels]\n\n    )\n\n```\n\ndef rr(results,labels): return max( \\[round(1/(results.index(label)+1),2) if label in results else 0 for label in labels\\] )\n\nThis is an aggressive metric and once we get to an position of > 10, the value doesn't change much anymore. Most of the big changes happen at indexes < 10.\n\n#### Recall [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/?q=\\#recall)\n\nAnother metric that we can track is recall which measures how many of our retrieved items were retrieved.\n\n![](https://python.useinstructor.com/tutorials/img/recall_eqn.png)\n\nIn \\[69\\]:\n\nCopied!\n\n```\ndef recall(results,relevant_chunks):\n    return sum([1 if chunk in results else 0 for chunk in relevant_chunks])/len(relevant_chunks)\n\n```\n\ndef recall(results,relevant\\_chunks): return sum(\\[1 if chunk in results else 0 for chunk in relevant\\_chunks\\])/len(relevant\\_chunks)\n\n## Using Our Questions [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/?q=\\#using-our-questions)\n\nNow that we've seen two metrics that we can use and how we might be able to generate some synthetic questions, let's try it out on an actual question.\n\nTo do so, we'll first generate a unique chunk id for our original passage that we generated the question from.\n\nWe'll then compare the chunk\\_ids of the retrieved chunks and then compute the `mrr` and the `recall` of the retrieved results.\n\nIn \\[86\\]:\n\nCopied!\n\n```\nimport hashlib\nsample_question,chunk = questions[0]\n\nchunk_id = hashlib.md5(chunk.encode()).hexdigest()\nchunk_id, sample_question.question, chunk\n\n```\n\nimport hashlib sample\\_question,chunk = questions\\[0\\] chunk\\_id = hashlib.md5(chunk.encode()).hexdigest() chunk\\_id, sample\\_question.question, chunk\n\nOut\\[86\\]:\n\n```\n('b6d9bf888fd53590ee69a913bd9bf8a4',\n \"What factors influence the average salary for people with a bachelor's degree?\",\n \"However, the average salary for people with a bachelor's degree varies widely based upon several factors, including their major, job position, location and years of experience. The National Association of Colleges and Employers conducted a salary survey that determined the average starting salary for graduates of various bachelor's degree programs.\")\n```\n\nIn \\[81\\]:\n\nCopied!\n\n```\nretrieved_results = table.search(sample_question.question,query_type='fts').limit(25).to_list()\nretrieved_chunk_ids = [item['chunk_id'] for item in retrieved_results]\n\nretrieved_chunk_ids[:3]\n\n```\n\nretrieved\\_results = table.search(sample\\_question.question,query\\_type='fts').limit(25).to\\_list() retrieved\\_chunk\\_ids = \\[item\\['chunk\\_id'\\] for item in retrieved\\_results\\] retrieved\\_chunk\\_ids\\[:3\\]\n\nOut\\[81\\]:\n\n```\n['b6d9bf888fd53590ee69a913bd9bf8a4',\\\n '7a0254c9dc709220367857dcb67f2c8d',\\\n '04e7e6f91463033aa87b4104ea16b477']\n```\n\nWe can now compute the results for the retrieved items that we've obtained using full text search relative to the ground truth label that we have - the original chunk that we generated it from\n\nIn \\[85\\]:\n\nCopied!\n\n```\nrecall(retrieved_chunk_ids,[chunk_id]), rr(retrieved_chunk_ids,[chunk_id])\n\n```\n\nrecall(retrieved\\_chunk\\_ids,\\[chunk\\_id\\]), rr(retrieved\\_chunk\\_ids,\\[chunk\\_id\\])\n\nOut\\[85\\]:\n\n```\n(1.0, 1.0)\n```\n\nScaling it up for different values of `k`, where we can see how this value changes for different subsets of the retrieved items is relatively simple.\n\nWe can generate this mapping automatically using `itertools.product`\n\nIn \\[112\\]:\n\nCopied!\n\n```\nfrom itertools import product\n\nSIZES = [3,5,10,15,25]\nMETRICS = [\\\n    [\"mrr\",rr],\\\n    [\"recall\",recall]\\\n]\n\nscore_fns = {}\n\nfor metric,size in product(METRICS,SIZES):\n    metric_name, score_fn = metric\n    score_fns[f\"{metric_name}@{size}\"] = lambda predictions,labels, fn=score_fn, k=size: fn(predictions[:k],labels) # type: ignore\n\n```\n\nfrom itertools import product SIZES = \\[3,5,10,15,25\\] METRICS = \\[ \\[\"mrr\",rr\\], \\[\"recall\",recall\\] \\] score\\_fns = {} for metric,size in product(METRICS,SIZES): metric\\_name, score\\_fn = metric score\\_fns\\[f\"{metric\\_name}@{size}\"\\] = lambda predictions,labels, fn=score\\_fn, k=size: fn(predictions\\[:k\\],labels) # type: ignore\n\n## Running an Evaluation [¶](https://python.useinstructor.com/tutorials/7-synthetic-data-generation/?q=\\#running-an-evaluation)\n\nWe can now use the code above to run a test to see how our full text search performs for our synthetic questions.\n\nIn \\[114\\]:\n\nCopied!\n\n```\nimport hashlib\nfrom tqdm import tqdm\n\nfts_results = []\n\nfor sample_qn, chunk in tqdm(questions):\n    chunk_id = hashlib.md5(chunk.encode()).hexdigest()\n    cleaned_question = ''.join(char for char in sample_qn.question if char.isalnum() or char.isspace())\n    retrieved_results = table.search(cleaned_question, query_type='fts').limit(25).to_list()\n    retrieved_chunk_ids = [item['chunk_id'] for item in retrieved_results]\n\n    fts_results.append(\n        {\n            metric: score_fn(retrieved_chunk_ids,[chunk_id])\n            for metric,score_fn\n            in score_fns.items()\n        }\n    )\n\n```\n\nimport hashlib from tqdm import tqdm fts\\_results = \\[\\] for sample\\_qn, chunk in tqdm(questions): chunk\\_id = hashlib.md5(chunk.encode()).hexdigest() cleaned\\_question = ''.join(char for char in sample\\_qn.question if char.isalnum() or char.isspace()) retrieved\\_results = table.search(cleaned\\_question, query\\_type='fts').limit(25).to\\_list() retrieved\\_chunk\\_ids = \\[item\\['chunk\\_id'\\] for item in retrieved\\_results\\] fts\\_results.append( { metric: score\\_fn(retrieved\\_chunk\\_ids,\\[chunk\\_id\\]) for metric,score\\_fn in score\\_fns.items() } )\n\n```\n100%|██████████| 300/300 [00:07<00:00, 41.64it/s]\n\n```\n\nIn \\[115\\]:\n\nCopied!\n\n```\nimport pandas as pd\n\ndf = pd.DataFrame(fts_results)\ndf.mean()\n\n```\n\nimport pandas as pd df = pd.DataFrame(fts\\_results) df.mean()\n\nOut\\[115\\]:\n\n```\nmrr@3        0.784267\nmrr@5        0.791267\nmrr@10       0.797633\nmrr@15       0.798133\nmrr@25       0.798433\nrecall@3     0.896667\nrecall@5     0.926667\nrecall@10    0.973333\nrecall@15    0.980000\nrecall@25    0.986667\ndtype: float64\n```\n\nWe can see that on average full text search is able to surface the relevant item 97-98% of the time if we take `k=10` and that we have the relevant item in between the first and second item here.\n\nNow, because these are synthetic question, there's likely to be a large amount of overlap in the phrases used in the questions and the original source text, leading to the high values.\n\nIn actual production applications and your domain specific dataset, it's useful to do these experiments and see what works best for your needs.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/7-synthetic-data-generation/?q=",
      "ogUrl": "https://python.useinstructor.com/tutorials/7-synthetic-data-generation/",
      "title": "Synthetic Data Generation - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/7-synthetic-data-generation/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/7-synthetic-data-generation.png",
      "ogTitle": "Synthetic Data Generation - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/7-synthetic-data-generation.png",
      "og:title": "Synthetic Data Generation - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/7-synthetic-data-generation/?q=",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/7-synthetic-data-generation.png",
      "twitter:title": "Synthetic Data Generation - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=#knowledge-graphs-for-complex-topics)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/5-knowledge-graphs.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/5-knowledge-graphs.ipynb \"View source of this page\")\n\n# Knowledge Graphs for Complex Topics [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=\\#knowledge-graphs-for-complex-topics)\n\n# Introduction [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=\\#introduction)\n\n**What is a knowledge graph?**\n\nA knowledge graph, also known as a semantic network, represents real-world entities and their relationships. It consists of nodes, edges, and labels. Nodes can represent any entity, while edges define the connections between them. For example, a node representing an author like \"J.K. Rowling\" can be connected to another node representing one of her books, \"Harry Potter\", with the edge \"author of\".\n\n**Applications of knowledge graphs**\n\nKnowledge graphs have various applications, including:\n\n- Search Engines: They enhance search results by incorporating semantic-search information from diverse sources.\n- Recommendation Systems: They suggest products or services based on user behavior and preferences.\n- Natural Language Processing: They aid in understanding and generating human language.\n- Data Integration: They facilitate the integration of data from different sources by identifying relationships.\n- Artificial Intelligence and Machine Learning: They provide contextual information to improve decision-making.\n\n* * *\n\n## Setup and Dependencies [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=\\#setup-and-dependencies)\n\nToday, we're going to use the [`instructor`](https://github.com/jxnl/instructor) library to simplify the interaction between OpenAI and our code. Along with [Graphviz](https://graphviz.org) library to bring structure to our intricate subjects and have a graph visualization.\n\nIn \\[2\\]:\n\nCopied!\n\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\n```\n\nimport instructor from openai import OpenAI client = instructor.patch(OpenAI())\n\nInstall the Graphviz based on your operation system [https://graphviz.org/download/](https://graphviz.org/download/)\n\n## Node and Edge Classes [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=\\#node-and-edge-classes)\n\nWe begin by modeling our knowledge graph with Node and Edge objects.\n\nNode objects represent key concepts or entities, while Edge objects signify the relationships between them.\n\nIn \\[3\\]:\n\nCopied!\n\n```\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\n```\n\nfrom pydantic import BaseModel, Field from typing import Optional class Node(BaseModel): id: int label: str color: str class Edge(BaseModel): source: int target: int label: str color: str = \"black\"\n\n## `KnowledgeGraph` Class [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=\\#knowledgegraph-class)\n\nThe `KnowledgeGraph` class combines nodes and edges to create a comprehensive graph structure. It includes lists of nodes and edges, where each node represents a key concept or entity, and each edge represents a relationship between two nodes.\n\nLater on, you'll see that we designed this class to match the graph object in the graphviz library, which makes it easier to visualize our graph.\n\nThe `visualize_knowledge_graph` function is used to visualize a knowledge graph. It takes a `KnowledgeGraph` object as input, which contains nodes and edges. The function utilizes the `graphviz` library to generate a directed graph ( `Digraph`). Each node and edge from the `KnowledgeGraph` is added to the `Digraph` with their respective attributes (id, label, color). Finally, the graph is rendered and displayed.\n\nIn \\[4\\]:\n\nCopied!\n\n```\nfrom graphviz import Digraph\nfrom IPython.display import display\n\nclass KnowledgeGraph(BaseModel):\n    nodes: list[Node] = Field(..., default_factory=list)  # A list of nodes in the knowledge graph.\n    edges: list[Edge] = Field(..., default_factory=list)  # A list of edges in the knowledge graph.\n\n    def visualize_knowledge_graph(self):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:\n            dot.node(name=str(node.id), label=node.label, color=node.color)\n        for edge in self.edges:\n            dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n\n        return display(dot)\n\n```\n\nfrom graphviz import Digraph from IPython.display import display class KnowledgeGraph(BaseModel): nodes: list\\[Node\\] = Field(..., default\\_factory=list) # A list of nodes in the knowledge graph. edges: list\\[Edge\\] = Field(..., default\\_factory=list) # A list of edges in the knowledge graph. def visualize\\_knowledge\\_graph(self): dot = Digraph(comment=\"Knowledge Graph\") for node in self.nodes: dot.node(name=str(node.id), label=node.label, color=node.color) for edge in self.edges: dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color) return display(dot)\n\n## Generating the Knowledge Graph [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=\\#generating-the-knowledge-graph)\n\n### generate\\_graph function [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=\\#generate_graph-function)\n\nThe `generate_graph` function uses OpenAI's model to create a KnowledgeGraph object from an input string.\n\nIt requests the model to interpret the input as a detailed knowledge graph and uses the response to form the KnowledgeGraph object.\n\nIn \\[8\\]:\n\nCopied!\n\n```\ndef generate_graph(input) -> KnowledgeGraph:\n    return client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"Help me understand the following by describing it as small knowledge graph: {input}\",\\\n            }\\\n        ],\n        response_model=KnowledgeGraph,\n    )\n\n```\n\ndef generate\\_graph(input) -> KnowledgeGraph: return client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[ { \"role\": \"user\", \"content\": f\"Help me understand the following by describing it as small knowledge graph: {input}\", } \\], response\\_model=KnowledgeGraph, )\n\nIn \\[9\\]:\n\nCopied!\n\n```\ngenerate_graph(\"Explain quantum mechanics\").visualize_knowledge_graph()\n\n```\n\ngenerate\\_graph(\"Explain quantum mechanics\").visualize\\_knowledge\\_graph()\n\n![No description has been provided for this image](<Base64-Image-Removed>)\n\n## Advanced: Accumulating Knowledge Graphs [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=\\#advanced-accumulating-knowledge-graphs)\n\nWhen dealing with larger datasets, or knowledge that grows over time, processing them all at once can be challenging due to limitations in prompt length or the complexity of the content. In such cases, an iterative approach to building the knowledge graph can be beneficial. This method involves processing the text in smaller, manageable chunks and updating the graph with new information from each chunk.\n\n### What are the benefits of this approach? [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=\\#what-are-the-benefits-of-this-approach)\n\n- Scalability: This approach can handle large datasets by breaking them down into smaller, more manageable pieces.\n\n- Flexibility: It allows for dynamic updates to the graph, accommodating new information as it becomes available.\n\n- Efficiency: Processing smaller chunks of text can be more efficient and less prone to errors or omissions.\n\n\n### What has changed? [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=\\#what-has-changed)\n\nThe previous example provided a basic structure, while this new example introduces additional complexity and functionality. The Node and Edge classes now have a **hash** method, allowing them to be used in sets and simplifying duplicate handling.\n\nThe KnowledgeGraph class has been enhanced with two new methods: `update` and `draw`.\n\nIn the KnowledgeGraph class, the nodes and edges fields are now optional, offering greater flexibility.\n\nThe `update` method enables the merging and removal of duplicates from two graphs.\n\nThe `draw` method includes a prefix parameter, making it easier to create different graph versions during iterations.\n\nIn \\[10\\]:\n\nCopied!\n\n```\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\n    def __hash__(self) -> int:\n        return hash((id, self.label))\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\n    def __hash__(self) -> int:\n        return hash((self.source, self.target, self.label))\n\n```\n\nclass Node(BaseModel): id: int label: str color: str def \\_\\_hash\\_\\_(self) -> int: return hash((id, self.label)) class Edge(BaseModel): source: int target: int label: str color: str = \"black\" def \\_\\_hash\\_\\_(self) -> int: return hash((self.source, self.target, self.label))\n\nIn \\[11\\]:\n\nCopied!\n\n```\nclass KnowledgeGraph(BaseModel):\n    # Optional list of nodes and edges in the knowledge graph\n    nodes: Optional[list[Node]] = Field(..., default_factory=list)\n    edges: Optional[list[Edge]] = Field(..., default_factory=list)\n\n    def update(self, other: \"KnowledgeGraph\") -> \"KnowledgeGraph\":\n        # This method updates the current graph with the other graph, deduplicating nodes and edges.\n        return KnowledgeGraph(\n            nodes=list(set(self.nodes + other.nodes)),  # Combine and deduplicate nodes\n            edges=list(set(self.edges + other.edges)),  # Combine and deduplicate edges\n        )\n\n\n    def visualize_knowledge_graph(self):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:\n            dot.node(str(node.id), node.label, color=node.color)\n        for edge in self.edges:\n            dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n\n        return display(dot)\n\n```\n\nclass KnowledgeGraph(BaseModel): # Optional list of nodes and edges in the knowledge graph nodes: Optional\\[list\\[Node\\]\\] = Field(..., default\\_factory=list) edges: Optional\\[list\\[Edge\\]\\] = Field(..., default\\_factory=list) def update(self, other: \"KnowledgeGraph\") -> \"KnowledgeGraph\": # This method updates the current graph with the other graph, deduplicating nodes and edges. return KnowledgeGraph( nodes=list(set(self.nodes + other.nodes)), # Combine and deduplicate nodes edges=list(set(self.edges + other.edges)), # Combine and deduplicate edges ) def visualize\\_knowledge\\_graph(self): dot = Digraph(comment=\"Knowledge Graph\") for node in self.nodes: dot.node(str(node.id), node.label, color=node.color) for edge in self.edges: dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color) return display(dot)\n\n### Generate iterative graphs [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=\\#generate-iterative-graphs)\n\nThe updated `generate_graph` function is specifically designed to handle a list of inputs iteratively. It updates the graph with each new piece of information.\n\nUpon closer inspection, this pattern resembles a common programming technique known as a \"reduce\" or \"fold\" function. A simple example of this would be iterating over a list to find the sum of all the elements squared.\n\nHere's an example in Python:\n\n```\ncur_state = 0\nfor i in [1, 2, 3, 4, 5]:\n    cur_state += i**2\nprint(cur_state)\n\n```\n\nIn \\[12\\]:\n\nCopied!\n\n```\ndef generate_graph(input: list[str]) -> KnowledgeGraph:\n    # Initialize an empty KnowledgeGraph\n    cur_state = KnowledgeGraph()\n\n    # Iterate over the input list\n    for i, inp in enumerate(input):\n        new_updates = client.chat.completions.create(\n            model=\"gpt-4-1106-preview\",\n            messages=[\\\n                {\\\n                    \"role\": \"system\",\\\n                    \"content\": \"\"\"You are an iterative knowledge graph builder.\\\n                    You are given the current state of the graph, and you must append the nodes and edges\\\n                    to it Do not procide any duplcates and try to reuse nodes as much as possible.\"\"\",\\\n                },\\\n                {\\\n                    \"role\": \"user\",\\\n                    \"content\": f\"\"\"Extract any new nodes and edges from the following:\\\n                    # Part {i}/{len(input)} of the input:\\\n\\\n                    {inp}\"\"\",\\\n                },\\\n                {\\\n                    \"role\": \"user\",\\\n                    \"content\": f\"\"\"Here is the current state of the graph:\\\n                    {cur_state.model_dump_json(indent=2)}\"\"\",\\\n                },\\\n            ],\n            response_model=KnowledgeGraph,\n        )  # type: ignore\n\n        # Update the current state with the new updates\n        cur_state = cur_state.update(new_updates)\n\n        # Draw the current state of the graph\n        cur_state.visualize_knowledge_graph()\n\n    # Return the final state of the KnowledgeGraph\n    return cur_state\n\n```\n\ndef generate\\_graph(input: list\\[str\\]) -> KnowledgeGraph: # Initialize an empty KnowledgeGraph cur\\_state = KnowledgeGraph() # Iterate over the input list for i, inp in enumerate(input): new\\_updates = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[ { \"role\": \"system\", \"content\": \"\"\"You are an iterative knowledge graph builder. You are given the current state of the graph, and you must append the nodes and edges to it Do not procide any duplcates and try to reuse nodes as much as possible.\"\"\", }, { \"role\": \"user\", \"content\": f\"\"\"Extract any new nodes and edges from the following: # Part {i}/{len(input)} of the input: {inp}\"\"\", }, { \"role\": \"user\", \"content\": f\"\"\"Here is the current state of the graph: {cur\\_state.model\\_dump\\_json(indent=2)}\"\"\", }, \\], response\\_model=KnowledgeGraph, ) # type: ignore # Update the current state with the new updates cur\\_state = cur\\_state.update(new\\_updates) # Draw the current state of the graph cur\\_state.visualize\\_knowledge\\_graph() # Return the final state of the KnowledgeGraph return cur\\_state\n\n### Examples Use Case [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=\\#examples-use-case)\n\nIn this approach, we process the text in manageable chunks, one at a time.\n\nThis method is particularly beneficial when dealing with extensive text that may not fit into a single prompt.\n\nIt is especially useful in scenarios such as constructing a knowledge graph for a complex topic, where the information is distributed across multiple documents or sections.\n\nIn \\[13\\]:\n\nCopied!\n\n```\ntext_chunks = [\\\n    \"Jason knows a lot about quantum mechanics. He is a physicist. He is a professor\",\\\n    \"Professors are smart.\",\\\n    \"Sarah knows Jason and is a student of his.\",\\\n    \"Sarah is a student at the University of Toronto. and UofT is in Canada.\",\\\n]\n\ngraph: KnowledgeGraph = generate_graph(text_chunks)\n\n```\n\ntext\\_chunks = \\[ \"Jason knows a lot about quantum mechanics. He is a physicist. He is a professor\", \"Professors are smart.\", \"Sarah knows Jason and is a student of his.\", \"Sarah is a student at the University of Toronto. and UofT is in Canada.\", \\] graph: KnowledgeGraph = generate\\_graph(text\\_chunks)\n\n![No description has been provided for this image](<Base64-Image-Removed>)\n\n![No description has been provided for this image](<Base64-Image-Removed>)\n\n![No description has been provided for this image](<Base64-Image-Removed>)\n\n![No description has been provided for this image](<Base64-Image-Removed>)\n\n## Conclusion [¶](https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=\\#conclusion)\n\nThis tutorial shows how to generate and visualize a knowledge graph for complex topics. It also demonstrates how to extract graphic knowledge from the language model or provided text. The tutorial highlights the iterative process of building the knowledge graph by processing text in smaller chunks and updating the graph with new information.\n\nUsing this approach, we can extract various things, including:\n\n1. People and their relationships in a story.\n\n```\nclass People(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Relationship(BaseModel):\n    id: str\n    source: str\n    target: str\n    label: str\n    description: str\n\nclass Story(BaseModel):\n    people: List[People]\n    relationships: List[Relationship]\n\n```\n\n2. Task dependencies and action items from a transcript.\n\n```\nclass Task(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Participant(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Assignment(BaseModel):\n    id: str\n    source: str\n    target: str\n    label: str\n    description: str\n\nclass Transcript(BaseModel):\n    tasks: List[Task]\n    participants: List[Participant]\n    assignments: List[Assignment]\n\n```\n\n3. Key concepts and their relationships from a research paper.\n4. Entities and their relationships from a news article.\n\nAs an exercise, try to implement one of the above examples.\n\nAll of them will follow an idea of iteratively extracting more and more information and accumulating it into some state.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=",
      "ogUrl": "https://python.useinstructor.com/tutorials/5-knowledge-graphs/",
      "title": "Knowledge Graphs - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/5-knowledge-graphs/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/5-knowledge-graphs.png",
      "ogTitle": "Knowledge Graphs - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/5-knowledge-graphs.png",
      "og:title": "Knowledge Graphs - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/5-knowledge-graphs/?q=",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/5-knowledge-graphs.png",
      "twitter:title": "Knowledge Graphs - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/3-1-validation-rag/?q=#understanding-validators)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/3-1-validation-rag.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/3-1-validation-rag.ipynb \"View source of this page\")\n\n# Understanding Validators [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/?q=\\#understanding-validators)\n\nPydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic [docs](https://docs.pydantic.dev/latest/) on validators.\n\nThen we'll bring it all together into the context of RAG from the previous notebook.\n\nValidators will enable us to control outputs by defining a function like so:\n\n```\ndef validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n\n```\n\nBefore we get started lets go over the general shape of a validator:\n\n## Defining Validator Functions [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/?q=\\#defining-validator-functions)\n\nIn \\[18\\]:\n\nCopied!\n\n```\nfrom typing import Annotated\nfrom pydantic import BaseModel, AfterValidator, WithJsonSchema\n\ndef name_must_contain_space(v: str) -> str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v\n\ndef uppercase_name(v: str) -> str:\n    return v.upper()\n\nFullName = Annotated[\\\n    str,\\\n    AfterValidator(name_must_contain_space),\\\n    AfterValidator(uppercase_name),\\\n    WithJsonSchema(\\\n        {\\\n            \"type\": \"string\",\\\n            \"description\": \"The user's full name\",\\\n        }\\\n    )]\n\nclass UserDetail(BaseModel):\n    age: int\n    name: FullName\n\n```\n\nfrom typing import Annotated from pydantic import BaseModel, AfterValidator, WithJsonSchema def name\\_must\\_contain\\_space(v: str) -> str: if \" \" not in v: raise ValueError(\"Name must contain a space.\") return v def uppercase\\_name(v: str) -> str: return v.upper() FullName = Annotated\\[ str, AfterValidator(name\\_must\\_contain\\_space), AfterValidator(uppercase\\_name), WithJsonSchema( { \"type\": \"string\", \"description\": \"The user's full name\", } )\\] class UserDetail(BaseModel): age: int name: FullName\n\nIn \\[19\\]:\n\nCopied!\n\n```\nUserDetail(age=30, name=\"Jason Liu\")\n\n```\n\nUserDetail(age=30, name=\"Jason Liu\")\n\nOut\\[19\\]:\n\n```\nUserDetail(age=30, name='JASON LIU')\n```\n\nIn \\[20\\]:\n\nCopied!\n\n```\nUserDetail.model_json_schema()\n\n```\n\nUserDetail.model\\_json\\_schema()\n\nOut\\[20\\]:\n\n```\n{'properties': {'age': {'title': 'Age', 'type': 'integer'},\n  'name': {'description': \"The user's full name\",\n   'title': 'Name',\n   'type': 'string'}},\n 'required': ['age', 'name'],\n 'title': 'UserDetail',\n 'type': 'object'}\n```\n\nIn \\[21\\]:\n\nCopied!\n\n```\ntry:\n    person = UserDetail.model_validate({\"age\": 24, \"name\": \"Jason\"})\nexcept Exception as e:\n    print(e)\n\n```\n\ntry: person = UserDetail.model\\_validate({\"age\": 24, \"name\": \"Jason\"}) except Exception as e: print(e)\n\n```\n1 validation error for UserDetail\nname\n  Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n\n```\n\n## Using Field [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/?q=\\#using-field)\n\nWe can also use the `Field` class to define validators. This is useful when we want to define a validator for a field that is primative, like a string or integer which supports a limited number of validators.\n\nIn \\[22\\]:\n\nCopied!\n\n```\nfrom pydantic import Field\n\nAge = Annotated[int, Field(gt=0)]\n\nclass UserDetail(BaseModel):\n    age: Age\n    name: FullName\n\ntry:\n    person = UserDetail(age=-10, name=\"Jason\")\nexcept Exception as e:\n    print(e)\n\n```\n\nfrom pydantic import Field Age = Annotated\\[int, Field(gt=0)\\] class UserDetail(BaseModel): age: Age name: FullName try: person = UserDetail(age=-10, name=\"Jason\") except Exception as e: print(e)\n\n```\n2 validation errors for UserDetail\nage\n  Input should be greater than 0 [type=greater_than, input_value=-10, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.5/v/greater_than\nname\n  Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n\n```\n\n## Providing Context [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/?q=\\#providing-context)\n\nIn \\[7\\]:\n\nCopied!\n\n```\nfrom pydantic import ValidationInfo\n\ndef message_cannot_have_blacklisted_words(v: str, info: ValidationInfo) -> str:\n    blacklist = info.context.get(\"blacklist\", [])\n    for word in blacklist:\n        assert word not in v.lower(), f\"`{word}` was found in the message `{v}`\"\n    return v\n\nModeratedStr = Annotated[str, AfterValidator(message_cannot_have_blacklisted_words)]\n\nclass Response(BaseModel):\n    message: ModeratedStr\n\ntry:\n    Response.model_validate(\n        {\"message\": \"I will hurt them.\"},\n        context={\n            \"blacklist\": {\n                \"rob\",\n                \"steal\",\n                \"hurt\",\n                \"kill\",\n                \"attack\",\n            }\n        },\n    )\nexcept Exception as e:\n    print(e)\n\n```\n\nfrom pydantic import ValidationInfo def message\\_cannot\\_have\\_blacklisted\\_words(v: str, info: ValidationInfo) -> str: blacklist = info.context.get(\"blacklist\", \\[\\]) for word in blacklist: assert word not in v.lower(), f\"\\`{word}\\` was found in the message \\`{v}\\`\" return v ModeratedStr = Annotated\\[str, AfterValidator(message\\_cannot\\_have\\_blacklisted\\_words)\\] class Response(BaseModel): message: ModeratedStr try: Response.model\\_validate( {\"message\": \"I will hurt them.\"}, context={ \"blacklist\": { \"rob\", \"steal\", \"hurt\", \"kill\", \"attack\", } }, ) except Exception as e: print(e)\n\n```\n1 validation error for Response\nmessage\n  Assertion failed, `hurt` was found in the message `I will hurt them.` [type=assertion_error, input_value='I will hurt them.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/assertion_error\n\n```\n\n## Using OpenAI Moderation [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/?q=\\#using-openai-moderation)\n\nTo enhance our validation measures, we'll extend the scope to flag any answer that contains hateful content, harassment, or similar issues. OpenAI offers a moderation endpoint that addresses these concerns, and it's freely available when using OpenAI models.\n\nWith the `instructor` library, this is just one function edit away:\n\nIn \\[13\\]:\n\nCopied!\n\n```\nfrom typing import Annotated\nfrom pydantic import AfterValidator\nfrom instructor import openai_moderation\n\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\n# This uses Annotated which is a new feature in Python 3.9\n# To define custom metadata for a type hint.\nModeratedStr = Annotated[str, AfterValidator(openai_moderation(client=client))]\n\nclass Response(BaseModel):\n    message: ModeratedStr\n\ntry:\n    Response(message=\"I want to make them suffer the consequences\")\nexcept Exception as e:\n    print(e)\n\n```\n\nfrom typing import Annotated from pydantic import AfterValidator from instructor import openai\\_moderation import instructor from openai import OpenAI client = instructor.patch(OpenAI()) # This uses Annotated which is a new feature in Python 3.9 # To define custom metadata for a type hint. ModeratedStr = Annotated\\[str, AfterValidator(openai\\_moderation(client=client))\\] class Response(BaseModel): message: ModeratedStr try: Response(message=\"I want to make them suffer the consequences\") except Exception as e: print(e)\n\n```\n1 validation error for Response\nmessage\n  Value error, `I want to make them suffer the consequences` was flagged for harassment, harassment_threatening, violence, harassment/threatening [type=value_error, input_value='I want to make them suffer the consequences', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n\n```\n\n## General Validator [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/?q=\\#general-validator)\n\nIn \\[ \\]:\n\nCopied!\n\n```\nfrom instructor import llm_validator\n\nHealthTopicStr = Annotated[\\\n    str,\\\n    AfterValidator(\\\n        llm_validator(\\\n            \"don't talk about any other topic except health best practices and topics\",\\\n            client=client,\\\n        )\\\n    ),\\\n]\n\nclass AssistantMessage(BaseModel):\n    message: HealthTopicStr\n\nAssistantMessage(\n    message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\"\n)\n\n```\n\nfrom instructor import llm\\_validator HealthTopicStr = Annotated\\[ str, AfterValidator( llm\\_validator( \"don't talk about any other topic except health best practices and topics\", client=client, ) ), \\] class AssistantMessage(BaseModel): message: HealthTopicStr AssistantMessage( message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\" )\n\n### Avoiding hallucination with citations [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/?q=\\#avoiding-hallucination-with-citations)\n\nWhen incorporating external knowledge bases, it's crucial to ensure that the agent uses the provided context accurately and doesn't fabricate responses. Validators can be effectively used for this purpose. We can illustrate this with an example where we validate that a provided citation is actually included in the referenced text chunk:\n\nIn \\[27\\]:\n\nCopied!\n\n```\nfrom pydantic import ValidationInfo\n\ndef citation_exists(v: str, info: ValidationInfo):\n    context = info.context\n    if context:\n        context = context.get(\"text_chunk\")\n        if v not in context:\n            raise ValueError(f\"Citation `{v}` not found in text, only use citations from the text.\")\n    return v\n\nCitation = Annotated[str, AfterValidator(citation_exists)]\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: Citation\n\ntry:\n    AnswerWithCitation.model_validate(\n        {\n            \"answer\": \"Blueberries are packed with protein\",\n            \"citation\": \"Blueberries contain high levels of protein\",\n        },\n        context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"},\n    )\nexcept Exception as e:\n    print(e)\n\n```\n\nfrom pydantic import ValidationInfo def citation\\_exists(v: str, info: ValidationInfo): context = info.context if context: context = context.get(\"text\\_chunk\") if v not in context: raise ValueError(f\"Citation \\`{v}\\` not found in text, only use citations from the text.\") return v Citation = Annotated\\[str, AfterValidator(citation\\_exists)\\] class AnswerWithCitation(BaseModel): answer: str citation: Citation try: AnswerWithCitation.model\\_validate( { \"answer\": \"Blueberries are packed with protein\", \"citation\": \"Blueberries contain high levels of protein\", }, context={\"text\\_chunk\": \"Blueberries are very rich in antioxidants\"}, ) except Exception as e: print(e)\n\n```\n1 validation error for AnswerWithCitation\ncitation\n  Value error, Citation `Blueberries contain high levels of protein` not found in text, only use citations from the text. [type=value_error, input_value='Blueberries contain high levels of protein', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n\n```\n\nHere we assume that there is a \"text\\_chunk\" field that contains the text that the model is supposed to use as context. We then use the `field_validator` decorator to define a validator that checks if the citation is included in the text chunk. If it's not, we raise a `ValueError` with a message that will be returned to the user.\n\nIf we want to pass in the context through the ```chat.completions.create`` endpoint, we can use the ``` validation\\_context\\` parameter\n\n```\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=AnswerWithCitation,\n    messages=[\\\n        {\"role\": \"user\", \"content\": f\"Answer the question `{q}` using the text chunk\\n`{text_chunk}`\"},\\\n    ],\n    validation_context={\"text_chunk\": text_chunk},\n)\n\n```\n\nIn practice there are many ways to implement this: we could use a regex to check if the citation is included in the text chunk, or we could use a more sophisticated approach like a semantic similarity check. The important thing is that we have a way to validate that the model is using the provided context accurately.\n\n## Reasking with validators [¶](https://python.useinstructor.com/tutorials/3-1-validation-rag/?q=\\#reasking-with-validators)\n\nFor most of these examples all we've done we've mostly only defined the validation logic. Which can be seperate from generation, however when we are given validation errors, we shouldn't end there! Instead instructor allows us to collect all the validation errors and reask the llm to rewrite their answer.\n\nLets try to use a extreme example to illustrate this point:\n\nIn \\[15\\]:\n\nCopied!\n\n```\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n\nquestion = \"What is the meaning of life?\"\ncontext = (\n    \"The according to the devil the meaning of life is a life of sin and debauchery.\"\n)\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\\\n        },\\\n    ],\n)\n\nprint(resp.model_dump_json(indent=2))\n\n```\n\nclass QuestionAnswer(BaseModel): question: str answer: str question = \"What is the meaning of life?\" context = ( \"The according to the devil the meaning of life is a life of sin and debauchery.\" ) resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\_model=QuestionAnswer, messages=\\[ { \"role\": \"system\", \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\", }, { \"role\": \"user\", \"content\": f\"using the context: \\`{context}\\`\\\\n\\\\nAnswer the following question: \\`{question}\\`\", }, \\], ) print(resp.model\\_dump\\_json(indent=2))\n\n```\n{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"According to the devil, the meaning of life is a life of sin and debauchery.\"\n}\n\n```\n\nIn \\[20\\]:\n\nCopied!\n\n```\nfrom instructor import llm_validator\n\nNotEvilAnswer = Annotated[\\\n    str,\\\n    AfterValidator(\\\n        llm_validator(\"don't say objectionable things\", client=client)\\\n    ),\\\n]\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: NotEvilAnswer\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    max_retries=2,\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\\\n        },\\\n    ],\n)\n\n```\n\nfrom instructor import llm\\_validator NotEvilAnswer = Annotated\\[ str, AfterValidator( llm\\_validator(\"don't say objectionable things\", client=client) ), \\] class QuestionAnswer(BaseModel): question: str answer: NotEvilAnswer resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\_model=QuestionAnswer, max\\_retries=2, messages=\\[ { \"role\": \"system\", \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\", }, { \"role\": \"user\", \"content\": f\"using the context: \\`{context}\\`\\\\n\\\\nAnswer the following question: \\`{question}\\`\", }, \\], )\n\n```\nRetrying, exception: 1 validation error for QuestionAnswer\nanswer\n  Assertion failed, The statement promotes sin and debauchery, which can be considered objectionable. [type=assertion_error, input_value='The meaning of life, acc... of sin and debauchery.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/assertion_error\nTraceback (most recent call last):\n  File \"/Users/jasonliu/dev/instructor/instructor/patch.py\", line 277, in retry_sync\n    return process_response(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/jasonliu/dev/instructor/instructor/patch.py\", line 164, in process_response\n    model = response_model.from_response(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jasonliu/dev/instructor/instructor/function_calls.py\", line 137, in from_response\n    return cls.model_validate_json(\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jasonliu/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py\", line 532, in model_validate_json\n    return cls.__pydantic_validator__.validate_json(json_data, strict=strict, context=context)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for QuestionAnswer\nanswer\n  Assertion failed, The statement promotes sin and debauchery, which can be considered objectionable. [type=assertion_error, input_value='The meaning of life, acc... of sin and debauchery.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/assertion_error\n\n```\n\nIn \\[21\\]:\n\nCopied!\n\n```\nprint(resp.model_dump_json(indent=2))\n\n```\n\nprint(resp.model\\_dump\\_json(indent=2))\n\n```\n{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life is subjective and can vary depending on one's beliefs and perspectives. According to the devil, it is a life of sin and debauchery. However, this viewpoint may not be universally accepted and should be evaluated critically.\"\n}\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/3-1-validation-rag/?q=",
      "ogUrl": "https://python.useinstructor.com/tutorials/3-1-validation-rag/",
      "title": "Applications RAG - 2 - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/3-1-validation-rag/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/3-1-validation-rag.png",
      "ogTitle": "Applications RAG - 2 - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/3-1-validation-rag.png",
      "og:title": "Applications RAG - 2 - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/3-1-validation-rag/?q=",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/3-1-validation-rag.png",
      "twitter:title": "Applications RAG - 2 - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/4-validation/?q=#validators)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/4-validation.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/4-validation.ipynb \"View source of this page\")\n\n# Validators [¶](https://python.useinstructor.com/tutorials/4-validation/?q=\\#validators)\n\nInstead of framing \"self-critique\" or \"self-reflection\" in AI as new concepts, we can view them as validation errors with clear error messages that the system can use to self correct.\n\nPydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic [docs](https://docs.pydantic.dev/latest/) on validators.\n\nNote: For the majority of this notebook we won't be calling openai, just using validators to see how we can control the validation of the objects.\n\nValidators will enable us to control outputs by defining a function like so:\n\n```\ndef validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n\n```\n\nBefore we get started lets go over the general shape of a validator:\n\nIn \\[61\\]:\n\nCopied!\n\n```\nfrom pydantic import BaseModel\nfrom typing import Annotated\nfrom pydantic import AfterValidator\n\ndef name_must_contain_space(v: str) -> str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\nperson = UserDetail(age=29, name=\"Jason\")\n\n```\n\nfrom pydantic import BaseModel from typing import Annotated from pydantic import AfterValidator def name\\_must\\_contain\\_space(v: str) -> str: if \" \" not in v: raise ValueError(\"Name must contain a space.\") return v.lower() class UserDetail(BaseModel): age: int name: Annotated\\[str, AfterValidator(name\\_must\\_contain\\_space)\\] person = UserDetail(age=29, name=\"Jason\")\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 4 line 1\n     <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>     age: int\n     <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>     name: Annotated[str, AfterValidator(name_must_contain_space)]\n---> <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a> person = UserDetail(age=29, name=\"Jason\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for UserDetail\nname\n  Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n```\n\n**Validation Applications**\n\nValidators are essential in tackling the unpredictabile nature of LLMs.\n\nStraightforward examples include:\n\n- Flagging outputs containing blacklisted words.\n- Identifying outputs with tones like racism or violence.\n\nFor more complex tasks:\n\n- Ensuring citations directly come from provided content.\n- Checking that the model's responses align with given context.\n- Validating the syntax of SQL queries before execution.\n\n## Setup and Dependencies [¶](https://python.useinstructor.com/tutorials/4-validation/?q=\\#setup-and-dependencies)\n\nUsing the [instructor](https://github.com/jxnl/instructor) library, we streamline the integration of these validators. `instructor` manages the parsing and validation of outputs and automates retries for compliant responses. This simplifies the process for developers to implement new validation logic, minimizing extra overhead.\n\nTo use instructor in our api calls, we just need to patch the openai client:\n\nIn \\[5\\]:\n\nCopied!\n\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\n```\n\nimport instructor from openai import OpenAI client = instructor.patch(OpenAI())\n\n## Software 2.0: Rule-based validators [¶](https://python.useinstructor.com/tutorials/4-validation/?q=\\#software-20-rule-based-validators)\n\nDeterministic validation, characterized by its rule-based logic, ensures consistent outcomes for the same input. Let's explore how we can apply this concept through some examples.\n\n### Flagging bad keywords [¶](https://python.useinstructor.com/tutorials/4-validation/?q=\\#flagging-bad-keywords)\n\nTo begin with, we aim to prevent engagement in topics involving explicit violence.\n\nWe will define a blacklist of violent words that cannot be mentioned in any messages:\n\nIn \\[63\\]:\n\nCopied!\n\n```\nblacklist = {\n    \"rob\",\n    \"steal\",\n    \"hurt\",\n    \"kill\",\n    \"attack\",\n}\n\n```\n\nblacklist = { \"rob\", \"steal\", \"hurt\", \"kill\", \"attack\", }\n\nTo validate if the message contains a blacklisted word we will use a [field\\_validator](https://jxnl.github.io/instructor/blog/2023/10/23/good-llm-validation-is-just-good-validation/#using-field_validator-decorator) over the 'message' field:\n\nIn \\[64\\]:\n\nCopied!\n\n```\nfrom pydantic import BaseModel, field_validator\nfrom pydantic.fields import Field\n\nclass Response(BaseModel):\n    message: str\n\n    @field_validator('message')\n    def message_cannot_have_blacklisted_words(cls, v: str) -> str:\n        for word in v.split():\n            if word.lower() in blacklist:\n                raise ValueError(f\"`{word}` was found in the message `{v}`\")\n        return v\n\nResponse(message=\"I will hurt him\")\n\n```\n\nfrom pydantic import BaseModel, field\\_validator from pydantic.fields import Field class Response(BaseModel): message: str @field\\_validator('message') def message\\_cannot\\_have\\_blacklisted\\_words(cls, v: str) -> str: for word in v.split(): if word.lower() in blacklist: raise ValueError(f\"\\`{word}\\` was found in the message \\`{v}\\`\") return v Response(message=\"I will hurt him\")\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 17 line 1\n     <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>                 raise ValueError(f\"`{word}` was found in the message `{v}`\")\n     <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>         return v\n---> <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a> Response(message=\"I will hurt him\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for Response\nmessage\n  Value error, `hurt` was found in the message `I will hurt him` [type=value_error, input_value='I will hurt him', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n```\n\n### Flagging using OpenAI Moderation [¶](https://python.useinstructor.com/tutorials/4-validation/?q=\\#flagging-using-openai-moderation)\n\nTo enhance our validation measures, we'll extend the scope to flag any answer that contains hateful content, harassment, or similar issues. OpenAI offers a moderation endpoint that addresses these concerns, and it's freely available when using OpenAI models.\n\nWith the `instructor` library, this is just one function edit away:\n\nIn \\[1\\]:\n\nCopied!\n\n```\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\n```\n\nfrom typing import Annotated from pydantic.functional\\_validators import AfterValidator\n\nIn \\[6\\]:\n\nCopied!\n\n```\nfrom instructor import openai_moderation\n\nclass Response(BaseModel):\n    message: Annotated[str, AfterValidator(openai_moderation(client=client))]\n\n```\n\nfrom instructor import openai\\_moderation class Response(BaseModel): message: Annotated\\[str, AfterValidator(openai\\_moderation(client=client))\\]\n\nNow we have a more comprehensive flagging for violence and we can outsource the moderation of our messages.\n\nIn \\[7\\]:\n\nCopied!\n\n```\nResponse(message=\"I want to make them suffer the consequences\")\n\n```\n\nResponse(message=\"I want to make them suffer the consequences\")\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[7], line 1\n----> 1 Response(message=\"I want to make them suffer the consequences\")\n\nFile ~/.virtualenvs/pampa-labs/lib/python3.10/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for Response\nmessage\n  Value error, `I want to make them suffer the consequences` was flagged for harassment, harassment_threatening, violence, harassment/threatening [type=value_error, input_value='I want to make them suffer the consequences', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n```\n\nAnd as an extra, we get flagging for other topics like religion, race etc.\n\nIn \\[26\\]:\n\nCopied!\n\n```\nResponse(message=\"I will mock their religion\")\n\n```\n\nResponse(message=\"I will mock their religion\")\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[26], line 1\n----> 1 Response(message=\"I will mock their religion\")\n\nFile ~/.virtualenvs/pampa-labs/lib/python3.10/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for Response\nmessage\n  Value error, `I will mock their religion` was flagged for ['harassment'] [type=value_error, input_value='I will mock their religion', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n```\n\n### Filtering very long messages [¶](https://python.useinstructor.com/tutorials/4-validation/?q=\\#filtering-very-long-messages)\n\nIn addition to content-based flags, we can also set criteria based on other aspects of the input text. For instance, to maintain user engagement, we might want to prevent the assistant from returning excessively long texts.\n\nHere, noticed that `Field` has built-in validators for `min_length` and `max_length`. to learn more checkout [Field Contraints](https://docs.pydantic.dev/latest/concepts/fields)\n\nIn \\[68\\]:\n\nCopied!\n\n```\nclass AssistantMessage(BaseModel):\n    message: str = Field(..., max_length=100)\n\n```\n\nclass AssistantMessage(BaseModel): message: str = Field(..., max\\_length=100)\n\nIn \\[69\\]:\n\nCopied!\n\n```\nAssistantMessage(message=\"Certainly! Lorem ipsum is a placeholder text commonly used in the printing and typesetting industry. Here's a sample of Lorem ipsum text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam euismod velit vel tellus tempor, non viverra eros iaculis. Sed vel nisl nec mauris bibendum tincidunt. Vestibulum sed libero euismod, eleifend tellus id, laoreet elit. Donec auctor arcu ac mi feugiat, vel lobortis justo efficitur. Fusce vel odio vitae justo varius dignissim. Integer sollicitudin mi a justo bibendum ultrices. Quisque id nisl a lectus venenatis luctus. Please note that Lorem ipsum text is a nonsensical Latin-like text used as a placeholder for content, and it has no specific meaning. It's often used in design and publishing to demonstrate the visual aspects of a document without focusing on the actual content.\")\n\n```\n\nAssistantMessage(message=\"Certainly! Lorem ipsum is a placeholder text commonly used in the printing and typesetting industry. Here's a sample of Lorem ipsum text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam euismod velit vel tellus tempor, non viverra eros iaculis. Sed vel nisl nec mauris bibendum tincidunt. Vestibulum sed libero euismod, eleifend tellus id, laoreet elit. Donec auctor arcu ac mi feugiat, vel lobortis justo efficitur. Fusce vel odio vitae justo varius dignissim. Integer sollicitudin mi a justo bibendum ultrices. Quisque id nisl a lectus venenatis luctus. Please note that Lorem ipsum text is a nonsensical Latin-like text used as a placeholder for content, and it has no specific meaning. It's often used in design and publishing to demonstrate the visual aspects of a document without focusing on the actual content.\")\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 29 line 1\n----> <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a> AssistantMessage(message=\"Certainly! Lorem ipsum is a placeholder text commonly used in the printing and typesetting industry. Here's a sample of Lorem ipsum text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam euismod velit vel tellus tempor, non viverra eros iaculis. Sed vel nisl nec mauris bibendum tincidunt. Vestibulum sed libero euismod, eleifend tellus id, laoreet elit. Donec auctor arcu ac mi feugiat, vel lobortis justo efficitur. Fusce vel odio vitae justo varius dignissim. Integer sollicitudin mi a justo bibendum ultrices. Quisque id nisl a lectus venenatis luctus. Please note that Lorem ipsum text is a nonsensical Latin-like text used as a placeholder for content, and it has no specific meaning. It's often used in design and publishing to demonstrate the visual aspects of a document without focusing on the actual content.\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for AssistantMessage\nmessage\n  String should have at most 100 characters [type=string_too_long, input_value=\"Certainly! Lorem ipsum i... on the actual content.\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/string_too_long\n```\n\n### Avoiding hallucination with citations [¶](https://python.useinstructor.com/tutorials/4-validation/?q=\\#avoiding-hallucination-with-citations)\n\nWhen incorporating external knowledge bases, it's crucial to ensure that the agent uses the provided context accurately and doesn't fabricate responses. Validators can be effectively used for this purpose. We can illustrate this with an example where we validate that a provided citation is actually included in the referenced text chunk:\n\nIn \\[70\\]:\n\nCopied!\n\n```\nfrom pydantic import ValidationInfo\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: str\n\n    @field_validator('citation')\n    @classmethod\n    def citation_exists(cls, v: str, info: ValidationInfo):\n        context = info.context\n        if context:\n            context = context.get('text_chunk')\n            if v not in context:\n                raise ValueError(f\"Citation `{v}` not found in text\")\n        return v\n\n```\n\nfrom pydantic import ValidationInfo class AnswerWithCitation(BaseModel): answer: str citation: str @field\\_validator('citation') @classmethod def citation\\_exists(cls, v: str, info: ValidationInfo): context = info.context if context: context = context.get('text\\_chunk') if v not in context: raise ValueError(f\"Citation \\`{v}\\` not found in text\") return v\n\nHere we assume that there is a \"text\\_chunk\" field that contains the text that the model is supposed to use as context. We then use the `field_validator` decorator to define a validator that checks if the citation is included in the text chunk. If it's not, we raise a `ValueError` with a message that will be returned to the user.\n\nIn \\[71\\]:\n\nCopied!\n\n```\nAnswerWithCitation.model_validate(\n    {\n        \"answer\": \"Blueberries are packed with protein\",\n        \"citation\": \"Blueberries contain high levels of protein\"\n    },\n    context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"},\n)\n\n```\n\nAnswerWithCitation.model\\_validate( { \"answer\": \"Blueberries are packed with protein\", \"citation\": \"Blueberries contain high levels of protein\" }, context={\"text\\_chunk\": \"Blueberries are very rich in antioxidants\"}, )\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 34 line 1\n----> <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a> AnswerWithCitation.model_validate(\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>     {\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>         \"answer\": \"Blueberries are packed with protein\",\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>         \"citation\": \"Blueberries contain high levels of protein\"\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>     },\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=5'>6</a>     context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"},\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a> )\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:503, in BaseModel.model_validate(cls, obj, strict, from_attributes, context)\n    501 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    502 __tracebackhide__ = True\n--> 503 return cls.__pydantic_validator__.validate_python(\n    504     obj, strict=strict, from_attributes=from_attributes, context=context\n    505 )\n\nValidationError: 1 validation error for AnswerWithCitation\ncitation\n  Value error, Citation `Blueberries contain high levels of protein` not found in text [type=value_error, input_value='Blueberries contain high levels of protein', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n```\n\n## Software 3.0: Probabilistic validators [¶](https://python.useinstructor.com/tutorials/4-validation/?q=\\#software-30-probabilistic-validators)\n\nFor scenarios requiring more nuanced validation than rule-based methods, we use probabilistic validation. This approach incorporates LLMs into the validation workflow for a sophisticated assessment of outputs.\n\nThe `instructor` library offers the `llm_validator` utility for this purpose. By specifying the desired directive, we can use LLMs for complex validation tasks. Let's explore some intriguing use cases enabled by LLMs.\n\n### Keeping an agent on topic [¶](https://python.useinstructor.com/tutorials/4-validation/?q=\\#keeping-an-agent-on-topic)\n\nWhen creating an agent focused on health improvement, providing answers and daily practice suggestions, it's crucial to ensure strict adherence to health-related topics. This is important because the knowledge base is limited to health topics, and veering off-topic could result in fabricated responses.\n\nTo achieve this focus, we'll follow a similar process as before, but with an important addition: integrating an LLM into our validator.\n\nThis LLM will be tasked with determining whether the agent's responses are exclusively related to health topics. For this, we will use the `llm_validator` from `instructor` like so:\n\nIn \\[73\\]:\n\nCopied!\n\n```\nfrom instructor import llm_validator\n\nclass AssistantMessage(BaseModel):\n    message: Annotated[str,\\\n                       AfterValidator(\\\n                           llm_validator(\"don't talk about any other topic except health best practices and topics\",\\\n                                         client=client))]\n\nAssistantMessage(message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\")\n\n```\n\nfrom instructor import llm\\_validator class AssistantMessage(BaseModel): message: Annotated\\[str, AfterValidator( llm\\_validator(\"don't talk about any other topic except health best practices and topics\", client=client))\\] AssistantMessage(message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\")\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 38 line 1\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a> class AssistantMessage(BaseModel):\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>     message: Annotated[str,\\\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>                        AfterValidator(\\\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>                            llm_validator(\"don't talk about any other topic except health best practices and topics\",\\\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>                                          openai_client=client))]\n---> <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a> AssistantMessage(message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for AssistantMessage\nmessage\n  Assertion failed, The statement is not related to health best practices or topics. [type=assertion_error, input_value='I would suggest you to v...is very nice in winter.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/assertion_error\n```\n\nImportant that for these examples we're not waiting for the messages, to get this message we would need to call the openai with `response_model=AssistantMessage`.\n\n### Validating agent thinking with CoT [¶](https://python.useinstructor.com/tutorials/4-validation/?q=\\#validating-agent-thinking-with-cot)\n\nUsing probabilistic validation, we can also assess the agent's reasoning process to ensure it's logical before providing a response. With [chain of thought](https://learnprompting.org/docs/intermediate/chain_of_thought) prompting, the model is expected to think in steps and arrive at an answer following its logical progression. If there are errors in this logic, the final response may be incorrect.\n\nHere we will use Pydantic's [model\\_validator](https://docs.pydantic.dev/latest/concepts/validators/#model-validators) which allows us to apply validation over all the properties of the `AIResponse` at once.\n\nTo make this easier we'll make a simple validation class that we can reuse for all our validation:\n\nIn \\[74\\]:\n\nCopied!\n\n```\nfrom typing import Optional\n\nclass Validation(BaseModel):\n    is_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\")\n    error_message: Optional[str] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\")\n\n```\n\nfrom typing import Optional class Validation(BaseModel): is\\_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\") error\\_message: Optional\\[str\\] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\")\n\nThe function we will call will integrate an LLM and will ask it to determine whether the answer the model provided follows from the chain of thought:\n\nIn \\[75\\]:\n\nCopied!\n\n```\ndef validate_chain_of_thought(values):\n    chain_of_thought = values[\"chain_of_thought\"]\n    answer = values[\"answer\"]\n    resp = client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"You are a validator. Determine if the value follows from the statement. If it is not, explain why.\",\\\n            },\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"Verify that `{answer}` follows the chain of thought: {chain_of_thought}\",\\\n            },\\\n        ],\n        response_model=Validation,\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return values\n\n```\n\ndef validate\\_chain\\_of\\_thought(values): chain\\_of\\_thought = values\\[\"chain\\_of\\_thought\"\\] answer = values\\[\"answer\"\\] resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[ { \"role\": \"system\", \"content\": \"You are a validator. Determine if the value follows from the statement. If it is not, explain why.\", }, { \"role\": \"user\", \"content\": f\"Verify that \\`{answer}\\` follows the chain of thought: {chain\\_of\\_thought}\", }, \\], response\\_model=Validation, ) if not resp.is\\_valid: raise ValueError(resp.error\\_message) return values\n\nThe use of the 'before' argument in this context is significant. It means that the validator will receive the complete dictionary of inputs in their raw form, before any parsing by Pydantic.\n\nIn \\[76\\]:\n\nCopied!\n\n```\nfrom typing import Any\nfrom pydantic import model_validator\n\nclass AIResponse(BaseModel):\n    chain_of_thought: str\n    answer: str\n\n    @model_validator(mode='before')\n    @classmethod\n    def chain_of_thought_makes_sense(cls, data: Any) -> Any:\n        # here we assume data is the dict representation of the model\n        # since we use 'before' mode.\n        return validate_chain_of_thought(data)\n\n```\n\nfrom typing import Any from pydantic import model\\_validator class AIResponse(BaseModel): chain\\_of\\_thought: str answer: str @model\\_validator(mode='before') @classmethod def chain\\_of\\_thought\\_makes\\_sense(cls, data: Any) -> Any: # here we assume data is the dict representation of the model # since we use 'before' mode. return validate\\_chain\\_of\\_thought(data)\n\nIn \\[77\\]:\n\nCopied!\n\n```\nAIResponse(chain_of_thought=\"The user suffers from diabetes.\", answer=\"The user has a broken leg.\")\n\n```\n\nAIResponse(chain\\_of\\_thought=\"The user suffers from diabetes.\", answer=\"The user has a broken leg.\")\n\n```\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 47 line 1\n----> <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a> AIResponse(chain_of_thought=\"The user suffers from diabetes.\", answer=\"The user has a broken leg.\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for AIResponse\n  Value error, The statement about the user having a broken leg does not logically follow from the information provided about the user suffering from diabetes. These are two separate health conditions and one does not imply the other. [type=value_error, input_value={'chain_of_thought': 'The...user has a broken leg.'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n```\n\n## Reasking with validators [¶](https://python.useinstructor.com/tutorials/4-validation/?q=\\#reasking-with-validators)\n\nFor most of these examples all we've done we've mostly only defined the validation logic.\n\nWe'eve covered field validators and model validators and even used LLMs to validate our outputs. But we haven't actually used the validators to reask the model! One of the most powerful features of `instructor` is that it will automatically reask the model when it receives a validation error. This means that we can use the same validation logic for both code-based and LLM-based validation.\n\nThis also means that our 'prompt' is not only the prompt we send, but the code that runs the validator, and the error message we send back to the model.\n\nIntegrating these validation examples with the OpenAI API is streamlined using `instructor`. After patching the OpenAI client with `instructor`, you simply need to specify a `response_model` for your requests. This setup ensures that all the validation processes occur automatically.\n\nTo enable reasking you can set a maximum number of retries. When calling the OpenAI client, the system can re-attempt to generate a correct answer. It does this by resending the original query along with feedback on why the previous response was rejected, guiding the LLM towards a more accurate answer in subsequent attempts.\n\nIn \\[79\\]:\n\nCopied!\n\n```\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n\nquestion = \"What is the meaning of life?\"\ncontext = \"The according to the devil the meaning of life is a life of sin and debauchery.\"\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=QuestionAnswer,\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\\\n        },\\\n    ],\n)\n\nresp.answer\n\n```\n\nclass QuestionAnswer(BaseModel): question: str answer: str question = \"What is the meaning of life?\" context = \"The according to the devil the meaning of life is a life of sin and debauchery.\" resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", response\\_model=QuestionAnswer, messages=\\[ { \"role\": \"system\", \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\", }, { \"role\": \"user\", \"content\": f\"using the context: \\`{context}\\`\\\\n\\\\nAnswer the following question: \\`{question}\\`\", }, \\], ) resp.answer\n\nOut\\[79\\]:\n\n```\n'a life of sin and debauchery'\n```\n\nIn \\[80\\]:\n\nCopied!\n\n```\nfrom pydantic import BeforeValidator\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\\\n        str,\\\n        BeforeValidator(\\\n            llm_validator(\"don't say objectionable things\", client=client)\\\n        ),\\\n    ]\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    max_retries=2,\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\\\n        },\\\n    ],\n)\n\nresp.answer\n\n```\n\nfrom pydantic import BeforeValidator class QuestionAnswer(BaseModel): question: str answer: Annotated\\[ str, BeforeValidator( llm\\_validator(\"don't say objectionable things\", client=client) ), \\] resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\_model=QuestionAnswer, max\\_retries=2, messages=\\[ { \"role\": \"system\", \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\", }, { \"role\": \"user\", \"content\": f\"using the context: \\`{context}\\`\\\\n\\\\nAnswer the following question: \\`{question}\\`\", }, \\], ) resp.answer\n\nOut\\[80\\]:\n\n```\n'The meaning of life is a concept that varies depending on individual perspectives and beliefs.'\n```\n\n# Conclusion [¶](https://python.useinstructor.com/tutorials/4-validation/?q=\\#conclusion)\n\nThis guide explains how to use deterministic and probabilistic validation techniques with Large Language Models (LLMs). We discussed using an instructor to establish validation processes for content filtering, context relevance maintenance, and model reasoning verification. These methods enhance the performance of LLMs across different tasks.\n\nFor those interested in further exploration, here's a to-do list:\n\n1. **SQL Syntax Checker**: Create a validator to check the syntax of SQL queries before executing them.\n2. **Context-Based Response Validation**: Design a method to flag responses based on the model's own knowledge rather than the provided context.\n3. **PII Detection**: Implement a mechanism to identify and handle Personally Identifiable Information in responses while prioritizing user privacy.\n4. **Targeted Rule-Based Filtering**: Develop filters to remove specific content types, such as responses mentioning named entities.\n\nCompleting these tasks will enable users to acquire practical skills in improving LLMs through advanced validation methods.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/4-validation/?q=",
      "ogUrl": "https://python.useinstructor.com/tutorials/4-validation/",
      "title": "Validation - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/4-validation/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/4-validation.png",
      "ogTitle": "Validation - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/4-validation.png",
      "og:title": "Validation - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/4-validation/?q=",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/4-validation.png",
      "twitter:title": "Validation - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/6-chain-of-density/?q=#chain-of-density-summarization)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/6-chain-of-density.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/6-chain-of-density.ipynb \"View source of this page\")\n\n# Chain Of Density Summarization [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/?q=\\#chain-of-density-summarization)\n\n## Introduction [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/?q=\\#introduction)\n\n**What is Chain Of Density summarization?**\n\nSummarizing extensive texts with AI can be challenging. Initially, an AI produces a summary, then refines it through multiple iterations, adding missing article entities. Each iteration adds new article entities to the summary, keeping length consistent, leading to an entity-dense, informative summary called Chain Of Density.\n\nIt was first introduced in the paper - From Sparse to Dense : GPT-4 Summarization with Chain of Density prompting.\n\nThis was done in the original paper by asking GPT-4 to generate all of the rewritten summaries in a single go with the following prompt below.\n\n> Article: {{ARTICLE}}\n>\n> You will generate increasingly concise, entity-dense summaries of the above Article.\n>\n> Repeat the following 2 steps 5 times.\n>\n> Step 1. Identify 1-3 informative Entities (\";\" delimited) from the Article which are missing from the previously generated summary. Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.\n>\n> A Missing Entity is:\n>\n> - Relevant: to the main story.\n> - Specific: descriptive yet concise (5 words or fewer).\n> - Novel; not in the previous summary.\n> - Faithful: present in the Article.\n> - Anywhere: located anywhere in the Article.\n>\n> Guidelines: phrases like \"the article discusses\"\n>\n> - The first summary should be long (4-5 sentences, -80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach -80 words.\n> - Make every word count: re-write the previous summary to improve flow and make space for additional entities.\n> - Make space with fusion, compression, and removal of uninformative\n>\n> - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n> - Missing entities can appear anywhere in the new summary.\n> - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n>\n> Remember, use the exact same number of words for each summary.\n>\n> Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing\\_Entities\" and \"Denser\\_Summary\"\n\nWhile the original paper used a single prompt to generate the iterative generations, we can go one step better with `Instructor` and break down the process into smaller API calls - with validation along the way.\n\nThe process can be broken down as seen below.\n\n![image.png](<Base64-Image-Removed>)\n\n### Setup and Dependencies [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/?q=\\#setup-and-dependencies)\n\nWe'll be using two new libraries for our demonstration\n\n1. `spaCy` : This provides a handful of useful utilities to do generic NLP tasks with\n2. `nltk` : This was used by the original paper to count the number of tokens in our generated summaries\n\nWe'll need to install the tokenizer packages and the spacy english library before we can proceed with the rest of the lesson\n\nIn \\[1\\]:\n\nCopied!\n\n```\nimport nltk\nnltk.download('punkt')\n\n!python -m spacy download en_core_web_sm --quiet\n\n```\n\nimport nltk nltk.download('punkt') !python -m spacy download en\\_core\\_web\\_sm --quiet\n\n```\n[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n```\n\n```\n✔ Download and installation successful\nYou can now load the package via spacy.load('en_core_web_sm')\n\n```\n\nOnce that's done, let's now move on to writing some code.\n\n## Definitions [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/?q=\\#definitions)\n\nThere are a few different definitions which we'll need to understand in the tutorial. They are\n\n1. Tokens and tokenizers\n2. Entities\n3. Entity-Dense\n\nOnce we've gotten a hang of these concepts, we'll walk through a simple implementation of a Chain Of Density summarizer\n\n### Tokens and Tokenizers [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/?q=\\#tokens-and-tokenizers)\n\nIn the original paper, the authors used `NLTK` to split the generated summary into tokens. These represent the smallest units that each sentence could be broken into where each hold semantic meaning.\n\nLet's walk through a simple example to see how the `NLTK` tokenizer might work\n\nIn \\[2\\]:\n\nCopied!\n\n```\nimport nltk\nsentence = \"My favourite type of Sashimi is Toro\"\n\nnltk.word_tokenize(sentence)\n\n```\n\nimport nltk sentence = \"My favourite type of Sashimi is Toro\" nltk.word\\_tokenize(sentence)\n\nOut\\[2\\]:\n\n```\n['My', 'favourite', 'type', 'of', 'Sashimi', 'is', 'Toro']\n```\n\nNLTK's word tokenizer does more than just split by empty whitespace. It handles a lot of nice edge cases and contractions such as `don't` or `I'm`.\n\nIn \\[3\\]:\n\nCopied!\n\n```\nsentence = \"I'm fascinated by machine learning!\"\n\nnltk.word_tokenize(sentence)\n\n```\n\nsentence = \"I'm fascinated by machine learning!\" nltk.word\\_tokenize(sentence)\n\nOut\\[3\\]:\n\n```\n['I', \"'m\", 'fascinated', 'by', 'machine', 'learning', '!']\n```\n\nWe can then calculate the number of tokens by simply finding the `len` of the generated sequence.\n\nIn \\[4\\]:\n\nCopied!\n\n```\nsentence = \"I'm fascinated by machine learning!\"\ntokens = nltk.word_tokenize(sentence)\nprint(tokens)\nprint(len(tokens))\n\n```\n\nsentence = \"I'm fascinated by machine learning!\" tokens = nltk.word\\_tokenize(sentence) print(tokens) print(len(tokens))\n\n```\n['I', \"'m\", 'fascinated', 'by', 'machine', 'learning', '!']\n7\n\n```\n\n### Entities [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/?q=\\#entities)\n\nA named entity is an object in the real-world that we identify using a name. Common examples include people, countries, products or even books that we know and love. We can use the `spaCy` library for us to be able to detect the number of entities in a given sentence.\n\nIn \\[5\\]:\n\nCopied!\n\n```\n# First we load in the library\nimport spacy\n\n# Then we initialise an NLP object.\nnlp = spacy.load(\"en_core_web_sm\")\n\n```\n\n\\# First we load in the library import spacy # Then we initialise an NLP object. nlp = spacy.load(\"en\\_core\\_web\\_sm\")\n\nIn \\[6\\]:\n\nCopied!\n\n```\nsentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n\ndoc = nlp(sentence)\ndoc.ents\n\n```\n\nsentence = \"Apple is looking at buying U.K. startup for $1 billion\" doc = nlp(sentence) doc.ents\n\nOut\\[6\\]:\n\n```\n(Apple, U.K., $1 billion)\n```\n\nWe can see that Spacy was able to identify unique and named entities that were present within the sentence using the `doc.ents` property. Let's see a few more examples.\n\nIn \\[7\\]:\n\nCopied!\n\n```\nsentence = \"A knowledge graph, also known as a semantic network\\\n, represents real-world entities and their relationships\"\n\ndoc = nlp(sentence)\ndoc.ents\n\n```\n\nsentence = \"A knowledge graph, also known as a semantic network\\ , represents real-world entities and their relationships\" doc = nlp(sentence) doc.ents\n\nOut\\[7\\]:\n\n```\n()\n```\n\nIn \\[8\\]:\n\nCopied!\n\n```\nsentence = \"For example, a node representing an author like 'J.K. Rowling'\\\ncan be connected to another node representing one of her books, 'Harry Potter'\\\n, with the edge 'author of'\"\n\ndoc = nlp(sentence)\ndoc.ents\n\n```\n\nsentence = \"For example, a node representing an author like 'J.K. Rowling'\\ can be connected to another node representing one of her books, 'Harry Potter'\\ , with the edge 'author of'\" doc = nlp(sentence) doc.ents\n\nOut\\[8\\]:\n\n```\n(J.K., one, Harry Potter')\n```\n\nAs we can see from the examples above, entities are not nouns. They're direct or indirect references to people, places, concepts.\n\n### Entity Density [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/?q=\\#entity-density)\n\nNow that we know what tokens and tokens are, we can move on to our last concept - that of entity density. Entity density is simply the mean number of entities present per token within your string of text.\n\nIn \\[9\\]:\n\nCopied!\n\n```\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef calculate_entity_density(sentence:str):\n    tokens = nltk.word_tokenize(sentence)\n    entities = nlp(sentence).ents\n    entity_density = round(len(entities)/len(tokens),3)\n\n    return len(tokens),len(entities),entity_density\n\n```\n\nnlp = spacy.load(\"en\\_core\\_web\\_sm\") def calculate\\_entity\\_density(sentence:str): tokens = nltk.word\\_tokenize(sentence) entities = nlp(sentence).ents entity\\_density = round(len(entities)/len(tokens),3) return len(tokens),len(entities),entity\\_density\n\nIn \\[10\\]:\n\nCopied!\n\n```\nsentence_1 = \"A knowledge graph, also known as a semantic network\\\n, represents real-world entities and their relationships\"\n\ncalculate_entity_density(sentence_1)\n\n```\n\nsentence\\_1 = \"A knowledge graph, also known as a semantic network\\ , represents real-world entities and their relationships\" calculate\\_entity\\_density(sentence\\_1)\n\nOut\\[10\\]:\n\n```\n(17, 0, 0.0)\n```\n\nIn \\[11\\]:\n\nCopied!\n\n```\nsentence_2 = \"Apple is looking at buying U.K. startup for $1 billion\"\n\ncalculate_entity_density(sentence_2)\n\n```\n\nsentence\\_2 = \"Apple is looking at buying U.K. startup for $1 billion\" calculate\\_entity\\_density(sentence\\_2)\n\nOut\\[11\\]:\n\n```\n(11, 3, 0.273)\n```\n\nThis gives us a quantitative method to be able to understand and compare two different sentences/summaries.\n\nWe want summaries that are more entity-dense\n\nIn \\[12\\]:\n\nCopied!\n\n```\nsummary_1 = \"\"\"\nThis article discusses an incident that occurred during the Chinese Grand Prix\ninvolving two racing drivers, Jenson Button and Pastor Maldonado. The two were\ncompeting for the 13th place when Button collided with Maldonado's vehicle,\ncausing damage to both cars. The incident resulted in a penalty for Button,\nwho was demoted to 14th place. Maldonado, on the other hand, had to retire from\nthe race due to the damage his car sustained.\n\"\"\"\n\nsummary_2 = \"\"\"\nJenson Button's McLaren collided with Pastor Maldonado's Lotus during the Chinese\nGrand Prix, causing front wing damage to Button's car and rear-end damage to\nMaldonado's, forcing his retirement. Button received a five-second penalty and\ntwo superlicence points, dropping himto 14th. Fernando Alonso advanced two places,\nwhile Button was lapped by Nico Rosberg and Alonso by Sebastian Vettel and\nKimi Raikkonen.\n\"\"\"\n\ncalculate_entity_density(summary_1),calculate_entity_density(summary_2)\n\n```\n\nsummary\\_1 = \"\"\" This article discusses an incident that occurred during the Chinese Grand Prix involving two racing drivers, Jenson Button and Pastor Maldonado. The two were competing for the 13th place when Button collided with Maldonado's vehicle, causing damage to both cars. The incident resulted in a penalty for Button, who was demoted to 14th place. Maldonado, on the other hand, had to retire from the race due to the damage his car sustained. \"\"\" summary\\_2 = \"\"\" Jenson Button's McLaren collided with Pastor Maldonado's Lotus during the Chinese Grand Prix, causing front wing damage to Button's car and rear-end damage to Maldonado's, forcing his retirement. Button received a five-second penalty and two superlicence points, dropping himto 14th. Fernando Alonso advanced two places, while Button was lapped by Nico Rosberg and Alonso by Sebastian Vettel and Kimi Raikkonen. \"\"\" calculate\\_entity\\_density(summary\\_1),calculate\\_entity\\_density(summary\\_2)\n\nOut\\[12\\]:\n\n```\n((82, 11, 0.134), (71, 17, 0.239))\n```\n\nWe can see that the final summary is almost twice as dense as the first summary and is hence more _entity dense_.\n\n## Implementation [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/?q=\\#implementation)\n\n### Data Classes [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/?q=\\#data-classes)\n\nLet's start by walking through some of the data models that we'll be using as the response\\_model for our open ai function calls. We'll need a total of two different classes\n\n1. Initial Summary: which is the lengthy and overly verbose article\n2. Rewritten Summary : which represents\n\nIn \\[13\\]:\n\nCopied!\n\n```\nfrom pydantic import BaseModel,Field,field_validator\n\n```\n\nfrom pydantic import BaseModel,Field,field\\_validator\n\nIn \\[14\\]:\n\nCopied!\n\n```\nclass InitialSummary(BaseModel):\n    \"\"\"\n    This is an initial summary which should be long ( 4-5 sentences, ~80 words)\n    yet highly non-specific, containing little information beyond the entities marked as missing.\n    Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a summary of the article provided which is overly verbose and uses fillers. \\\n        It should be roughly 80 words in length\",\n    )\n\n```\n\nclass InitialSummary(BaseModel): \"\"\" This is an initial summary which should be long ( 4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words. \"\"\" summary: str = Field( ..., description=\"This is a summary of the article provided which is overly verbose and uses fillers. \\ It should be roughly 80 words in length\", )\n\nPydantic is extremely handy because it allows us to do two things\n\n1. We can validate that our generated outputs are consistent with what we want, **and write vanilla python to validate so**\n2. We can export the generated class definition into a simple schema that fits in perfectly with OpenAI's function calling\n\nIn \\[15\\]:\n\nCopied!\n\n```\nInitialSummary.model_json_schema()\n\n```\n\nInitialSummary.model\\_json\\_schema()\n\nOut\\[15\\]:\n\n```\n{'description': 'This is an initial summary which should be long ( 4-5 sentences, ~80 words)\\nyet highly non-specific, containing little information beyond the entities marked as missing.\\nUse overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.',\n 'properties': {'summary': {'description': 'This is a summary of the article provided which is overly verbose and uses fillers.         It should be roughly 80 words in length',\n   'title': 'Summary',\n   'type': 'string'}},\n 'required': ['summary'],\n 'title': 'InitialSummary',\n 'type': 'object'}\n```\n\nIt's important here to provide a good description of the overall class and the respective fields. This is because all of the descriptions that we write for the individual fields and the class itself **are directly used by the llm when generating outputs**.\n\nNow, as a quick recap, when we rewrite our summaries at each step, we're performing a few things\n\n1. We identify any entities from the original article that are relevant which are **missing from our current summary**\n2. We then rewrite our summary, making sure to include as many of these new entities as possible with the goal of increasing the entity density of the new summary\n3. We then make sure that we have included all of the entities in our previous summary in the new rewritten summary.\n\nWe can express this in the form of the data model seen below called `RewrittenSummary`.\n\nIn \\[16\\]:\n\nCopied!\n\n```\nclass RewrittenSummary(BaseModel):\n    \"\"\"\n    This is a new, denser summary of identical length which covers every entity\n    and detail from the previous summary plus the Missing Entities.\n\n    Guidelines\n    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    - Missing entities can appear anywhere in the new summary\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n    )\n    absent: list[str] = Field(\n        ...,\n        default_factory=list,\n        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n    )\n    missing: list[str] = Field(\n        default_factory=list,\n        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n    )\n\n```\n\nclass RewrittenSummary(BaseModel): \"\"\" This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. Guidelines - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" - Missing entities can appear anywhere in the new summary An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. \"\"\" summary: str = Field( ..., description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\", ) absent: list\\[str\\] = Field( ..., default\\_factory=list, description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\", ) missing: list\\[str\\] = Field( default\\_factory=list, description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\", )\n\nWe'd also want our rewritten summary to have\n\n1. No missing entities => `absent` should have a length of 0\n2. New entities to be added in the next rewrite -> `missing` should have at least 1 entry\n3. A minimum length of 60 tokens and to have a density of at least 0.08 ( **NOTE**: 60 tokens and the 0.08 cut off are chosen arbitrarily, feel free to adjust them even higher if you wish. However, this might require you to add more retries in your code )\n\nWe can do so using the `field_validator` that we learnt in the previous lesson. This allows us to add in a validator for a specific field to ensure it meets our requirements.\n\nThis gives us the final definition of our `RewrittenSummary` class as seen below\n\nIn \\[17\\]:\n\nCopied!\n\n```\nclass RewrittenSummary(BaseModel):\n    \"\"\"\n    This is a new, denser summary of identical length which covers every entity\n    and detail from the previous summary plus the Missing Entities.\n\n    Guidelines\n    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    - Missing entities can appear anywhere in the new summary\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n    )\n    absent: list[str] = Field(\n        ...,\n        default_factory=list,\n        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n    )\n    missing: list[str] = Field(\n        default_factory=list,\n        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n    )\n\n\n    @field_validator(\"summary\")\n    def min_length(cls, v: str):\n        tokens = nltk.word_tokenize(v)\n        num_tokens = len(tokens)\n        if num_tokens < 60:\n            raise ValueError(\n                \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\"\n            )\n        return v\n\n    @field_validator(\"missing\")\n    def has_missing_entities(cls, missing_entities: list[str]):\n        if len(missing_entities) == 0:\n            raise ValueError(\n                \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\"\n            )\n        return missing_entities\n\n    @field_validator(\"absent\")\n    def has_no_absent_entities(cls, absent_entities: list[str]):\n        absent_entity_string = \",\".join(absent_entities)\n        if len(absent_entities) > 0:\n            print(f\"Detected absent entities of {absent_entity_string}\")\n            raise ValueError(\n                f\"Do not omit the following Entities {absent_entity_string} from the new summary\"\n            )\n        return absent_entities\n\n    @field_validator(\"summary\")\n    def min_entity_density(cls, v: str):\n        tokens = nltk.word_tokenize(v)\n        num_tokens = len(tokens)\n\n        # Extract Entities\n        doc = nlp(v)\n        num_entities = len(doc.ents)\n\n        density = num_entities / num_tokens\n        if density < 0.08:\n            raise ValueError(\n                f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\"\n            )\n\n        return v\n\n```\n\nclass RewrittenSummary(BaseModel): \"\"\" This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. Guidelines - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" - Missing entities can appear anywhere in the new summary An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. \"\"\" summary: str = Field( ..., description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\", ) absent: list\\[str\\] = Field( ..., default\\_factory=list, description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\", ) missing: list\\[str\\] = Field( default\\_factory=list, description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\", ) @field\\_validator(\"summary\") def min\\_length(cls, v: str): tokens = nltk.word\\_tokenize(v) num\\_tokens = len(tokens) if num\\_tokens < 60: raise ValueError( \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\" ) return v @field\\_validator(\"missing\") def has\\_missing\\_entities(cls, missing\\_entities: list\\[str\\]): if len(missing\\_entities) == 0: raise ValueError( \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\" ) return missing\\_entities @field\\_validator(\"absent\") def has\\_no\\_absent\\_entities(cls, absent\\_entities: list\\[str\\]): absent\\_entity\\_string = \",\".join(absent\\_entities) if len(absent\\_entities) > 0: print(f\"Detected absent entities of {absent\\_entity\\_string}\") raise ValueError( f\"Do not omit the following Entities {absent\\_entity\\_string} from the new summary\" ) return absent\\_entities @field\\_validator(\"summary\") def min\\_entity\\_density(cls, v: str): tokens = nltk.word\\_tokenize(v) num\\_tokens = len(tokens) # Extract Entities doc = nlp(v) num\\_entities = len(doc.ents) density = num\\_entities / num\\_tokens if density < 0.08: raise ValueError( f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\" ) return v\n\n### Putting it all together [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/?q=\\#putting-it-all-together)\n\nNow that we have our models, let's implement a function to summarize a piece of text using a Chain Of Density summarization\n\nIn \\[18\\]:\n\nCopied!\n\n```\nfrom openai import OpenAI\nimport instructor\n\nclient = instructor.patch(OpenAI())\n\ndef summarize_article(article: str, summary_steps: int = 3):\n    summary_chain = []\n    # We first generate an initial summary\n    summary: InitialSummary = client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        response_model=InitialSummary,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\",\\\n            },\\\n            {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": \"The generated summary should be about 80 words.\",\\\n            },\\\n        ],\n        max_retries=2,\n    )\n    prev_summary = None\n    summary_chain.append(summary.summary)\n    for _i in range(summary_steps):\n        missing_entity_message = (\n            []\n            if prev_summary is None\n            else [\\\n                {\\\n                    \"role\": \"user\",\\\n                    \"content\": f\"Please include these Missing Entities: {','.join(prev_summary.missing)}\",\\\n                },\\\n            ]\n        )\n        new_summary: RewrittenSummary = client.chat.completions.create(\n            model=\"gpt-4-1106-preview\",\n            messages=[\\\n                {\\\n                    \"role\": \"system\",\\\n                    \"content\": \"\"\"\\\n                You are going to generate an increasingly concise,entity-dense summary of the following article.\\\n\\\n                Perform the following two tasks\\\n                - Identify 1-3 informative entities from the following article which is missing from the previous summary\\\n                - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities\\\n\\\n                Guidelines\\\n                - Make every word count: re-write the previous summary to improve flow and make space for additional entities\\\n                - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\\\n                - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\\\n                - Missing entities can appear anywhere in the new summary\\\n                - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\\\n                \"\"\",\\\n                },\\\n                {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\\\n                {\\\n                    \"role\": \"user\",\\\n                    \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",\\\n                },\\\n                *missing_entity_message,\\\n            ],\n            max_retries=3,\n            max_tokens=1000,\n            response_model=RewrittenSummary,\n        )\n        summary_chain.append(new_summary.summary)\n        prev_summary = new_summary\n\n    return summary_chain\n\n```\n\nfrom openai import OpenAI import instructor client = instructor.patch(OpenAI()) def summarize\\_article(article: str, summary\\_steps: int = 3): summary\\_chain = \\[\\] # We first generate an initial summary summary: InitialSummary = client.chat.completions.create( model=\"gpt-4-1106-preview\", response\\_model=InitialSummary, messages=\\[ { \"role\": \"system\", \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": \"The generated summary should be about 80 words.\", }, \\], max\\_retries=2, ) prev\\_summary = None summary\\_chain.append(summary.summary) for \\_i in range(summary\\_steps): missing\\_entity\\_message = ( \\[\\] if prev\\_summary is None else \\[ { \"role\": \"user\", \"content\": f\"Please include these Missing Entities: {','.join(prev\\_summary.missing)}\", }, \\] ) new\\_summary: RewrittenSummary = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[ { \"role\": \"system\", \"content\": \"\"\" You are going to generate an increasingly concise,entity-dense summary of the following article. Perform the following two tasks - Identify 1-3 informative entities from the following article which is missing from the previous summary - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities Guidelines - Make every word count: re-write the previous summary to improve flow and make space for additional entities - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\". - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article. - Missing entities can appear anywhere in the new summary - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \"\"\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": f\"Here is the previous summary: {summary\\_chain\\[-1\\]}\", }, \\*missing\\_entity\\_message, \\], max\\_retries=3, max\\_tokens=1000, response\\_model=RewrittenSummary, ) summary\\_chain.append(new\\_summary.summary) prev\\_summary = new\\_summary return summary\\_chain\n\n### Trial Run [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/?q=\\#trial-run)\n\nLet's try running this on some sample text which we can import in from our repository. We've provided a sample article in a file called `article.txt`\n\nIn \\[19\\]:\n\nCopied!\n\n```\nwith open(\"./assets/article.txt\",\"r+\") as file:\n    article = file.readline()\n\n```\n\nwith open(\"./assets/article.txt\",\"r+\") as file: article = file.readline()\n\nIn \\[ \\]:\n\nCopied!\n\n```\n%%time\n\nsummaries = summarize_article(article)\n\n```\n\n%%time summaries = summarize\\_article(article)\n\nWe can see that it took roughly 40 seconds to do an iterative chain of density using this article. But does our approach increase the density of each individual summary? We can check by calculating the entity density of each summary in our list of summaries using the `calculate_entity_density` function we defined above.\n\nIn \\[ \\]:\n\nCopied!\n\n```\nfor index,summary in enumerate(summaries):\n    tokens,entity,density = calculate_entity_density(summary)\n    print(f\"Article {index+1} -> Results (Tokens: {tokens}, Entity Count: {entity}, Density: {density})\")\n\n```\n\nfor index,summary in enumerate(summaries): tokens,entity,density = calculate\\_entity\\_density(summary) print(f\"Article {index+1} -> Results (Tokens: {tokens}, Entity Count: {entity}, Density: {density})\")\n\nWe can take a look at the articles themselves to see if they qualitatively show improvement\n\nIn \\[ \\]:\n\nCopied!\n\n```\nfor summary in summaries:\n    print(f\"\\n{summary}\\n\")\n\n```\n\nfor summary in summaries: print(f\"\\\\n{summary}\\\\n\")\n\nAs we can see, the articles progressively introduce more entities and become more entity dense. We've performed 4 rounds of summarization here but you could definitely do with maybe 2-3 if latency is a significant issue.\n\n## Future Steps [¶](https://python.useinstructor.com/tutorials/6-chain-of-density/?q=\\#future-steps)\n\nThis guide showed how to to generate complex summaries using chain of density summarization. We spent some time covering how to apply more complex validators - using `spaCy` and `NLTK` to ensure we had a minimum number of tokens and entity density as well as how you might apply instructor in a multi-stage process.\n\nBy building in validation at each step of the proccess, this helps to improve the performance of your LLM across various tasks.\n\nFor those looking to delve deeper, here are some to-do lists to explore.\n\n- **Validate Increasing Entity Density**: `Pydantic` exposes a more complex validator that can take in an arbitrary python dictionary. Use the validation context to check the entity density of the previous summary and the new summary to validate that our model has generated a more entity-dense rewrite\n- **Fine-Tuning** : `Instructor` comes with a simple to use interface to help you fine-tune other OpenAI models for your needs. This can be accomplished by capturing the outputs of LLMs using the `Instructions` module to generate training data for fine-tuning. In this specific case, finetuning a model to generate dense summaries could decrease latency and cost significantly by replacing the iterative LLM calls that we make .\n\nBy accomplishing these tasks, you'll gain practical experience in tuning your models to suit your specific tasks as well as build in more complex validation processes when working with LLMs to ensure more reliable, accurate and consistent outputs.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/6-chain-of-density/?q=",
      "ogUrl": "https://python.useinstructor.com/tutorials/6-chain-of-density/",
      "title": "Chain of Density - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/6-chain-of-density/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/6-chain-of-density.png",
      "ogTitle": "Chain of Density - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/6-chain-of-density.png",
      "og:title": "Chain of Density - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/6-chain-of-density/?q=",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/6-chain-of-density.png",
      "twitter:title": "Chain of Density - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/tutorials/2-tips/?q=#general-tips-on-prompting)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/tutorials/2-tips.ipynb \"Edit this page\")[View source of this page](https://github.com/jxnl/instructor/raw/main/docs/tutorials/2-tips.ipynb \"View source of this page\")\n\n# General Tips on Prompting [¶](https://python.useinstructor.com/tutorials/2-tips/?q=\\#general-tips-on-prompting)\n\nBefore we get into some big applications of schema engineering I want to equip you with the tools for success. This notebook is to share some general advice when using prompts to get the most of your models.\n\nBefore you might think of prompt engineering as massaging this wall of text, almost like coding in a notepad. But with schema engineering you can get a lot more out of your prompts with a lot less work.\n\n## Classification [¶](https://python.useinstructor.com/tutorials/2-tips/?q=\\#classification)\n\nFor classification we've found theres generally two methods of modeling.\n\n1. using Enums\n2. using Literals\n\nUse an enum in Python when you need a set of named constants that are related and you want to ensure type safety, readability, and prevent invalid values. Enums are helpful for grouping and iterating over these constants.\n\nUse literals when you have a small, unchanging set of values that you don't need to group or iterate over, and when type safety and preventing invalid values is less of a concern. Literals are simpler and more direct for basic, one-off values.\n\nIn \\[1\\]:\n\nCopied!\n\n```\nimport instructor\nfrom openai import OpenAI\n\nfrom enum import Enum\nfrom pydantic import BaseModel, Field\nfrom typing_extensions import Literal\n\nclient = instructor.patch(OpenAI())\n\n# Tip: Do not use auto() as they cast to 1,2,3,4\nclass House(Enum):\n    Gryffindor = \"gryffindor\"\n    Hufflepuff = \"hufflepuff\"\n    Ravenclaw = \"ravenclaw\"\n    Slytherin = \"slytherin\"\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: House\n\n    def say_hello(self):\n        print(\n            f\"Hello, I'm {self.name}, I'm {self.age} years old and I'm from {self.house.value.title()}\"\n        )\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n\n```\n\nimport instructor from openai import OpenAI from enum import Enum from pydantic import BaseModel, Field from typing\\_extensions import Literal client = instructor.patch(OpenAI()) # Tip: Do not use auto() as they cast to 1,2,3,4 class House(Enum): Gryffindor = \"gryffindor\" Hufflepuff = \"hufflepuff\" Ravenclaw = \"ravenclaw\" Slytherin = \"slytherin\" class Character(BaseModel): age: int name: str house: House def say\\_hello(self): print( f\"Hello, I'm {self.name}, I'm {self.age} years old and I'm from {self.house.value.title()}\" ) resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[{\"role\": \"user\", \"content\": \"Harry Potter\"}\\], response\\_model=Character, ) resp.model\\_dump()\n\nOut\\[1\\]:\n\n```\n{'age': 17, 'name': 'Harry Potter', 'house': <House.Gryffindor: 'gryffindor'>}\n```\n\nIn \\[2\\]:\n\nCopied!\n\n```\nresp.say_hello()\n\n```\n\nresp.say\\_hello()\n\n```\nHello, I'm Harry Potter, I'm 17 years old and I'm from Gryffindor\n\n```\n\nIn \\[3\\]:\n\nCopied!\n\n```\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n\n```\n\nclass Character(BaseModel): age: int name: str house: Literal\\[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"\\] resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[{\"role\": \"user\", \"content\": \"Harry Potter\"}\\], response\\_model=Character, ) resp.model\\_dump()\n\nOut\\[3\\]:\n\n```\n{'age': 11, 'name': 'Harry Potter', 'house': 'Gryffindor'}\n```\n\n## Arbitrary properties [¶](https://python.useinstructor.com/tutorials/2-tips/?q=\\#arbitrary-properties)\n\nOften times there are long properties that you might want to extract from data that we can not specify in advanced. We can get around this by defining an arbitrary key value store like so:\n\nIn \\[4\\]:\n\nCopied!\n\n```\nclass Property(BaseModel):\n    key: str = Field(description=\"Must be snake case\")\n    value: str\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n    properties: list[Property]\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n\n```\n\nclass Property(BaseModel): key: str = Field(description=\"Must be snake case\") value: str class Character(BaseModel): age: int name: str house: Literal\\[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"\\] properties: list\\[Property\\] resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}\\], response\\_model=Character, ) resp.model\\_dump()\n\nOut\\[4\\]:\n\n```\n{'age': 38,\n 'name': 'Severus Snape',\n 'house': 'Slytherin',\n 'properties': [{'key': 'role', 'value': 'Potions Master'},\\\n  {'key': 'patronus', 'value': 'Doe'},\\\n  {'key': 'loyalty', 'value': 'Dumbledore'},\\\n  {'key': 'played_by', 'value': 'Alan Rickman'}]}\n```\n\n## Limiting the length of lists [¶](https://python.useinstructor.com/tutorials/2-tips/?q=\\#limiting-the-length-of-lists)\n\nIn later chapters we'll talk about how to use validators to assert the length of lists but we can also use prompting tricks to enumerate values. Here we'll define a index to count the properties.\n\nIn this following example instead of extraction we're going to work on generation instead.\n\nIn \\[5\\]:\n\nCopied!\n\n```\nclass Property(BaseModel):\n    index: str = Field(..., description=\"Monotonically increasing ID\")\n    key: str = Field(description=\"Must be snake case\")\n    value: str\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n    properties: list[Property] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be exactly 5\",\n    )\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n\n```\n\nclass Property(BaseModel): index: str = Field(..., description=\"Monotonically increasing ID\") key: str = Field(description=\"Must be snake case\") value: str class Character(BaseModel): age: int name: str house: Literal\\[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"\\] properties: list\\[Property\\] = Field( ..., description=\"Numbered list of arbitrary extracted properties, should be exactly 5\", ) resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}\\], response\\_model=Character, ) resp.model\\_dump()\n\nOut\\[5\\]:\n\n```\n{'age': 38,\n 'name': 'Severus Snape',\n 'house': 'Slytherin',\n 'properties': [{'index': '1',\\\n   'key': 'position_at_hogwarts',\\\n   'value': 'Potions Master'},\\\n  {'index': '2', 'key': 'patronus_form', 'value': 'Doe'},\\\n  {'index': '3', 'key': 'loyalty', 'value': 'Albus Dumbledore'},\\\n  {'index': '4', 'key': 'played_by', 'value': 'Alan Rickman'},\\\n  {'index': '5', 'key': 'final_act', 'value': 'Protecting Harry Potter'}]}\n```\n\n## Defining Multiple Entities [¶](https://python.useinstructor.com/tutorials/2-tips/?q=\\#defining-multiple-entities)\n\nNow that we see a single entity with many properties we can continue to nest them into many users\n\nIn \\[6\\]:\n\nCopied!\n\n```\nfrom collections.abc import Iterable\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n\n```\n\nfrom collections.abc import Iterable class Character(BaseModel): age: int name: str house: Literal\\[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"\\] resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}\\], response\\_model=Iterable\\[Character\\], ) for character in resp: print(character)\n\n```\nage=11 name='Harry Potter' house='Gryffindor'\nage=11 name='Hermione Granger' house='Gryffindor'\nage=11 name='Ron Weasley' house='Gryffindor'\nage=11 name='Draco Malfoy' house='Slytherin'\nage=11 name='Neville Longbottom' house='Gryffindor'\n\n```\n\nIn \\[7\\]:\n\nCopied!\n\n```\nfrom collections.abc import Iterable\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],\n    stream=True,\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n\n```\n\nfrom collections.abc import Iterable class Character(BaseModel): age: int name: str house: Literal\\[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"\\] resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}\\], stream=True, response\\_model=Iterable\\[Character\\], ) for character in resp: print(character)\n\n```\nage=11 name='Harry Potter' house='Gryffindor'\nage=11 name='Hermione Granger' house='Gryffindor'\nage=11 name='Ron Weasley' house='Gryffindor'\nage=17 name='Draco Malfoy' house='Slytherin'\nage=11 name='Luna Lovegood' house='Ravenclaw'\n\n```\n\n## Defining Relationships [¶](https://python.useinstructor.com/tutorials/2-tips/?q=\\#defining-relationships)\n\nNot only can we define lists of users, but with lists of properties we can also easily define lists of references. It's one of the more interesting things I've learned about prompting.\n\nIn \\[8\\]:\n\nCopied!\n\n```\nclass Character(BaseModel):\n    id: int\n    name: str\n    friends_array: list[int] = Field(description=\"Relationships to their friends using the id\")\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"5 kids from Harry Potter\"}],\n    stream=True,\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n\n```\n\nclass Character(BaseModel): id: int name: str friends\\_array: list\\[int\\] = Field(description=\"Relationships to their friends using the id\") resp = client.chat.completions.create( model=\"gpt-4-1106-preview\", messages=\\[{\"role\": \"user\", \"content\": \"5 kids from Harry Potter\"}\\], stream=True, response\\_model=Iterable\\[Character\\], ) for character in resp: print(character)\n\n```\nid=1 name='Harry Potter' friends_array=[2, 3, 4, 5, 6]\nid=2 name='Hermione Granger' friends_array=[1, 3, 4, 5]\nid=3 name='Ron Weasley' friends_array=[1, 2, 4, 6]\nid=4 name='Neville Longbottom' friends_array=[1, 2, 3, 5]\nid=5 name='Luna Lovegood' friends_array=[1, 2, 4, 6]\nid=6 name='Draco Malfoy' friends_array=[1, 3, 5]\n\n```\n\nWith the tools we've discussed, we can find numerous real-world applications in production settings. These include extracting action items from transcripts, generating fake data, filling out forms, and creating objects that correspond to generative UI. These simple tricks will be highly useful.\n\n# Missing Data [¶](https://python.useinstructor.com/tutorials/2-tips/?q=\\#missing-data)\n\nThe Maybe pattern is a concept in functional programming used for error handling. Instead of raising exceptions or returning None, you can use a Maybe type to encapsulate both the result and potential errors.\n\nThis pattern is particularly useful when making LLM calls, as providing language models with an escape hatch can effectively reduce hallucinations.\n\nIn \\[9\\]:\n\nCopied!\n\n```\nfrom typing import Optional\n\nclass Character(BaseModel):\n    age: int\n    name: str\n\nclass MaybeCharacter(BaseModel):\n    result: Optional[Character] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n```\n\nfrom typing import Optional class Character(BaseModel): age: int name: str class MaybeCharacter(BaseModel): result: Optional\\[Character\\] = Field(default=None) error: bool = Field(default=False) message: Optional\\[str\\]\n\nIn \\[10\\]:\n\nCopied!\n\n```\ndef extract(content: str) -> MaybeCharacter:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MaybeCharacter,\n        messages=[\\\n            {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},\\\n        ],\n    )\n\n```\n\ndef extract(content: str) -> MaybeCharacter: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\_model=MaybeCharacter, messages=\\[ {\"role\": \"user\", \"content\": f\"Extract \\`{content}\\`\"}, \\], )\n\nIn \\[11\\]:\n\nCopied!\n\n```\nextract(\"Harry Potter\")\n\n```\n\nextract(\"Harry Potter\")\n\nOut\\[11\\]:\n\n```\nMaybeCharacter(result=Character(age=17, name='Harry Potter'), error=False, message=None)\n```\n\nIn \\[12\\]:\n\nCopied!\n\n```\nuser = extract(\"404 Error\")\n\nif user.error:\n    raise ValueError(user.message)\n\n```\n\nuser = extract(\"404 Error\") if user.error: raise ValueError(user.message)\n\n```\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb Cell 20 line 4\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a> user = extract(\"404 Error\")\n      <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a> if user.error:\n----> <a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>     raise ValueError(user.message)\n\nValueError: 404 Error\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/tutorials/2-tips/?q=",
      "ogUrl": "https://python.useinstructor.com/tutorials/2-tips/",
      "title": "Tips and Tricks - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/tutorials/2-tips/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/tutorials/2-tips.png",
      "ogTitle": "Tips and Tricks - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/tutorials/2-tips.png",
      "og:title": "Tips and Tricks - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/tutorials/2-tips/?q=",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/tutorials/2-tips.png",
      "twitter:title": "Tips and Tricks - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  }
]