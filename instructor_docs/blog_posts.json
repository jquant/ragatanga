[
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/archive/2023/#2023)\n\n## [Introduction to Caching in Python](https://python.useinstructor.com/blog/2023/11/26/python-caching/)\n\n> Instructor makes working with language models easy, but they are still computationally expensive.\n\nToday, we're diving into optimizing instructor code while maintaining the excellent DX offered by [Pydantic](https://docs.pydantic.dev/latest/) models. We'll tackle the challenges of caching Pydantic models, typically incompatible with `pickle`, and explore solutions that use `decorators` like `functools.cache`. Then, we'll craft custom decorators with `diskcache` and `redis` to support persistent caching and distributed systems.\n\n## [Generators and LLM Streaming](https://python.useinstructor.com/blog/2023/11/26/python-generators-and-llm-streaming/)\n\nLatency is crucial, especially in eCommerce and newer chat applications like ChatGPT. Streaming is the solution that enables us to enhance the user experience without the need for faster response times.\n\nAnd what makes streaming possible? Generators!\n\n## [Verifying LLM Citations with Pydantic](https://python.useinstructor.com/blog/2023/11/18/validate-citations/)\n\nEnsuring the accuracy of information is crucial. This blog post explores how Pydantic's powerful and flexible validators can enhance data accuracy through citation verification.\n\nWe'll start with using a simple substring check to verify citations. Then we'll use `instructor` itself to power an LLM to verify citations and align answers with the given citations. Finally, we'll explore how we can use these techniques to generate a dataset of accurate responses.\n\n## [Async Processing OpenAI using `asyncio` and `Instructor` with Python](https://python.useinstructor.com/blog/2023/11/13/learn-async/)\n\nToday, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using `instructor` and learn how to use `asyncio.gather` and `asyncio.as_completed` for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using `asyncio.Semaphore`.\n\n## [Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density](https://python.useinstructor.com/blog/2023/11/05/chain-of-density/)\n\n> Discover how to distil an iterative method like Chain Of Density into a single finetuned model using Instructor\n\nIn this article, we'll guide you through implementing the original Chain of Density method using Instructor, then show how to distile a GPT 3.5 model to match GPT-4's iterative summarization capabilities. Using these methods were able to decrease latency by 20x, reduce costs by 50x and maintain entity density.\n\nBy the end you'll end up with a GPT 3.5 model, (fine-tuned using Instructor's great tooling), capable of producing summaries that rival the effectiveness of Chain of Density [\\[Adams et al. (2023)\\]](https://arxiv.org/abs/2309.04269). As always, all code is readily available in our `examples/chain-of-density` folder in our repo for your reference.\n\n## [AI Engineer Keynote: Pydantic is all you need](https://python.useinstructor.com/blog/2023/11/02/ai-engineer-keynote-pydantic-is-all-you-need/)\n\n[![Pydantic is all you need](https://img.youtube.com/vi/yj-wSRJwrrc/0.jpg)](https://www.youtube.com/watch?v=yj-wSRJwrrc)\n\n[Click here to watch the full talk](https://www.youtube.com/watch?v=yj-wSRJwrrc)\n\n## [Good LLM Validation is Just Good Validation](https://python.useinstructor.com/blog/2023/10/23/good-llm-validation-is-just-good-validation/)\n\n> What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here.\n\nValidation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like `Pydantic` and `Instructor`. We validate these outputs using a validation function which conforms to the structure seen below.\n\n```md-code__content\ndef validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n\n```\n\n## [Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation](https://python.useinstructor.com/blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/)\n\n### [Introduction](https://python.useinstructor.com/blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/\\#introduction)\n\nGet ready to dive deep into the world of fine-tuning task specific language models with Python functions. We'll explore how the `instructor.instructions` streamlines this process, making the task you want to distil more efficient and powerful while preserving its original functionality and backwards compatibility.\n\nIf you want to see the full example checkout [examples/distillation](https://github.com/jxnl/instructor/tree/main/examples/distilations)\n\n## [RAG is more than just embedding search](https://python.useinstructor.com/blog/2023/09/17/rag-is-more-than-just-embedding-search/)\n\nWith the advent of large language models (LLM), retrieval augmented generation (RAG) has become a hot topic. However throughout the past year of [helping startups](https://jxnl.co) integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware.\n\nWhat is RAG?\n\nRetrieval augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized.\n\n![RAG](https://python.useinstructor.com/blog/img/dumb_rag.png)\n\nSimple RAG that embedded the user query and makes a search.\n\nSo let's kick things off by examining what I like to call the 'Dumb' RAG Model—a basic setup that's more common than you'd think.\n\n## [Generating Structured Output / JSON from LLMs](https://python.useinstructor.com/blog/2023/09/11/generating-structured-output--json-from-llms/)\n\nLanguage models have seen significant growth. Using them effectively often requires complex frameworks. This post discusses how Instructor simplifies this process using Pydantic.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/archive/2023/",
      "ogUrl": "https://python.useinstructor.com/blog/archive/2023/",
      "title": "2023 - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/archive/2023/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/archive/2023.png",
      "ogTitle": "2023 - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/archive/2023.png",
      "og:title": "2023 - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/archive/2023/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/archive/2023.png",
      "twitter:title": "2023 - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/api-development/#api-development)\n\n## [Introducing structured outputs with Cerebras Inference](https://python.useinstructor.com/blog/2024/10/15/introducing-structured-outputs-with-cerebras-inference/)\n\n### [What's Cerebras?](https://python.useinstructor.com/blog/2024/10/15/introducing-structured-outputs-with-cerebras-inference/\\#whats-cerebras)\n\nCerebras offers the fastest inference on the market, 20x faster than on GPUs.\n\nSign up for a Cerebras Inference API key here at [cloud.cerebras.ai](http://cloud.cerebras.ai).\n\n#### [Basic Usage](https://python.useinstructor.com/blog/2024/10/15/introducing-structured-outputs-with-cerebras-inference/\\#basic-usage)\n\nTo get guaranteed structured outputs with Cerebras Inference, you\n\n## [Structured Output for Open Source and Local LLMs](https://python.useinstructor.com/blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/)\n\nInstructor has expanded its capabilities for language models. It started with API interactions via the OpenAI SDK, using [Pydantic](https://pydantic-docs.helpmanual.io/) for structured data validation. Now, Instructor supports multiple models and platforms.\n\nThe integration of [JSON mode](https://python.useinstructor.com/concepts/patching/#json-mode) improved adaptability to vision models and open source alternatives. This allows support for models from [GPT](https://openai.com/api/) and [Mistral](https://mistral.ai) to models on [Ollama](https://ollama.ai) and [Hugging Face](https://huggingface.co/models), using [llama-cpp-python](https://python.useinstructor.com/integrations/llama-cpp-python/).\n\nInstructor now works with cloud-based APIs and local models for structured data extraction. Developers can refer to our guide on [Patching](https://python.useinstructor.com/concepts/patching/) for information on using JSON mode with different models.\n\nFor learning about Instructor and Pydantic, we offer a course on [Steering language models towards structured outputs](https://www.wandb.courses/courses/steering-language-models).\n\nThe following sections show examples of Instructor's integration with platforms and local setups for structured outputs in AI projects.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/api-development/",
      "ogUrl": "https://python.useinstructor.com/blog/category/api-development/",
      "title": "API Development - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/api-development/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/api-development.png",
      "ogTitle": "API Development - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/api-development.png",
      "og:title": "API Development - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/api-development/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/api-development.png",
      "twitter:title": "API Development - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/audio/#audio)\n\n## [Audio Support in OpenAI's Chat Completions API](https://python.useinstructor.com/blog/2024/10/17/audio-support-in-openais-chat-completions-api/)\n\nOpenAI has recently introduced audio support in their Chat Completions API, opening up exciting new possibilities for developers working with audio and text interactions. This feature is powered by the new `gpt-4o-audio-preview` model, which brings advanced voice capabilities to the familiar Chat Completions API interface.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/audio/",
      "ogUrl": "https://python.useinstructor.com/blog/category/audio/",
      "title": "Audio - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/audio/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/audio.png",
      "ogTitle": "Audio - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/audio.png",
      "og:title": "Audio - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/audio/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/audio.png",
      "twitter:title": "Audio - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/pydantic/#pydantic)\n\n## [Building an LLM-based Reranker for your RAG pipeline](https://python.useinstructor.com/blog/2024/10/23/building-an-llm-based-reranker-for-your-rag-pipeline/)\n\nAre you struggling with irrelevant search results in your Retrieval-Augmented Generation (RAG) pipeline?\n\nImagine having a powerful tool that can intelligently reassess and reorder your search results, significantly improving their relevance to user queries.\n\nIn this blog post, we'll show you how to create an LLM-based reranker using Instructor and Pydantic. This approach will:\n\n- Enhance the accuracy of your search results\n- Leverage the power of large language models (LLMs)\n- Utilize structured outputs for precise information retrieval\n\nBy the end of this tutorial, you'll be able to implement a llm reranker to label your synthetic data for fine-tuning a traditional reranker, or to build out an evaluation pipeline for your RAG system. Let's dive in!\n\n## [Building a Pairwise LLM Judge with Instructor and Pydantic](https://python.useinstructor.com/blog/2024/10/17/building-a-pairwise-llm-judge-with-instructor-and-pydantic/)\n\nIn this blog post, we'll explore how to create a pairwise LLM judge using Instructor and Pydantic. This judge will evaluate the relevance between a question and a piece of text, demonstrating a practical application of structured outputs in language model interactions.\n\n### [Introduction](https://python.useinstructor.com/blog/2024/10/17/building-a-pairwise-llm-judge-with-instructor-and-pydantic/\\#introduction)\n\nEvaluating text relevance is a common task in natural language processing and information retrieval. By leveraging large language models (LLMs) and structured outputs, we can create a system that judges the similarity or relevance between a question and a given text.\n\n## [Introducing structured outputs with Cerebras Inference](https://python.useinstructor.com/blog/2024/10/15/introducing-structured-outputs-with-cerebras-inference/)\n\n### [What's Cerebras?](https://python.useinstructor.com/blog/2024/10/15/introducing-structured-outputs-with-cerebras-inference/\\#whats-cerebras)\n\nCerebras offers the fastest inference on the market, 20x faster than on GPUs.\n\nSign up for a Cerebras Inference API key here at [cloud.cerebras.ai](http://cloud.cerebras.ai).\n\n#### [Basic Usage](https://python.useinstructor.com/blog/2024/10/15/introducing-structured-outputs-with-cerebras-inference/\\#basic-usage)\n\nTo get guaranteed structured outputs with Cerebras Inference, you\n\n## [Ensuring Consistent Timestamp Formats with Language Models](https://python.useinstructor.com/blog/2024/09/26/consistent-timestamp-formats/)\n\nGemini can Understand timestamps in language model outputs, but they can be inconsistent. Video content timestamps vary between HH:MM:SS and MM:SS formats, causing parsing errors and calculations. This post presents a technique to handle timestamps for clips and films without formatting issues.\n\nWe combine Pydantic's data validation with custom parsing for consistent timestamp handling. You'll learn to process timestamps in any format, reducing errors in video content workflows. Kinda like how we ensured [matching language in multilingal summarization](https://python.useinstructor.com/blog/2024/03/28/matching-language-summaries/) by adding a simple field.\n\nThe post provides a solution using Pydantic to improve timestamp handling in language model projects. This method addresses format inconsistencies and enables timestamp processing.\n\n## [Pydantic is Still All You Need: Reflections on a Year of Structured Outputs](https://python.useinstructor.com/blog/2024/09/07/pydantic-is-still-all-you-need/)\n\nA year ago, I gave a talk titled \"Pydantic: All You Need\" that kickstarted my Twitter career. Today, I'm back to reaffirm that message and share what I've learned in the past year about using structured outputs with language models.\n\n[Watch the youtube video](https://www.youtube.com/watch?v=pZ4DIH2BVqg)\n\n## [Matching Language in Multilingual Summarization Tasks](https://python.useinstructor.com/blog/2024/03/28/matching-language-summaries/)\n\nWhen asking language models to summarize text, there's a risk that the generated summary ends up in English, even if the source text is in another language. This is likely due to the instructions being provided in English, biasing the model towards English output.\n\nIn this post, we explore techniques to ensure the language of the generated summary matches the language of the source text. We leverage Pydantic for data validation and the `langdetect` library for language identification.\n\n## [Simple Synthetic Data Generation](https://python.useinstructor.com/blog/2024/03/08/simple-synthetic-data-generation/)\n\nWhat that people have been using instructor for is to generate synthetic data rather than extracting data itself. We can even use the J-Schemo extra fields to give specific examples to control how we generate data.\n\nConsider the example below. We'll likely generate very simple names.\n\n```md-code__content\nfrom typing import Iterable\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\ndef generate_fake_users(count: int) -> Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        messages=[\\\n            {\"role\": \"user\", \"content\": f\"Generate a {count} synthetic users\"},\\\n        ],\n    )\n\nfor user in generate_fake_users(5):\n    print(user)\n    #> name='Alice' age=25\n    #> name='Bob' age=30\n    #> name='Charlie' age=35\n    #> name='David' age=40\n    #> name='Eve' age=22\n\n```\n\n### [Leveraging Simple Examples](https://python.useinstructor.com/blog/2024/03/08/simple-synthetic-data-generation/\\#leveraging-simple-examples)\n\nWe might want to set examples as part of the prompt by leveraging Pydantics configuration. We can set examples directly in the JSON scheme itself.\n\n```md-code__content\nfrom typing import Iterable\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    name: str = Field(examples=[\"Timothee Chalamet\", \"Zendaya\"])\n    age: int\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\ndef generate_fake_users(count: int) -> Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        messages=[\\\n            {\"role\": \"user\", \"content\": f\"Generate a {count} synthetic users\"},\\\n        ],\n    )\n\nfor user in generate_fake_users(5):\n    print(user)\n    #> name='John Doe' age=25\n    #> name='Jane Smith' age=30\n    #> name='Michael Johnson' age=22\n    #> name='Emily Davis' age=28\n    #> name='David Brown' age=35\n\n```\n\nBy incorporating names of celebrities as examples, we have shifted towards generating synthetic data featuring well-known personalities, moving away from the simplistic, single-word names previously used.\n\n### [Leveraging Complex Example](https://python.useinstructor.com/blog/2024/03/08/simple-synthetic-data-generation/\\#leveraging-complex-example)\n\nTo effectively generate synthetic examples with more nuance, lets upgrade to the \"gpt-4-turbo-preview\" model, use model level examples rather than attribute level examples:\n\n```md-code__content\nimport instructor\n\nfrom typing import Iterable\nfrom pydantic import BaseModel, ConfigDict\nfrom openai import OpenAI\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    \"\"\"Old Wizards\"\"\"\n\n    name: str\n    age: int\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\\\n                {\"name\": \"Gandalf the Grey\", \"age\": 1000},\\\n                {\"name\": \"Albus Dumbledore\", \"age\": 150},\\\n            ]\n        }\n    )\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\ndef generate_fake_users(count: int) -> Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        response_model=Iterable[UserDetail],\n        messages=[\\\n            {\"role\": \"user\", \"content\": f\"Generate `{count}` synthetic examples\"},\\\n        ],\n    )\n\nfor user in generate_fake_users(5):\n    print(user)\n    #> name='Merlin' age=1000\n    #> name='Saruman the White' age=700\n    #> name='Radagast the Brown' age=600\n    #> name='Elminster Aumar' age=1200\n    #> name='Mordenkainen' age=850\n\n```\n\n### [Leveraging Descriptions](https://python.useinstructor.com/blog/2024/03/08/simple-synthetic-data-generation/\\#leveraging-descriptions)\n\nBy adjusting the descriptions within our Pydantic models, we can subtly influence the nature of the synthetic data generated. This method allows for a more nuanced control over the output, ensuring that the generated data aligns more closely with our expectations or requirements.\n\nFor instance, specifying \"Fancy French sounding names\" as a description for the `name` field in our `UserDetail` model directs the generation process to produce names that fit this particular criterion, resulting in a dataset that is both diverse and tailored to specific linguistic characteristics.\n\n```md-code__content\nimport instructor\n\nfrom typing import Iterable\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    name: str = Field(description=\"Fancy French sounding names\")\n    age: int\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\ndef generate_fake_users(count: int) -> Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        messages=[\\\n            {\"role\": \"user\", \"content\": f\"Generate `{count}` synthetic users\"},\\\n        ],\n    )\n\nfor user in generate_fake_users(5):\n    print(user)\n    #> name='Jean Luc' age=30\n    #> name='Claire Belle' age=25\n    #> name='Pierre Leclair' age=40\n    #> name='Amelie Rousseau' age=35\n    #> name='Etienne Lefevre' age=28\n\n```\n\n## [Verifying LLM Citations with Pydantic](https://python.useinstructor.com/blog/2023/11/18/validate-citations/)\n\nEnsuring the accuracy of information is crucial. This blog post explores how Pydantic's powerful and flexible validators can enhance data accuracy through citation verification.\n\nWe'll start with using a simple substring check to verify citations. Then we'll use `instructor` itself to power an LLM to verify citations and align answers with the given citations. Finally, we'll explore how we can use these techniques to generate a dataset of accurate responses.\n\n## [AI Engineer Keynote: Pydantic is all you need](https://python.useinstructor.com/blog/2023/11/02/ai-engineer-keynote-pydantic-is-all-you-need/)\n\n[![Pydantic is all you need](https://img.youtube.com/vi/yj-wSRJwrrc/0.jpg)](https://www.youtube.com/watch?v=yj-wSRJwrrc)\n\n[Click here to watch the full talk](https://www.youtube.com/watch?v=yj-wSRJwrrc)\n\n## [Good LLM Validation is Just Good Validation](https://python.useinstructor.com/blog/2023/10/23/good-llm-validation-is-just-good-validation/)\n\n> What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here.\n\nValidation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like `Pydantic` and `Instructor`. We validate these outputs using a validation function which conforms to the structure seen below.\n\n```md-code__content\ndef validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n\n```\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/pydantic/",
      "ogUrl": "https://python.useinstructor.com/blog/category/pydantic/",
      "title": "Pydantic - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/pydantic/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/pydantic.png",
      "ogTitle": "Pydantic - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/pydantic.png",
      "og:title": "Pydantic - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/pydantic/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/pydantic.png",
      "twitter:title": "Pydantic - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/page/4/#subscribe-to-our-newsletter-for-updates-and-tips)\n\n## [Simple Synthetic Data Generation](https://python.useinstructor.com/blog/2024/03/08/simple-synthetic-data-generation/)\n\nWhat that people have been using instructor for is to generate synthetic data rather than extracting data itself. We can even use the J-Schemo extra fields to give specific examples to control how we generate data.\n\nConsider the example below. We'll likely generate very simple names.\n\n```md-code__content\nfrom typing import Iterable\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\ndef generate_fake_users(count: int) -> Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        messages=[\\\n            {\"role\": \"user\", \"content\": f\"Generate a {count} synthetic users\"},\\\n        ],\n    )\n\nfor user in generate_fake_users(5):\n    print(user)\n    #> name='Alice' age=25\n    #> name='Bob' age=30\n    #> name='Charlie' age=35\n    #> name='David' age=40\n    #> name='Eve' age=22\n\n```\n\n### [Leveraging Simple Examples](https://python.useinstructor.com/blog/2024/03/08/simple-synthetic-data-generation/\\#leveraging-simple-examples)\n\nWe might want to set examples as part of the prompt by leveraging Pydantics configuration. We can set examples directly in the JSON scheme itself.\n\n```md-code__content\nfrom typing import Iterable\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    name: str = Field(examples=[\"Timothee Chalamet\", \"Zendaya\"])\n    age: int\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\ndef generate_fake_users(count: int) -> Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        messages=[\\\n            {\"role\": \"user\", \"content\": f\"Generate a {count} synthetic users\"},\\\n        ],\n    )\n\nfor user in generate_fake_users(5):\n    print(user)\n    #> name='John Doe' age=25\n    #> name='Jane Smith' age=30\n    #> name='Michael Johnson' age=22\n    #> name='Emily Davis' age=28\n    #> name='David Brown' age=35\n\n```\n\nBy incorporating names of celebrities as examples, we have shifted towards generating synthetic data featuring well-known personalities, moving away from the simplistic, single-word names previously used.\n\n### [Leveraging Complex Example](https://python.useinstructor.com/blog/2024/03/08/simple-synthetic-data-generation/\\#leveraging-complex-example)\n\nTo effectively generate synthetic examples with more nuance, lets upgrade to the \"gpt-4-turbo-preview\" model, use model level examples rather than attribute level examples:\n\n```md-code__content\nimport instructor\n\nfrom typing import Iterable\nfrom pydantic import BaseModel, ConfigDict\nfrom openai import OpenAI\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    \"\"\"Old Wizards\"\"\"\n\n    name: str\n    age: int\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\\\n                {\"name\": \"Gandalf the Grey\", \"age\": 1000},\\\n                {\"name\": \"Albus Dumbledore\", \"age\": 150},\\\n            ]\n        }\n    )\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\ndef generate_fake_users(count: int) -> Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        response_model=Iterable[UserDetail],\n        messages=[\\\n            {\"role\": \"user\", \"content\": f\"Generate `{count}` synthetic examples\"},\\\n        ],\n    )\n\nfor user in generate_fake_users(5):\n    print(user)\n    #> name='Merlin' age=1000\n    #> name='Saruman the White' age=700\n    #> name='Radagast the Brown' age=600\n    #> name='Elminster Aumar' age=1200\n    #> name='Mordenkainen' age=850\n\n```\n\n### [Leveraging Descriptions](https://python.useinstructor.com/blog/2024/03/08/simple-synthetic-data-generation/\\#leveraging-descriptions)\n\nBy adjusting the descriptions within our Pydantic models, we can subtly influence the nature of the synthetic data generated. This method allows for a more nuanced control over the output, ensuring that the generated data aligns more closely with our expectations or requirements.\n\nFor instance, specifying \"Fancy French sounding names\" as a description for the `name` field in our `UserDetail` model directs the generation process to produce names that fit this particular criterion, resulting in a dataset that is both diverse and tailored to specific linguistic characteristics.\n\n```md-code__content\nimport instructor\n\nfrom typing import Iterable\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    name: str = Field(description=\"Fancy French sounding names\")\n    age: int\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\ndef generate_fake_users(count: int) -> Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        messages=[\\\n            {\"role\": \"user\", \"content\": f\"Generate `{count}` synthetic users\"},\\\n        ],\n    )\n\nfor user in generate_fake_users(5):\n    print(user)\n    #> name='Jean Luc' age=30\n    #> name='Claire Belle' age=25\n    #> name='Pierre Leclair' age=40\n    #> name='Amelie Rousseau' age=35\n    #> name='Etienne Lefevre' age=28\n\n```\n\n## [Structured Output for Open Source and Local LLMs](https://python.useinstructor.com/blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/)\n\nInstructor has expanded its capabilities for language models. It started with API interactions via the OpenAI SDK, using [Pydantic](https://pydantic-docs.helpmanual.io/) for structured data validation. Now, Instructor supports multiple models and platforms.\n\nThe integration of [JSON mode](https://python.useinstructor.com/concepts/patching/#json-mode) improved adaptability to vision models and open source alternatives. This allows support for models from [GPT](https://openai.com/api/) and [Mistral](https://mistral.ai) to models on [Ollama](https://ollama.ai) and [Hugging Face](https://huggingface.co/models), using [llama-cpp-python](https://python.useinstructor.com/integrations/llama-cpp-python/).\n\nInstructor now works with cloud-based APIs and local models for structured data extraction. Developers can refer to our guide on [Patching](https://python.useinstructor.com/concepts/patching/) for information on using JSON mode with different models.\n\nFor learning about Instructor and Pydantic, we offer a course on [Steering language models towards structured outputs](https://www.wandb.courses/courses/steering-language-models).\n\nThe following sections show examples of Instructor's integration with platforms and local setups for structured outputs in AI projects.\n\n## [Why Instructor is the Best Library for Structured LLM Outputs](https://python.useinstructor.com/blog/2024/03/05/zero-cost-abstractions/)\n\nLarge language models (LLMs) like GPTs are incredibly powerful, but working with their open-ended text outputs can be challenging. This is where the Instructor library shines - it allows you to easily map LLM outputs to structured data using Python type annotations.\n\n## [Seamless Support with Langsmith](https://python.useinstructor.com/blog/2024/02/18/seamless-support-with-langsmith/)\n\nIts a common misconception that LangChain's [LangSmith](https://www.langchain.com/langsmith) is only compatible with LangChain's models. In reality, LangSmith is a unified DevOps platform for developing, collaborating, testing, deploying, and monitoring LLM applications. In this blog we will explore how LangSmith can be used to enhance the OpenAI client alongside `instructor`.\n\n## [Free course on Weights and Biases](https://python.useinstructor.com/blog/2024/02/14/weights-and-biases-course/)\n\nI just released a free course on wits and biases. It goes over the material from [tutorial](https://python.useinstructor.com/tutorials/1-introduction/). Check it out at [wandb.courses](https://www.wandb.courses/courses/steering-language-models) its free and open to everyone and just under an hour long!\n\n[![](https://python.useinstructor.com/blog/img/course.png)](https://www.wandb.courses/courses/steering-language-models)\n\n> Click the image to access the course\n\n## [Introduction to Caching in Python](https://python.useinstructor.com/blog/2023/11/26/python-caching/)\n\n> Instructor makes working with language models easy, but they are still computationally expensive.\n\nToday, we're diving into optimizing instructor code while maintaining the excellent DX offered by [Pydantic](https://docs.pydantic.dev/latest/) models. We'll tackle the challenges of caching Pydantic models, typically incompatible with `pickle`, and explore solutions that use `decorators` like `functools.cache`. Then, we'll craft custom decorators with `diskcache` and `redis` to support persistent caching and distributed systems.\n\n## [Generators and LLM Streaming](https://python.useinstructor.com/blog/2023/11/26/python-generators-and-llm-streaming/)\n\nLatency is crucial, especially in eCommerce and newer chat applications like ChatGPT. Streaming is the solution that enables us to enhance the user experience without the need for faster response times.\n\nAnd what makes streaming possible? Generators!\n\n## [Verifying LLM Citations with Pydantic](https://python.useinstructor.com/blog/2023/11/18/validate-citations/)\n\nEnsuring the accuracy of information is crucial. This blog post explores how Pydantic's powerful and flexible validators can enhance data accuracy through citation verification.\n\nWe'll start with using a simple substring check to verify citations. Then we'll use `instructor` itself to power an LLM to verify citations and align answers with the given citations. Finally, we'll explore how we can use these techniques to generate a dataset of accurate responses.\n\n## [Async Processing OpenAI using `asyncio` and `Instructor` with Python](https://python.useinstructor.com/blog/2023/11/13/learn-async/)\n\nToday, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using `instructor` and learn how to use `asyncio.gather` and `asyncio.as_completed` for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using `asyncio.Semaphore`.\n\n## [Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density](https://python.useinstructor.com/blog/2023/11/05/chain-of-density/)\n\n> Discover how to distil an iterative method like Chain Of Density into a single finetuned model using Instructor\n\nIn this article, we'll guide you through implementing the original Chain of Density method using Instructor, then show how to distile a GPT 3.5 model to match GPT-4's iterative summarization capabilities. Using these methods were able to decrease latency by 20x, reduce costs by 50x and maintain entity density.\n\nBy the end you'll end up with a GPT 3.5 model, (fine-tuned using Instructor's great tooling), capable of producing summaries that rival the effectiveness of Chain of Density [\\[Adams et al. (2023)\\]](https://arxiv.org/abs/2309.04269). As always, all code is readily available in our `examples/chain-of-density` folder in our repo for your reference.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/page/4/",
      "ogUrl": "https://python.useinstructor.com/blog/page/4/",
      "title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/page/4/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/page/4/index.png",
      "ogTitle": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/page/4/index.png",
      "og:title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/page/4/",
      "statusCode": 200,
      "description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/page/4/index.png",
      "twitter:title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "og:description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/writer-sdk/#writer-sdk)\n\n## [Structured Outputs with Writer now supported](https://python.useinstructor.com/blog/2024/11/19/writer-support/)\n\nWe're excited to announce that `instructor` now supports [Writer](https://writer.com)'s enterprise-grade LLMs, including their latest Palmyra X 004 model. This integration enables structured outputs and enterprise AI workflows with Writer's powerful language models.\n\n### [Getting Started](https://python.useinstructor.com/blog/2024/11/19/writer-support/\\#getting-started)\n\nFirst, make sure that you've signed up for an account on [Writer](https://app.writer.com/aistudio/signup?utm_campaign=devrel) and obtained an API key using this [quickstart guide](https://dev.writer.com/api-guides/quickstart). Once you've done so, install `instructor` with Writer support by running `pip install instructor[writer]` in your terminal.\n\nMake sure to set the `WRITER_API_KEY` environment variable with your Writer API key or pass it as an argument to the `Writer` constructor.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/writer-sdk/",
      "ogUrl": "https://python.useinstructor.com/blog/category/writer-sdk/",
      "title": "Writer SDK - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/writer-sdk/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/writer-sdk.png",
      "ogTitle": "Writer SDK - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/writer-sdk.png",
      "og:title": "Writer SDK - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/writer-sdk/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/writer-sdk.png",
      "twitter:title": "Writer SDK - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/gemini/#gemini)\n\n## [Eliminating Hallucinations with Structured Outputs using Gemini](https://python.useinstructor.com/blog/2024/11/15/eliminating-hallucinations-with-structured-outputs-using-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to generate accurate citations from PDFs. This approach ensures that answers are grounded in the actual content of the PDF, reducing the risk of hallucinations.\n\nWe'll be using the Nvidia 10k report for this example which you can download at this [link](https://d18rn0p25nwr6d.cloudfront.net/CIK-0001045810/78501ce3-7816-4c4d-8688-53dd140df456.pdf).\n\n## [PDF Processing with Structured Outputs with Gemini](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to analyse the [Gemini 1.5 Pro Paper](https://github.com/google-gemini/generative-ai-python/blob/0e5c5f25fe4ce266791fa2afb20d17dee780ca9e/third_party/test.pdf) and extract a structured summary.\n\n### [The Problem](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#the-problem)\n\nProcessing PDFs programmatically has always been painful. The typical approaches all have significant drawbacks:\n\n- **PDF parsing libraries** require complex rules and break easily\n- **OCR solutions** are slow and error-prone\n- **Specialized PDF APIs** are expensive and require additional integration\n- **LLM solutions** often need complex document chunking and embedding pipelines\n\nWhat if we could just hand a PDF to an LLM and get structured data back? With Gemini's multimodal capabilities and Instructor's structured output handling, we can do exactly that.\n\n### [Quick Setup](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#quick-setup)\n\nFirst, install the required packages:\n\n```md-code__content\npip install \"instructor[google-generativeai]\"\n\n```\n\nThen, here's all the code you need:\n\n```md-code__content\nimport instructor\nimport google.generativeai as genai\nfrom google.ai.generativelanguage_v1beta.types.file import File\nfrom pydantic import BaseModel\nimport time\n\n# Initialize the client\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    )\n)\n\n# Define your output structure\nclass Summary(BaseModel):\n    summary: str\n\n# Upload the PDF\nfile = genai.upload_file(\"path/to/your.pdf\")\n\n# Wait for file to finish processing\nwhile file.state != File.State.ACTIVE:\n    time.sleep(1)\n    file = genai.get_file(file.name)\n    print(f\"File is still uploading, state: {file.state}\")\n\nprint(f\"File is now active, state: {file.state}\")\nprint(file)\n\nresp = client.chat.completions.create(\n    messages=[\\\n        {\"role\": \"user\", \"content\": [\"Summarize the following file\", file]},\\\n    ],\n    response_model=Summary,\n)\n\nprint(resp.summary)\n\n```\n\nExpand to see Raw Results\n\n```md-code__content\nsummary=\"Gemini 1.5 Pro is a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. It achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Gemini 1.5 Pro is built to handle extremely long contexts; it has the ability to recall and reason over fine-grained information from up to at least 10M tokens. This scale is unprecedented among contemporary large language models (LLMs), and enables the processing of long-form mixed-modality inputs including entire collections of documents, multiple hours of video, and almost five days long of audio. Gemini 1.5 Pro surpasses Gemini 1.0 Pro and performs at a similar level to 1.0 Ultra on a wide array of benchmarks while requiring significantly less compute to train. It can recall information amidst distractor context, and it can learn to translate a new language from a single set of linguistic documentation. With only instructional materials (a 500-page reference grammar, a dictionary, and ≈ 400 extra parallel sentences) all provided in context, Gemini 1.5 Pro is capable of learning to translate from English to Kalamang, a Papuan language with fewer than 200 speakers, and therefore almost no online presence.\"\n\n```\n\n### [Benefits](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#benefits)\n\nThe combination of Gemini and Instructor offers several key advantages over traditional PDF processing approaches:\n\n**Simple Integration** \\- Unlike traditional approaches that require complex document processing pipelines, chunking strategies, and embedding databases, you can directly process PDFs with just a few lines of code. This dramatically reduces development time and maintenance overhead.\n\n**Structured Output** \\- Instructor's Pydantic integration ensures you get exactly the data structure you need. The model's outputs are automatically validated and typed, making it easier to build reliable applications. If the extraction fails, Instructor automatically handles the retries for you with support for [custom retry logic using tenacity](https://python.useinstructor.com/concepts/retrying/).\n\n**Multimodal Support** \\- Gemini's multimodal capabilities mean this same approach works for various file types. You can process images, videos, and audio files all in the same api request. Check out our [multimodal processing guide](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/) to see how we extract structured data from travel videos.\n\n### [Conclusion](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#conclusion)\n\nWorking with PDFs doesn't have to be complicated.\n\nBy combining Gemini's multimodal capabilities with Instructor's structured output handling, we can transform complex document processing into simple, Pythonic code.\n\nNo more wrestling with parsing rules, managing embeddings, or building complex pipelines – just define your data model and let the LLM do the heavy lifting.\n\nIf you liked this, give `instructor` a try today and see how much easier structured outputs makes working with LLMs become. [Get started with Instructor today!](https://python.useinstructor.com/)\n\n## [Structured Outputs with Multimodal Gemini](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to analyze [travel videos](https://www.youtube.com/watch?v=_R8yhW_H9NQ) and extract structured recommendations. This powerful combination allows us to process multimodal inputs (video) and generate structured outputs using Pydantic models. This post was done in collaboration with [Kino.ai](https://kino.ai), a company that uses instructor to do structured extraction from multimodal inputs to improve search for film makers.\n\n### [Setting Up the Environment](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/\\#setting-up-the-environment)\n\nFirst, let's set up our environment with the necessary libraries:\n\n```md-code__content\n\n```\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/gemini/",
      "ogUrl": "https://python.useinstructor.com/blog/category/gemini/",
      "title": "Gemini - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/gemini/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/gemini.png",
      "ogTitle": "Gemini - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/gemini.png",
      "og:title": "Gemini - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/gemini/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/gemini.png",
      "twitter:title": "Gemini - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/python/#python)\n\n## [Implementing Anthropic's Contextual Retrieval with Async Processing](https://python.useinstructor.com/blog/2024/09/26/implementing-anthropics-contextual-retrieval-with-async-processing/)\n\nAnthropic's [Contextual Retrieval](https://www.anthropic.com/blog/contextual-retrieval-for-rag) technique enhances RAG systems by preserving crucial context.\n\nThis post examines the method and demonstrates an efficient implementation using async processing. We'll explore how to optimize your RAG applications with this approach, building on concepts from our [async processing guide](https://python.useinstructor.com/blog/2023/11/13/learn-async/).\n\n## [Good LLM Validation is Just Good Validation](https://python.useinstructor.com/blog/2023/10/23/good-llm-validation-is-just-good-validation/)\n\n> What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here.\n\nValidation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like `Pydantic` and `Instructor`. We validate these outputs using a validation function which conforms to the structure seen below.\n\n```md-code__content\ndef validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n\n```\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/python/",
      "ogUrl": "https://python.useinstructor.com/blog/category/python/",
      "title": "Python - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/python/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/python.png",
      "ogTitle": "Python - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/python.png",
      "og:title": "Python - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/python/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/python.png",
      "twitter:title": "Python - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/data-processing/#data-processing)\n\n## [Flashcard generator with Instructor + Burr](https://python.useinstructor.com/blog/2024/10/18/youtube-flashcards/)\n\nFlashcards help break down complex topics and learn anything from biology to a new language or lines for a play. This blog will show how to use LLMs to generate flashcards and kickstart your learning!\n\n**Instructor** lets us get structured outputs from LLMs reliably, and [Burr](https://github.com/dagworks-inc/burr) helps create an LLM application that's easy to understand and debug. It comes with **Burr UI**, a free, open-source, and local-first tool for observability, annotations, and more!\n\n## [Analyzing Youtube Transcripts with Instructor](https://python.useinstructor.com/blog/2024/07/11/youtube-transcripts/)\n\n### [Extracting Chapter Information](https://python.useinstructor.com/blog/2024/07/11/youtube-transcripts/\\#extracting-chapter-information)\n\nCode Snippets\n\nAs always, the code is readily available in our `examples/youtube` folder in our repo for your reference in the `run.py` file.\n\nIn this post, we'll show you how to summarise Youtube video transcripts into distinct chapters using `instructor` before exploring some ways you can adapt the code to different applications.\n\nBy the end of this article, you'll be able to build an application as per the video below.\n\n![](https://python.useinstructor.com/img/youtube.gif)\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/data-processing/",
      "ogUrl": "https://python.useinstructor.com/blog/category/data-processing/",
      "title": "Data Processing - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/data-processing/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/data-processing.png",
      "ogTitle": "Data Processing - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/data-processing.png",
      "og:title": "Data Processing - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/data-processing/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/data-processing.png",
      "twitter:title": "Data Processing - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/anthropic/#anthropic)\n\n## [Structured Outputs and Prompt Caching with Anthropic](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-and-prompt-caching-with-anthropic/)\n\nAnthropic's ecosystem now offers two powerful features for AI developers: structured outputs and prompt caching. These advancements enable more efficient use of large language models (LLMs). This guide demonstrates how to leverage these features with the Instructor library to enhance your AI applications.\n\n### [Structured Outputs with Anthropic and Instructor](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-and-prompt-caching-with-anthropic/\\#structured-outputs-with-anthropic-and-instructor)\n\nInstructor now offers seamless integration with Anthropic's powerful language models, allowing developers to easily create structured outputs using Pydantic models. This integration simplifies the process of extracting specific information from AI-generated responses.\n\n## [Implementing Anthropic's Contextual Retrieval with Async Processing](https://python.useinstructor.com/blog/2024/09/26/implementing-anthropics-contextual-retrieval-with-async-processing/)\n\nAnthropic's [Contextual Retrieval](https://www.anthropic.com/blog/contextual-retrieval-for-rag) technique enhances RAG systems by preserving crucial context.\n\nThis post examines the method and demonstrates an efficient implementation using async processing. We'll explore how to optimize your RAG applications with this approach, building on concepts from our [async processing guide](https://python.useinstructor.com/blog/2023/11/13/learn-async/).\n\n## [Why should I use prompt caching?](https://python.useinstructor.com/blog/2024/09/14/why-should-i-use-prompt-caching/)\n\nDevelopers often face two key challenges when working with large context - Slow response times and high costs. This is especially true when we're making multiple of these calls over time, severely impacting the cost and latency of our applications. With Anthropic's new prompt caching feature, we can easily solve both of these issues.\n\nSince the new feature is still in beta, we're going to wait for it to be generally avaliable before we integrate it into instructor. In the meantime, we've put together a quickstart guide on how to use the feature in your own applications.\n\n## [Structured Outputs with Anthropic](https://python.useinstructor.com/blog/2024/03/20/structured-outputs-with-anthropic/)\n\nA special shoutout to [Shreya](https://twitter.com/shreyaw_) for her contributions to the anthropic support. As of now, all features are operational with the exception of streaming support.\n\nFor those eager to experiment, simply patch the client with `ANTHROPIC_JSON`, which will enable you to leverage the `anthropic` client for making requests.\n\n```md-code__content\npip install instructor[anthropic]\n\n```\n\nMissing Features\n\nJust want to acknowledge that we know that we are missing partial streaming and some better re-asking support for XML. We are working on it and will have it soon.\n\n```md-code__content\nfrom pydantic import BaseModel\nfrom typing import List\nimport anthropic\nimport instructor\n\n# Patching the Anthropics client with the instructor for enhanced capabilities\nanthropic_client = instructor.from_openai(\n    create=anthropic.Anthropic().messages.create,\n    mode=instructor.Mode.ANTHROPIC_JSON\n)\n\nclass Properties(BaseModel):\n    name: str\n    value: str\n\nclass User(BaseModel):\n    name: str\n    age: int\n    properties: List[Properties]\n\nuser_response = anthropic_client(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=1024,\n    max_retries=0,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Create a user for a model with a name, age, and properties.\",\\\n        }\\\n    ],\n    response_model=User,\n)  # type: ignore\n\nprint(user_response.model_dump_json(indent=2))\n\"\"\"\n{\n    \"name\": \"John\",\n    \"age\": 25,\n    \"properties\": [\\\n        {\\\n            \"key\": \"favorite_color\",\\\n            \"value\": \"blue\"\\\n        }\\\n    ]\n}\n\n```\n\nWe're encountering challenges with deeply nested types and eagerly invite the community to test, provide feedback, and suggest necessary improvements as we enhance the anthropic client's support.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/anthropic/",
      "ogUrl": "https://python.useinstructor.com/blog/category/anthropic/",
      "title": "Anthropic - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/anthropic/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/anthropic.png",
      "ogTitle": "Anthropic - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/anthropic.png",
      "og:title": "Anthropic - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/anthropic/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/anthropic.png",
      "twitter:title": "Anthropic - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/openai/#openai)\n\n## [Do I Still Need Instructor with Google's New OpenAI Integration?](https://python.useinstructor.com/blog/2024/11/10/do-i-still-need-instructor-with-googles-new-openai-integration/)\n\nGoogle recently launched OpenAI client compatibility for Gemini.\n\nWhile this is a significant step forward for developers by simplifying Gemini model interactions, **you absolutely still need instructor**.\n\nIf you're unfamiliar with instructor, we provide a simple interface to get structured outputs from LLMs across different providers.\n\nThis makes it easy to switch between providers, get reliable outputs from language models and ultimately build production grade LLM applications.\n\n## [Audio Support in OpenAI's Chat Completions API](https://python.useinstructor.com/blog/2024/10/17/audio-support-in-openais-chat-completions-api/)\n\nOpenAI has recently introduced audio support in their Chat Completions API, opening up exciting new possibilities for developers working with audio and text interactions. This feature is powered by the new `gpt-4o-audio-preview` model, which brings advanced voice capabilities to the familiar Chat Completions API interface.\n\n## [OpenAI API Model Distillation with Instructor](https://python.useinstructor.com/blog/2024/10/02/openai-api-model-distillation-with-instructor/)\n\nOpenAI has recently introduced a new feature called [API Model Distillation](https://openai.com/index/api-model-distillation/), which allows developers to create custom models tailored to their specific use cases. This feature is particularly powerful when combined with Instructor's structured output capabilities. In this post, we'll explore how to leverage API Model Distillation with Instructor to create more efficient and specialized models.\n\n## [Should I Be Using Structured Outputs?](https://python.useinstructor.com/blog/2024/08/20/should-i-be-using-structured-outputs/)\n\nOpenAI recently announced Structured Outputs which ensures that generated responses match any arbitrary provided JSON Schema. In their [announcement article](https://openai.com/index/introducing-structured-outputs-in-the-api/), they acknowledged that it had been inspired by libraries such as `instructor`.\n\n### [Main Challenges](https://python.useinstructor.com/blog/2024/08/20/should-i-be-using-structured-outputs/\\#main-challenges)\n\nIf you're building complex LLM workflows, you've likely considered OpenAI's Structured Outputs as a potential replacement for `instructor`.\n\nBut before you do so, three key challenges remain:\n\n1. **Limited Validation And Retry Logic**: Structured Outputs ensure adherence to the schema but not useful content. You might get perfectly formatted yet unhelpful responses\n2. **Streaming Challenges**: Parsing raw JSON objects from streamed responses with the sdk is error-prone and inefficient\n3. **Unpredictable Latency Issues** : Structured Outputs suffers from random latency spikes that might result in an almost 20x increase in response time\n\nAdditionally, adopting Structured Outputs locks you into OpenAI's ecosystem, limiting your ability to experiment with diverse models or providers that might better suit specific use-cases.\n\nThis vendor lock-in increases vulnerability to provider outages, potentially causing application downtime and SLA violations, which can damage user trust and impact your business reputation.\n\nIn this article, we'll show how `instructor` addresses many of these challenges with features such as automatic reasking when validation fails, automatic support for validated streaming data and more.\n\n## [Announcing instructor=1.0.0](https://python.useinstructor.com/blog/2024/04/01/announce-instructor-v1/)\n\nOver the past 10 months, we've build up instructor with the [principle](https://python.useinstructor.com/why/) of 'easy to try, and easy to delete'. We accomplished this by patching the openai client with the `instructor` package and adding new arguments like `response_model`, `max_retries`, and `validation_context`. As a result I truely believe isntructor is the [best way](https://python.useinstructor.com/blog/2024/03/05/zero-cost-abstractions/) to get structured data out of llm apis.\n\nBut as a result, we've been a bit stuck on getting typing to work well while giving you more control at development time. I'm excited to launch version 1.0.0 which cleans up the api w.r.t. typing without compromising the ease of use.\n\n## [Free course on Weights and Biases](https://python.useinstructor.com/blog/2024/02/14/weights-and-biases-course/)\n\nI just released a free course on wits and biases. It goes over the material from [tutorial](https://python.useinstructor.com/tutorials/1-introduction/). Check it out at [wandb.courses](https://www.wandb.courses/courses/steering-language-models) its free and open to everyone and just under an hour long!\n\n[![](https://python.useinstructor.com/blog/img/course.png)](https://www.wandb.courses/courses/steering-language-models)\n\n> Click the image to access the course\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/openai/",
      "ogUrl": "https://python.useinstructor.com/blog/category/openai/",
      "title": "OpenAI - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/openai/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/openai.png",
      "ogTitle": "OpenAI - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/openai.png",
      "og:title": "OpenAI - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/openai/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/openai.png",
      "twitter:title": "OpenAI - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/performance-optimization/#performance-optimization)\n\n## [Introducing structured outputs with Cerebras Inference](https://python.useinstructor.com/blog/2024/10/15/introducing-structured-outputs-with-cerebras-inference/)\n\n### [What's Cerebras?](https://python.useinstructor.com/blog/2024/10/15/introducing-structured-outputs-with-cerebras-inference/\\#whats-cerebras)\n\nCerebras offers the fastest inference on the market, 20x faster than on GPUs.\n\nSign up for a Cerebras Inference API key here at [cloud.cerebras.ai](http://cloud.cerebras.ai).\n\n#### [Basic Usage](https://python.useinstructor.com/blog/2024/10/15/introducing-structured-outputs-with-cerebras-inference/\\#basic-usage)\n\nTo get guaranteed structured outputs with Cerebras Inference, you\n\n## [Introduction to Caching in Python](https://python.useinstructor.com/blog/2023/11/26/python-caching/)\n\n> Instructor makes working with language models easy, but they are still computationally expensive.\n\nToday, we're diving into optimizing instructor code while maintaining the excellent DX offered by [Pydantic](https://docs.pydantic.dev/latest/) models. We'll tackle the challenges of caching Pydantic models, typically incompatible with `pickle`, and explore solutions that use `decorators` like `functools.cache`. Then, we'll craft custom decorators with `diskcache` and `redis` to support persistent caching and distributed systems.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/performance-optimization/",
      "ogUrl": "https://python.useinstructor.com/blog/category/performance-optimization/",
      "title": "Performance Optimization - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/performance-optimization/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/performance-optimization.png",
      "ogTitle": "Performance Optimization - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/performance-optimization.png",
      "og:title": "Performance Optimization - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/performance-optimization/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/performance-optimization.png",
      "twitter:title": "Performance Optimization - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/multimodal/#multimodal)\n\n## [Structured Outputs with Multimodal Gemini](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to analyze [travel videos](https://www.youtube.com/watch?v=_R8yhW_H9NQ) and extract structured recommendations. This powerful combination allows us to process multimodal inputs (video) and generate structured outputs using Pydantic models. This post was done in collaboration with [Kino.ai](https://kino.ai), a company that uses instructor to do structured extraction from multimodal inputs to improve search for film makers.\n\n### [Setting Up the Environment](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/\\#setting-up-the-environment)\n\nFirst, let's set up our environment with the necessary libraries:\n\n```md-code__content\n\n```\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/multimodal/",
      "ogUrl": "https://python.useinstructor.com/blog/category/multimodal/",
      "title": "Multimodal - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/multimodal/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/multimodal.png",
      "ogTitle": "Multimodal - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/multimodal.png",
      "og:title": "Multimodal - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/multimodal/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/multimodal.png",
      "twitter:title": "Multimodal - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/document-processing/#document-processing)\n\n## [Eliminating Hallucinations with Structured Outputs using Gemini](https://python.useinstructor.com/blog/2024/11/15/eliminating-hallucinations-with-structured-outputs-using-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to generate accurate citations from PDFs. This approach ensures that answers are grounded in the actual content of the PDF, reducing the risk of hallucinations.\n\nWe'll be using the Nvidia 10k report for this example which you can download at this [link](https://d18rn0p25nwr6d.cloudfront.net/CIK-0001045810/78501ce3-7816-4c4d-8688-53dd140df456.pdf).\n\n## [PDF Processing with Structured Outputs with Gemini](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to analyse the [Gemini 1.5 Pro Paper](https://github.com/google-gemini/generative-ai-python/blob/0e5c5f25fe4ce266791fa2afb20d17dee780ca9e/third_party/test.pdf) and extract a structured summary.\n\n### [The Problem](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#the-problem)\n\nProcessing PDFs programmatically has always been painful. The typical approaches all have significant drawbacks:\n\n- **PDF parsing libraries** require complex rules and break easily\n- **OCR solutions** are slow and error-prone\n- **Specialized PDF APIs** are expensive and require additional integration\n- **LLM solutions** often need complex document chunking and embedding pipelines\n\nWhat if we could just hand a PDF to an LLM and get structured data back? With Gemini's multimodal capabilities and Instructor's structured output handling, we can do exactly that.\n\n### [Quick Setup](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#quick-setup)\n\nFirst, install the required packages:\n\n```md-code__content\npip install \"instructor[google-generativeai]\"\n\n```\n\nThen, here's all the code you need:\n\n```md-code__content\nimport instructor\nimport google.generativeai as genai\nfrom google.ai.generativelanguage_v1beta.types.file import File\nfrom pydantic import BaseModel\nimport time\n\n# Initialize the client\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    )\n)\n\n# Define your output structure\nclass Summary(BaseModel):\n    summary: str\n\n# Upload the PDF\nfile = genai.upload_file(\"path/to/your.pdf\")\n\n# Wait for file to finish processing\nwhile file.state != File.State.ACTIVE:\n    time.sleep(1)\n    file = genai.get_file(file.name)\n    print(f\"File is still uploading, state: {file.state}\")\n\nprint(f\"File is now active, state: {file.state}\")\nprint(file)\n\nresp = client.chat.completions.create(\n    messages=[\\\n        {\"role\": \"user\", \"content\": [\"Summarize the following file\", file]},\\\n    ],\n    response_model=Summary,\n)\n\nprint(resp.summary)\n\n```\n\nExpand to see Raw Results\n\n```md-code__content\nsummary=\"Gemini 1.5 Pro is a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. It achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Gemini 1.5 Pro is built to handle extremely long contexts; it has the ability to recall and reason over fine-grained information from up to at least 10M tokens. This scale is unprecedented among contemporary large language models (LLMs), and enables the processing of long-form mixed-modality inputs including entire collections of documents, multiple hours of video, and almost five days long of audio. Gemini 1.5 Pro surpasses Gemini 1.0 Pro and performs at a similar level to 1.0 Ultra on a wide array of benchmarks while requiring significantly less compute to train. It can recall information amidst distractor context, and it can learn to translate a new language from a single set of linguistic documentation. With only instructional materials (a 500-page reference grammar, a dictionary, and ≈ 400 extra parallel sentences) all provided in context, Gemini 1.5 Pro is capable of learning to translate from English to Kalamang, a Papuan language with fewer than 200 speakers, and therefore almost no online presence.\"\n\n```\n\n### [Benefits](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#benefits)\n\nThe combination of Gemini and Instructor offers several key advantages over traditional PDF processing approaches:\n\n**Simple Integration** \\- Unlike traditional approaches that require complex document processing pipelines, chunking strategies, and embedding databases, you can directly process PDFs with just a few lines of code. This dramatically reduces development time and maintenance overhead.\n\n**Structured Output** \\- Instructor's Pydantic integration ensures you get exactly the data structure you need. The model's outputs are automatically validated and typed, making it easier to build reliable applications. If the extraction fails, Instructor automatically handles the retries for you with support for [custom retry logic using tenacity](https://python.useinstructor.com/concepts/retrying/).\n\n**Multimodal Support** \\- Gemini's multimodal capabilities mean this same approach works for various file types. You can process images, videos, and audio files all in the same api request. Check out our [multimodal processing guide](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/) to see how we extract structured data from travel videos.\n\n### [Conclusion](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#conclusion)\n\nWorking with PDFs doesn't have to be complicated.\n\nBy combining Gemini's multimodal capabilities with Instructor's structured output handling, we can transform complex document processing into simple, Pythonic code.\n\nNo more wrestling with parsing rules, managing embeddings, or building complex pipelines – just define your data model and let the LLM do the heavy lifting.\n\nIf you liked this, give `instructor` a try today and see how much easier structured outputs makes working with LLMs become. [Get started with Instructor today!](https://python.useinstructor.com/)\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/document-processing/",
      "ogUrl": "https://python.useinstructor.com/blog/category/document-processing/",
      "title": "Document Processing - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/document-processing/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/document-processing.png",
      "ogTitle": "Document Processing - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/document-processing.png",
      "og:title": "Document Processing - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/document-processing/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/document-processing.png",
      "twitter:title": "Document Processing - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/structured-outputs/#structured-outputs)\n\n## [Using Structured Outputs to convert messy tables into tidy data](https://python.useinstructor.com/blog/2024/11/21/using-structured-outputs-to-convert-messy-tables-into-tidy-data/)\n\n### [Why is this a problem?](https://python.useinstructor.com/blog/2024/11/21/using-structured-outputs-to-convert-messy-tables-into-tidy-data/\\#why-is-this-a-problem)\n\nMessy data exports are a common problem. Whether it's multiple headers in the table, implicit relationships that make analysis a pain or even just merged cells, using `instructor` with structured outputs makes it easy to convert messy tables into tidy data, even if all you have is just an image of the table as we'll see below.\n\nLet's look at the following table as an example. It makes analysis unnecessarily difficult because it hides data relationships through empty cells and implicit repetition. If we were using it for data analysis, cleaning it manually would be a huge nightmare.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/structured-outputs/",
      "ogUrl": "https://python.useinstructor.com/blog/category/structured-outputs/",
      "title": "Structured Outputs - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/structured-outputs/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/structured-outputs.png",
      "ogTitle": "Structured Outputs - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/structured-outputs.png",
      "og:title": "Structured Outputs - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/structured-outputs/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/structured-outputs.png",
      "twitter:title": "Structured Outputs - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/page/2/#subscribe-to-our-newsletter-for-updates-and-tips)\n\n## [Building a Pairwise LLM Judge with Instructor and Pydantic](https://python.useinstructor.com/blog/2024/10/17/building-a-pairwise-llm-judge-with-instructor-and-pydantic/)\n\nIn this blog post, we'll explore how to create a pairwise LLM judge using Instructor and Pydantic. This judge will evaluate the relevance between a question and a piece of text, demonstrating a practical application of structured outputs in language model interactions.\n\n### [Introduction](https://python.useinstructor.com/blog/2024/10/17/building-a-pairwise-llm-judge-with-instructor-and-pydantic/\\#introduction)\n\nEvaluating text relevance is a common task in natural language processing and information retrieval. By leveraging large language models (LLMs) and structured outputs, we can create a system that judges the similarity or relevance between a question and a given text.\n\n## [Introducing structured outputs with Cerebras Inference](https://python.useinstructor.com/blog/2024/10/15/introducing-structured-outputs-with-cerebras-inference/)\n\n### [What's Cerebras?](https://python.useinstructor.com/blog/2024/10/15/introducing-structured-outputs-with-cerebras-inference/\\#whats-cerebras)\n\nCerebras offers the fastest inference on the market, 20x faster than on GPUs.\n\nSign up for a Cerebras Inference API key here at [cloud.cerebras.ai](http://cloud.cerebras.ai).\n\n#### [Basic Usage](https://python.useinstructor.com/blog/2024/10/15/introducing-structured-outputs-with-cerebras-inference/\\#basic-usage)\n\nTo get guaranteed structured outputs with Cerebras Inference, you\n\n## [OpenAI API Model Distillation with Instructor](https://python.useinstructor.com/blog/2024/10/02/openai-api-model-distillation-with-instructor/)\n\nOpenAI has recently introduced a new feature called [API Model Distillation](https://openai.com/index/api-model-distillation/), which allows developers to create custom models tailored to their specific use cases. This feature is particularly powerful when combined with Instructor's structured output capabilities. In this post, we'll explore how to leverage API Model Distillation with Instructor to create more efficient and specialized models.\n\n## [Bad Schemas could break your LLM Structured Outputs](https://python.useinstructor.com/blog/2024/09/26/bad-schemas-could-break-your-llm-structured-outputs/)\n\nYou might be leaving up to 60% performance gains on the table with the wrong response model. Response Models impact model performance massively with Claude and GPT-4o, irregardless of you’re using JSON mode or Tool Calling.\n\nUsing the right response model can help ensure [your models respond in the right language](https://python.useinstructor.com/blog/2024/03/28/matching-language-summaries/) or prevent [hallucinations when extracting video timestamps](https://python.useinstructor.com/blog/2024/09/26/consistent-timestamp-formats/).\n\nWe decided to investigate this by benchmarking Claude and GPT-4o on the GSM8k dataset and found that\n\n1. **Field Naming drastically impacts performance** \\- Changing a single field name from `final_choice` to `answer` improved model accuracy from 4.5% to 95%. The way we structure and name fields in our response models can fundamentally alter how the model interprets and responds to queries.\n2. **Chain Of Thought significantly boosts performance** \\- Adding a `reasoning` field increased model accuracy by 60% on the GSM8k dataset. Models perform significantly better when they explain their logic step-by-step.\n3. **Be careful with JSON mode** \\- JSON mode exhibited 50% more performance variation than Tool Calling when renaming fields. Different response models showed varying levels of performance between JSON mode and Tool Calling, indicating that JSON mode requires more careful optimisation.\n\n## [Implementing Anthropic's Contextual Retrieval with Async Processing](https://python.useinstructor.com/blog/2024/09/26/implementing-anthropics-contextual-retrieval-with-async-processing/)\n\nAnthropic's [Contextual Retrieval](https://www.anthropic.com/blog/contextual-retrieval-for-rag) technique enhances RAG systems by preserving crucial context.\n\nThis post examines the method and demonstrates an efficient implementation using async processing. We'll explore how to optimize your RAG applications with this approach, building on concepts from our [async processing guide](https://python.useinstructor.com/blog/2023/11/13/learn-async/).\n\n## [Ensuring Consistent Timestamp Formats with Language Models](https://python.useinstructor.com/blog/2024/09/26/consistent-timestamp-formats/)\n\nGemini can Understand timestamps in language model outputs, but they can be inconsistent. Video content timestamps vary between HH:MM:SS and MM:SS formats, causing parsing errors and calculations. This post presents a technique to handle timestamps for clips and films without formatting issues.\n\nWe combine Pydantic's data validation with custom parsing for consistent timestamp handling. You'll learn to process timestamps in any format, reducing errors in video content workflows. Kinda like how we ensured [matching language in multilingal summarization](https://python.useinstructor.com/blog/2024/03/28/matching-language-summaries/) by adding a simple field.\n\nThe post provides a solution using Pydantic to improve timestamp handling in language model projects. This method addresses format inconsistencies and enables timestamp processing.\n\n## [Instructor Proposal: Integrating Jinja Templating](https://python.useinstructor.com/blog/2024/09/19/instructor-proposal-integrating-jinja-templating/)\n\nAs the creator of Instructor, I've always aimed to keep our product development streamlined and avoid unnecessary complexity. However, I'm now convinced that it's time to incorporate better templating into our data structure, specifically by integrating Jinja.\n\nThis decision serves multiple purposes:\n\n1. It addresses the growing complexity in my prompt formatting needs\n2. It allows us to differentiate ourselves from the standard library while adding proven utility.\n3. It aligns with the practices I've consistently employed in both production and client code.\n4. It provides an opportunity to introduce API changes that have been tested in private versions of Instructor.\n\n### [Why Jinja is the Right Choice](https://python.useinstructor.com/blog/2024/09/19/instructor-proposal-integrating-jinja-templating/\\#why-jinja-is-the-right-choice)\n\n01. **Formatting Capabilities**\n02. Prompt formatting complexity has increased.\n03. List iteration and conditional implementation are necessary for formatting.\n04. This improves chunk generation, few shots, and dynamic rules.\n\n05. **Validation**\n\n06. Jinja template variables serve rendering and validation purposes.\n07. Pydantic's validation context allows access to template variables in validation functions.\n\n08. **Versioning and Logging**\n\n09. Render variable separation enhances prompt versioning and logging.\n10. Template variable diffing simplifies prompt change comparisons.\n\nBy integrating Jinja into Instructor, we're not just adding a feature; we're enhancing our ability to handle complex formatting, improve validation processes, and streamline our versioning and logging capabilities. This addition will significantly boost the power and flexibility of Instructor, making it an even more robust tool for our users.\n\n### [Enhancing Formatting Capabilities](https://python.useinstructor.com/blog/2024/09/19/instructor-proposal-integrating-jinja-templating/\\#enhancing-formatting-capabilities)\n\nIn Instructor, we propose implementing a new `context` keyword in our create methods. This addition will allow users to render the prompt using a provided context, leveraging Jinja's templating capabilities. Here's how it would work:\n\n1. Users pass a `context` dictionary to the create method.\n2. The prompt template, written in Jinja syntax, is defined in the `content` field of the message.\n3. Instructor renders the prompt using the provided context, filling in the template variables.\n\nThis approach offers these benefits:\n\n- Separation of prompt structure and dynamic content\n- Management of complex prompts with conditionals and loops\n- Reusability of prompt templates across different contexts\n\nLet's look at an example to illustrate this feature:\n\n```md-code__content\nclient.create(\n    model=\"gpt-4o\",\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"\"\"\\\n                You are a {{ role }} tasks with the following question\\\n\\\n                <question>\\\n                {{ question }}\\\n                </question>\\\n\\\n                Use the following context to answer the question, make sure to return [id] for every citation:\\\n\\\n                <context>\\\n                {% for chunk in context %}\\\n                  <context_chunk>\\\n                    <id>{{ chunk.id }}</id>\\\n                    <text>{{ chunk.text }}</text>\\\n                  </context_chunk>\\\n                {% endfor %}\\\n                </context>\\\n\\\n                {% if rules %}\\\n                Make sure to follow these rules:\\\n\\\n                {% for rule in rules %}\\\n                  * {{ rule }}\\\n                {% endfor %}\\\n                {% endif %}\\\n            \"\"\",\\\n        },\\\n    ],\n    context={\n        \"role\": \"professional educator\",\n        \"question\": \"What is the capital of France?\",\n        \"context\": [\\\n            {\"id\": 1, \"text\": \"Paris is the capital of France.\"},\\\n            {\"id\": 2, \"text\": \"France is a country in Europe.\"},\\\n        ],\n        \"rules\": [\"Use markdown.\"],\n    },\n)\n\n```\n\n### [Validation](https://python.useinstructor.com/blog/2024/09/19/instructor-proposal-integrating-jinja-templating/\\#validation)\n\nLet's consider a scenario where we redact words from text. By using `ValidationInfo` to access context and passing it to the validator and template, we can implement a system for handling sensitive information. This approach allows us to:\n\n1. Validate input to ensure it doesn't contain banned words.\n2. Redact patterns using regular expressions.\n3. Provide instructions to the language model about word usage restrictions.\n\nHere's an example demonstrating this concept using Pydantic validators:\n\n```md-code__content\nfrom pydantic import BaseModel, ValidationInfo, field_validator\n\nclass Response(BaseModel):\n    text: str\n\n    @field_validator('text')\n    @classmethod\n    def no_banned_words(cls, v: str, info: ValidationInfo):\n        context = info.context\n        if context:\n            banned_words = context.get('banned_words', set())\n            banned_words_found = [word for word in banned_words if word.lower() in v.lower()]\n            if banned_words_found:\n                raise ValueError(f\"Banned words found in text: {', '.join(banned_words_found)}, rewrite it but just without the banned words\")\n        return v\n\n    @field_validator('text')\n    @classmethod\n    def redact_regex(cls, v: str, info: ValidationInfo):\n        context = info.context\n        if context:\n            redact_patterns = context.get('redact_patterns', [])\n            for pattern in redact_patterns:\n                v = re.sub(pattern, '****', v)\n        return v\n\nresponse = client.create(\n    model=\"gpt-4o\",\n    response_model=Response,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"\"\"\\\n                Write about a {{ topic }}\\\n\\\n                {% if banned_words %}\\\n                You must not use the following banned words:\\\n\\\n                <banned_words>\\\n                {% for word in banned_words %}\\\n                * {{ word }}\\\n                {% endfor %}\\\n                </banned_words>\\\n                {% endif %}\\\n              \"\"\"\\\n        },\\\n    ],\n    context={\n        \"topic\": \"jason and now his phone number is 123-456-7890\"\n        \"banned_words\": [\"jason\"],\n        \"redact_patterns\": [\\\n            r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",  # Phone number pattern\\\n            r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",          # SSN pattern\\\n        ],\n    },\n    max_retries=3,\n)\n\nprint(response.text)\n# > While i can't say his name anymore, his phone number is ****\n\n```\n\n### [Better Versioning and Logging](https://python.useinstructor.com/blog/2024/09/19/instructor-proposal-integrating-jinja-templating/\\#better-versioning-and-logging)\n\nWith the separation of prompt templates and variables, we gain several advantages:\n\n1. Version Control: We can now version the templates and retrieve the appropriate one for a given prompt. This allows for better management of template history, diffing and comparison.\n\n2. Enhanced Logging: The separation facilitates structured logging, enabling easier debugging and integration with various logging sinks, databases, and observability tools like OpenTelemetry.\n\n3. Security: Sensitive information in variables can be handled separately from the templates, allowing for better access control and data protection.\n\n\nThis separation of concerns adheres to best practices in software design, resulting in a more maintainable, scalable, and robust system for managing prompts and their associated data.\n\n#### [Side effect of Context also being Pydantic Models](https://python.useinstructor.com/blog/2024/09/19/instructor-proposal-integrating-jinja-templating/\\#side-effect-of-context-also-being-pydantic-models)\n\nSince they are just python objects we can use Pydantic models to validate the context and also control how they are rendered, so even secret information can be dynamically rendered! Consider using secret string to pass in sensitive information to the llm.\n\n```md-code__content\nfrom pydantic import BaseModel, SecretStr\n\nclass UserContext(BaseModel):\n    name: str\n    address: SecretStr\n\nclass Address(BaseModel):\n    street: SecretStr\n    city: str\n    state: str\n    zipcode: str\n\ndef normalize_address(address: Address):\n    context = UserContext(username=\"scolvin\", address=address)\n    address = client.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": \"{{ user.name }} is `{{ user.address.get_secret_value() }}`, normalize it to an address object\",\\\n            },\\\n        ],\n        context={\"user\": context},\n    )\n    print(context)\n    #> UserContext(username='jliu', address=\"******\")\n    print(address)\n    #> Address(street='******', city=\"Toronto\", state=\"Ontario\", zipcode=\"M5A 0J3\")\n    logger.info(\n        f\"Normalized address: {address}\",\n        extra={\"user_context\": context, \"address\": address},\n    )\n    return address\n\n```\n\nThis approach offers several advantages:\n\n1. Secure logging: You can confidently log your template variables without risking the exposure of sensitive information.\n2. Type safety: Pydantic models provide type checking and validation, reducing the risk of errors.\n3. Flexibility: You can easily control how different types of data are displayed or used in templates.\n\n## [Why should I use prompt caching?](https://python.useinstructor.com/blog/2024/09/14/why-should-i-use-prompt-caching/)\n\nDevelopers often face two key challenges when working with large context - Slow response times and high costs. This is especially true when we're making multiple of these calls over time, severely impacting the cost and latency of our applications. With Anthropic's new prompt caching feature, we can easily solve both of these issues.\n\nSince the new feature is still in beta, we're going to wait for it to be generally avaliable before we integrate it into instructor. In the meantime, we've put together a quickstart guide on how to use the feature in your own applications.\n\n## [Pydantic is Still All You Need: Reflections on a Year of Structured Outputs](https://python.useinstructor.com/blog/2024/09/07/pydantic-is-still-all-you-need/)\n\nA year ago, I gave a talk titled \"Pydantic: All You Need\" that kickstarted my Twitter career. Today, I'm back to reaffirm that message and share what I've learned in the past year about using structured outputs with language models.\n\n[Watch the youtube video](https://www.youtube.com/watch?v=pZ4DIH2BVqg)\n\n## [Structured Outputs for Gemini now supported](https://python.useinstructor.com/blog/2024/09/03/structured-outputs-for-gemini-now-supported/)\n\nWe're excited to announce that `instructor` now supports structured outputs using tool calling for both the Gemini SDK and the VertexAI SDK.\n\nA special shoutout to [Sonal](https://x.com/sonalsaldanha) for his contributions to the Gemini Tool Calling support.\n\nLet's walk through a simple example of how to use these new features\n\n### [Installation](https://python.useinstructor.com/blog/2024/09/03/structured-outputs-for-gemini-now-supported/\\#installation)\n\nTo get started, install the latest version of `instructor`. Depending on whether you're using Gemini or VertexAI, you should install the following:\n\n[Gemini](#__tabbed_2_1)[VertexAI](#__tabbed_2_2)\n\n```md-code__content\npip install \"instructor[google-generativeai]\"\n\n```\n\n```md-code__content\npip install \"instructor[vertexai]\"\n\n```\n\nThis ensures that you have the necessary dependencies to use the Gemini or VertexAI SDKs with instructor.\n\nWe recommend using the Gemini SDK over the VertexAI SDK for two main reasons.\n\n1. Compared to the VertexAI SDK, the Gemini SDK comes with a free daily quota of 1.5 billion tokens to use for developers.\n2. The Gemini SDK is significantly easier to setup, all you need is a `GOOGLE_API_KEY` that you can generate in your GCP console. THe VertexAI SDK on the other hand requires a credentials.json file or an OAuth integration to use.\n\n### [Getting Started](https://python.useinstructor.com/blog/2024/09/03/structured-outputs-for-gemini-now-supported/\\#getting-started)\n\nWith our provider agnostic API, you can use the same interface to interact with both SDKs, the only thing that changes here is how we initialise the client itself.\n\nBefore running the following code, you'll need to make sure that you have your Gemini API Key set in your shell under the alias `GOOGLE_API_KEY`.\n\n```md-code__content\nimport instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    )\n)\n\nresp = client.chat.completions.create(\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Extract Jason is 25 years old.\",\\\n        }\\\n    ],\n    response_model=User,\n)\n\nprint(resp)\n#> name='Jason' age=25\n\n```\n\nWe can achieve a similar thing with the VertexAI SDK. For this to work, you'll need to authenticate to VertexAI.\n\nThere are some instructions [here](https://cloud.google.com/vertex-ai/docs/authentication) but the easiest way I found was to simply download the GCloud cli and run `gcloud auth application-default login`.\n\n```md-code__content\nimport instructor\nimport vertexai  # type: ignore\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nfrom pydantic import BaseModel\n\nvertexai.init()\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-1.5-pro-preview-0409\"),\n)\n\nresp = client.chat.completions.create(\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Extract Jason is 25 years old.\",\\\n        }\\\n    ],\n    response_model=User,\n)\n\nprint(resp)\n#> name='Jason' age=25\n\n```\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/page/2/",
      "ogUrl": "https://python.useinstructor.com/blog/page/2/",
      "title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/page/2/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/page/2/index.png",
      "ogTitle": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/page/2/index.png",
      "og:title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/page/2/",
      "statusCode": 200,
      "description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/page/2/index.png",
      "twitter:title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "og:description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/#subscribe-to-our-newsletter-for-updates-and-tips)\n\n## [Using Structured Outputs to convert messy tables into tidy data](https://python.useinstructor.com/blog/2024/11/21/using-structured-outputs-to-convert-messy-tables-into-tidy-data/)\n\n### [Why is this a problem?](https://python.useinstructor.com/blog/2024/11/21/using-structured-outputs-to-convert-messy-tables-into-tidy-data/\\#why-is-this-a-problem)\n\nMessy data exports are a common problem. Whether it's multiple headers in the table, implicit relationships that make analysis a pain or even just merged cells, using `instructor` with structured outputs makes it easy to convert messy tables into tidy data, even if all you have is just an image of the table as we'll see below.\n\nLet's look at the following table as an example. It makes analysis unnecessarily difficult because it hides data relationships through empty cells and implicit repetition. If we were using it for data analysis, cleaning it manually would be a huge nightmare.\n\n## [Structured Outputs with Writer now supported](https://python.useinstructor.com/blog/2024/11/19/writer-support/)\n\nWe're excited to announce that `instructor` now supports [Writer](https://writer.com/)'s enterprise-grade LLMs, including their latest Palmyra X 004 model. This integration enables structured outputs and enterprise AI workflows with Writer's powerful language models.\n\n### [Getting Started](https://python.useinstructor.com/blog/2024/11/19/writer-support/\\#getting-started)\n\nFirst, make sure that you've signed up for an account on [Writer](https://app.writer.com/aistudio/signup?utm_campaign=devrel) and obtained an API key using this [quickstart guide](https://dev.writer.com/api-guides/quickstart). Once you've done so, install `instructor` with Writer support by running `pip install instructor[writer]` in your terminal.\n\nMake sure to set the `WRITER_API_KEY` environment variable with your Writer API key or pass it as an argument to the `Writer` constructor.\n\n## [Eliminating Hallucinations with Structured Outputs using Gemini](https://python.useinstructor.com/blog/2024/11/15/eliminating-hallucinations-with-structured-outputs-using-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to generate accurate citations from PDFs. This approach ensures that answers are grounded in the actual content of the PDF, reducing the risk of hallucinations.\n\nWe'll be using the Nvidia 10k report for this example which you can download at this [link](https://d18rn0p25nwr6d.cloudfront.net/CIK-0001045810/78501ce3-7816-4c4d-8688-53dd140df456.pdf).\n\n## [PDF Processing with Structured Outputs with Gemini](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to analyse the [Gemini 1.5 Pro Paper](https://github.com/google-gemini/generative-ai-python/blob/0e5c5f25fe4ce266791fa2afb20d17dee780ca9e/third_party/test.pdf) and extract a structured summary.\n\n### [The Problem](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#the-problem)\n\nProcessing PDFs programmatically has always been painful. The typical approaches all have significant drawbacks:\n\n- **PDF parsing libraries** require complex rules and break easily\n- **OCR solutions** are slow and error-prone\n- **Specialized PDF APIs** are expensive and require additional integration\n- **LLM solutions** often need complex document chunking and embedding pipelines\n\nWhat if we could just hand a PDF to an LLM and get structured data back? With Gemini's multimodal capabilities and Instructor's structured output handling, we can do exactly that.\n\n### [Quick Setup](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#quick-setup)\n\nFirst, install the required packages:\n\n```md-code__content\npip install \"instructor[google-generativeai]\"\n\n```\n\nThen, here's all the code you need:\n\n```md-code__content\nimport instructor\nimport google.generativeai as genai\nfrom google.ai.generativelanguage_v1beta.types.file import File\nfrom pydantic import BaseModel\nimport time\n\n# Initialize the client\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    )\n)\n\n# Define your output structure\nclass Summary(BaseModel):\n    summary: str\n\n# Upload the PDF\nfile = genai.upload_file(\"path/to/your.pdf\")\n\n# Wait for file to finish processing\nwhile file.state != File.State.ACTIVE:\n    time.sleep(1)\n    file = genai.get_file(file.name)\n    print(f\"File is still uploading, state: {file.state}\")\n\nprint(f\"File is now active, state: {file.state}\")\nprint(file)\n\nresp = client.chat.completions.create(\n    messages=[\\\n        {\"role\": \"user\", \"content\": [\"Summarize the following file\", file]},\\\n    ],\n    response_model=Summary,\n)\n\nprint(resp.summary)\n\n```\n\nExpand to see Raw Results\n\n```md-code__content\nsummary=\"Gemini 1.5 Pro is a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. It achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Gemini 1.5 Pro is built to handle extremely long contexts; it has the ability to recall and reason over fine-grained information from up to at least 10M tokens. This scale is unprecedented among contemporary large language models (LLMs), and enables the processing of long-form mixed-modality inputs including entire collections of documents, multiple hours of video, and almost five days long of audio. Gemini 1.5 Pro surpasses Gemini 1.0 Pro and performs at a similar level to 1.0 Ultra on a wide array of benchmarks while requiring significantly less compute to train. It can recall information amidst distractor context, and it can learn to translate a new language from a single set of linguistic documentation. With only instructional materials (a 500-page reference grammar, a dictionary, and ≈ 400 extra parallel sentences) all provided in context, Gemini 1.5 Pro is capable of learning to translate from English to Kalamang, a Papuan language with fewer than 200 speakers, and therefore almost no online presence.\"\n\n```\n\n### [Benefits](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#benefits)\n\nThe combination of Gemini and Instructor offers several key advantages over traditional PDF processing approaches:\n\n**Simple Integration** \\- Unlike traditional approaches that require complex document processing pipelines, chunking strategies, and embedding databases, you can directly process PDFs with just a few lines of code. This dramatically reduces development time and maintenance overhead.\n\n**Structured Output** \\- Instructor's Pydantic integration ensures you get exactly the data structure you need. The model's outputs are automatically validated and typed, making it easier to build reliable applications. If the extraction fails, Instructor automatically handles the retries for you with support for [custom retry logic using tenacity](https://python.useinstructor.com/concepts/retrying/).\n\n**Multimodal Support** \\- Gemini's multimodal capabilities mean this same approach works for various file types. You can process images, videos, and audio files all in the same api request. Check out our [multimodal processing guide](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/) to see how we extract structured data from travel videos.\n\n### [Conclusion](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#conclusion)\n\nWorking with PDFs doesn't have to be complicated.\n\nBy combining Gemini's multimodal capabilities with Instructor's structured output handling, we can transform complex document processing into simple, Pythonic code.\n\nNo more wrestling with parsing rules, managing embeddings, or building complex pipelines – just define your data model and let the LLM do the heavy lifting.\n\nIf you liked this, give `instructor` a try today and see how much easier structured outputs makes working with LLMs become. [Get started with Instructor today!](https://python.useinstructor.com/)\n\n## [Do I Still Need Instructor with Google's New OpenAI Integration?](https://python.useinstructor.com/blog/2024/11/10/do-i-still-need-instructor-with-googles-new-openai-integration/)\n\nGoogle recently launched OpenAI client compatibility for Gemini.\n\nWhile this is a significant step forward for developers by simplifying Gemini model interactions, **you absolutely still need instructor**.\n\nIf you're unfamiliar with instructor, we provide a simple interface to get structured outputs from LLMs across different providers.\n\nThis makes it easy to switch between providers, get reliable outputs from language models and ultimately build production grade LLM applications.\n\n## [Building an LLM-based Reranker for your RAG pipeline](https://python.useinstructor.com/blog/2024/10/23/building-an-llm-based-reranker-for-your-rag-pipeline/)\n\nAre you struggling with irrelevant search results in your Retrieval-Augmented Generation (RAG) pipeline?\n\nImagine having a powerful tool that can intelligently reassess and reorder your search results, significantly improving their relevance to user queries.\n\nIn this blog post, we'll show you how to create an LLM-based reranker using Instructor and Pydantic. This approach will:\n\n- Enhance the accuracy of your search results\n- Leverage the power of large language models (LLMs)\n- Utilize structured outputs for precise information retrieval\n\nBy the end of this tutorial, you'll be able to implement a llm reranker to label your synthetic data for fine-tuning a traditional reranker, or to build out an evaluation pipeline for your RAG system. Let's dive in!\n\n## [Structured Outputs with Multimodal Gemini](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to analyze [travel videos](https://www.youtube.com/watch?v=_R8yhW_H9NQ) and extract structured recommendations. This powerful combination allows us to process multimodal inputs (video) and generate structured outputs using Pydantic models. This post was done in collaboration with [Kino.ai](https://kino.ai/), a company that uses instructor to do structured extraction from multimodal inputs to improve search for film makers.\n\n### [Setting Up the Environment](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/\\#setting-up-the-environment)\n\nFirst, let's set up our environment with the necessary libraries:\n\n```md-code__content\n\n```\n\n## [Structured Outputs and Prompt Caching with Anthropic](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-and-prompt-caching-with-anthropic/)\n\nAnthropic's ecosystem now offers two powerful features for AI developers: structured outputs and prompt caching. These advancements enable more efficient use of large language models (LLMs). This guide demonstrates how to leverage these features with the Instructor library to enhance your AI applications.\n\n### [Structured Outputs with Anthropic and Instructor](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-and-prompt-caching-with-anthropic/\\#structured-outputs-with-anthropic-and-instructor)\n\nInstructor now offers seamless integration with Anthropic's powerful language models, allowing developers to easily create structured outputs using Pydantic models. This integration simplifies the process of extracting specific information from AI-generated responses.\n\n## [Flashcard generator with Instructor + Burr](https://python.useinstructor.com/blog/2024/10/18/youtube-flashcards/)\n\nFlashcards help break down complex topics and learn anything from biology to a new language or lines for a play. This blog will show how to use LLMs to generate flashcards and kickstart your learning!\n\n**Instructor** lets us get structured outputs from LLMs reliably, and [Burr](https://github.com/dagworks-inc/burr) helps create an LLM application that's easy to understand and debug. It comes with **Burr UI**, a free, open-source, and local-first tool for observability, annotations, and more!\n\n## [Audio Support in OpenAI's Chat Completions API](https://python.useinstructor.com/blog/2024/10/17/audio-support-in-openais-chat-completions-api/)\n\nOpenAI has recently introduced audio support in their Chat Completions API, opening up exciting new possibilities for developers working with audio and text interactions. This feature is powered by the new `gpt-4o-audio-preview` model, which brings advanced voice capabilities to the familiar Chat Completions API interface.",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/",
      "ogUrl": "https://python.useinstructor.com/blog/",
      "title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/",
      "robots": "noindex",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/index.png",
      "ogTitle": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/index.png",
      "og:title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "viewport": [
        "width=device-width,initial-scale=1",
        "width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"
      ],
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/",
      "statusCode": 200,
      "description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/index.png",
      "twitter:title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "og:description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/llm-techniques/#llm-techniques)\n\n## [Bad Schemas could break your LLM Structured Outputs](https://python.useinstructor.com/blog/2024/09/26/bad-schemas-could-break-your-llm-structured-outputs/)\n\nYou might be leaving up to 60% performance gains on the table with the wrong response model. Response Models impact model performance massively with Claude and GPT-4o, irregardless of you’re using JSON mode or Tool Calling.\n\nUsing the right response model can help ensure [your models respond in the right language](https://python.useinstructor.com/blog/2024/03/28/matching-language-summaries/) or prevent [hallucinations when extracting video timestamps](https://python.useinstructor.com/blog/2024/09/26/consistent-timestamp-formats/).\n\nWe decided to investigate this by benchmarking Claude and GPT-4o on the GSM8k dataset and found that\n\n1. **Field Naming drastically impacts performance** \\- Changing a single field name from `final_choice` to `answer` improved model accuracy from 4.5% to 95%. The way we structure and name fields in our response models can fundamentally alter how the model interprets and responds to queries.\n2. **Chain Of Thought significantly boosts performance** \\- Adding a `reasoning` field increased model accuracy by 60% on the GSM8k dataset. Models perform significantly better when they explain their logic step-by-step.\n3. **Be careful with JSON mode** \\- JSON mode exhibited 50% more performance variation than Tool Calling when renaming fields. Different response models showed varying levels of performance between JSON mode and Tool Calling, indicating that JSON mode requires more careful optimisation.\n\n## [Implementing Anthropic's Contextual Retrieval with Async Processing](https://python.useinstructor.com/blog/2024/09/26/implementing-anthropics-contextual-retrieval-with-async-processing/)\n\nAnthropic's [Contextual Retrieval](https://www.anthropic.com/blog/contextual-retrieval-for-rag) technique enhances RAG systems by preserving crucial context.\n\nThis post examines the method and demonstrates an efficient implementation using async processing. We'll explore how to optimize your RAG applications with this approach, building on concepts from our [async processing guide](https://python.useinstructor.com/blog/2023/11/13/learn-async/).\n\n## [Instructor Proposal: Integrating Jinja Templating](https://python.useinstructor.com/blog/2024/09/19/instructor-proposal-integrating-jinja-templating/)\n\nAs the creator of Instructor, I've always aimed to keep our product development streamlined and avoid unnecessary complexity. However, I'm now convinced that it's time to incorporate better templating into our data structure, specifically by integrating Jinja.\n\nThis decision serves multiple purposes:\n\n1. It addresses the growing complexity in my prompt formatting needs\n2. It allows us to differentiate ourselves from the standard library while adding proven utility.\n3. It aligns with the practices I've consistently employed in both production and client code.\n4. It provides an opportunity to introduce API changes that have been tested in private versions of Instructor.\n\n### [Why Jinja is the Right Choice](https://python.useinstructor.com/blog/2024/09/19/instructor-proposal-integrating-jinja-templating/\\#why-jinja-is-the-right-choice)\n\n01. **Formatting Capabilities**\n02. Prompt formatting complexity has increased.\n03. List iteration and conditional implementation are necessary for formatting.\n04. This improves chunk generation, few shots, and dynamic rules.\n\n05. **Validation**\n\n06. Jinja template variables serve rendering and validation purposes.\n07. Pydantic's validation context allows access to template variables in validation functions.\n\n08. **Versioning and Logging**\n\n09. Render variable separation enhances prompt versioning and logging.\n10. Template variable diffing simplifies prompt change comparisons.\n\nBy integrating Jinja into Instructor, we're not just adding a feature; we're enhancing our ability to handle complex formatting, improve validation processes, and streamline our versioning and logging capabilities. This addition will significantly boost the power and flexibility of Instructor, making it an even more robust tool for our users.\n\n### [Enhancing Formatting Capabilities](https://python.useinstructor.com/blog/2024/09/19/instructor-proposal-integrating-jinja-templating/\\#enhancing-formatting-capabilities)\n\nIn Instructor, we propose implementing a new `context` keyword in our create methods. This addition will allow users to render the prompt using a provided context, leveraging Jinja's templating capabilities. Here's how it would work:\n\n1. Users pass a `context` dictionary to the create method.\n2. The prompt template, written in Jinja syntax, is defined in the `content` field of the message.\n3. Instructor renders the prompt using the provided context, filling in the template variables.\n\nThis approach offers these benefits:\n\n- Separation of prompt structure and dynamic content\n- Management of complex prompts with conditionals and loops\n- Reusability of prompt templates across different contexts\n\nLet's look at an example to illustrate this feature:\n\n```md-code__content\nclient.create(\n    model=\"gpt-4o\",\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"\"\"\\\n                You are a {{ role }} tasks with the following question\\\n\\\n                <question>\\\n                {{ question }}\\\n                </question>\\\n\\\n                Use the following context to answer the question, make sure to return [id] for every citation:\\\n\\\n                <context>\\\n                {% for chunk in context %}\\\n                  <context_chunk>\\\n                    <id>{{ chunk.id }}</id>\\\n                    <text>{{ chunk.text }}</text>\\\n                  </context_chunk>\\\n                {% endfor %}\\\n                </context>\\\n\\\n                {% if rules %}\\\n                Make sure to follow these rules:\\\n\\\n                {% for rule in rules %}\\\n                  * {{ rule }}\\\n                {% endfor %}\\\n                {% endif %}\\\n            \"\"\",\\\n        },\\\n    ],\n    context={\n        \"role\": \"professional educator\",\n        \"question\": \"What is the capital of France?\",\n        \"context\": [\\\n            {\"id\": 1, \"text\": \"Paris is the capital of France.\"},\\\n            {\"id\": 2, \"text\": \"France is a country in Europe.\"},\\\n        ],\n        \"rules\": [\"Use markdown.\"],\n    },\n)\n\n```\n\n### [Validation](https://python.useinstructor.com/blog/2024/09/19/instructor-proposal-integrating-jinja-templating/\\#validation)\n\nLet's consider a scenario where we redact words from text. By using `ValidationInfo` to access context and passing it to the validator and template, we can implement a system for handling sensitive information. This approach allows us to:\n\n1. Validate input to ensure it doesn't contain banned words.\n2. Redact patterns using regular expressions.\n3. Provide instructions to the language model about word usage restrictions.\n\nHere's an example demonstrating this concept using Pydantic validators:\n\n```md-code__content\nfrom pydantic import BaseModel, ValidationInfo, field_validator\n\nclass Response(BaseModel):\n    text: str\n\n    @field_validator('text')\n    @classmethod\n    def no_banned_words(cls, v: str, info: ValidationInfo):\n        context = info.context\n        if context:\n            banned_words = context.get('banned_words', set())\n            banned_words_found = [word for word in banned_words if word.lower() in v.lower()]\n            if banned_words_found:\n                raise ValueError(f\"Banned words found in text: {', '.join(banned_words_found)}, rewrite it but just without the banned words\")\n        return v\n\n    @field_validator('text')\n    @classmethod\n    def redact_regex(cls, v: str, info: ValidationInfo):\n        context = info.context\n        if context:\n            redact_patterns = context.get('redact_patterns', [])\n            for pattern in redact_patterns:\n                v = re.sub(pattern, '****', v)\n        return v\n\nresponse = client.create(\n    model=\"gpt-4o\",\n    response_model=Response,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"\"\"\\\n                Write about a {{ topic }}\\\n\\\n                {% if banned_words %}\\\n                You must not use the following banned words:\\\n\\\n                <banned_words>\\\n                {% for word in banned_words %}\\\n                * {{ word }}\\\n                {% endfor %}\\\n                </banned_words>\\\n                {% endif %}\\\n              \"\"\"\\\n        },\\\n    ],\n    context={\n        \"topic\": \"jason and now his phone number is 123-456-7890\"\n        \"banned_words\": [\"jason\"],\n        \"redact_patterns\": [\\\n            r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",  # Phone number pattern\\\n            r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",          # SSN pattern\\\n        ],\n    },\n    max_retries=3,\n)\n\nprint(response.text)\n# > While i can't say his name anymore, his phone number is ****\n\n```\n\n### [Better Versioning and Logging](https://python.useinstructor.com/blog/2024/09/19/instructor-proposal-integrating-jinja-templating/\\#better-versioning-and-logging)\n\nWith the separation of prompt templates and variables, we gain several advantages:\n\n1. Version Control: We can now version the templates and retrieve the appropriate one for a given prompt. This allows for better management of template history, diffing and comparison.\n\n2. Enhanced Logging: The separation facilitates structured logging, enabling easier debugging and integration with various logging sinks, databases, and observability tools like OpenTelemetry.\n\n3. Security: Sensitive information in variables can be handled separately from the templates, allowing for better access control and data protection.\n\n\nThis separation of concerns adheres to best practices in software design, resulting in a more maintainable, scalable, and robust system for managing prompts and their associated data.\n\n#### [Side effect of Context also being Pydantic Models](https://python.useinstructor.com/blog/2024/09/19/instructor-proposal-integrating-jinja-templating/\\#side-effect-of-context-also-being-pydantic-models)\n\nSince they are just python objects we can use Pydantic models to validate the context and also control how they are rendered, so even secret information can be dynamically rendered! Consider using secret string to pass in sensitive information to the llm.\n\n```md-code__content\nfrom pydantic import BaseModel, SecretStr\n\nclass UserContext(BaseModel):\n    name: str\n    address: SecretStr\n\nclass Address(BaseModel):\n    street: SecretStr\n    city: str\n    state: str\n    zipcode: str\n\ndef normalize_address(address: Address):\n    context = UserContext(username=\"scolvin\", address=address)\n    address = client.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": \"{{ user.name }} is `{{ user.address.get_secret_value() }}`, normalize it to an address object\",\\\n            },\\\n        ],\n        context={\"user\": context},\n    )\n    print(context)\n    #> UserContext(username='jliu', address=\"******\")\n    print(address)\n    #> Address(street='******', city=\"Toronto\", state=\"Ontario\", zipcode=\"M5A 0J3\")\n    logger.info(\n        f\"Normalized address: {address}\",\n        extra={\"user_context\": context, \"address\": address},\n    )\n    return address\n\n```\n\nThis approach offers several advantages:\n\n1. Secure logging: You can confidently log your template variables without risking the exposure of sensitive information.\n2. Type safety: Pydantic models provide type checking and validation, reducing the risk of errors.\n3. Flexibility: You can easily control how different types of data are displayed or used in templates.\n\n## [Structured Outputs for Gemini now supported](https://python.useinstructor.com/blog/2024/09/03/structured-outputs-for-gemini-now-supported/)\n\nWe're excited to announce that `instructor` now supports structured outputs using tool calling for both the Gemini SDK and the VertexAI SDK.\n\nA special shoutout to [Sonal](https://x.com/sonalsaldanha) for his contributions to the Gemini Tool Calling support.\n\nLet's walk through a simple example of how to use these new features\n\n### [Installation](https://python.useinstructor.com/blog/2024/09/03/structured-outputs-for-gemini-now-supported/\\#installation)\n\nTo get started, install the latest version of `instructor`. Depending on whether you're using Gemini or VertexAI, you should install the following:\n\n[Gemini](#__tabbed_1_1)[VertexAI](#__tabbed_1_2)\n\n```md-code__content\npip install \"instructor[google-generativeai]\"\n\n```\n\n```md-code__content\npip install \"instructor[vertexai]\"\n\n```\n\nThis ensures that you have the necessary dependencies to use the Gemini or VertexAI SDKs with instructor.\n\nWe recommend using the Gemini SDK over the VertexAI SDK for two main reasons.\n\n1. Compared to the VertexAI SDK, the Gemini SDK comes with a free daily quota of 1.5 billion tokens to use for developers.\n2. The Gemini SDK is significantly easier to setup, all you need is a `GOOGLE_API_KEY` that you can generate in your GCP console. THe VertexAI SDK on the other hand requires a credentials.json file or an OAuth integration to use.\n\n### [Getting Started](https://python.useinstructor.com/blog/2024/09/03/structured-outputs-for-gemini-now-supported/\\#getting-started)\n\nWith our provider agnostic API, you can use the same interface to interact with both SDKs, the only thing that changes here is how we initialise the client itself.\n\nBefore running the following code, you'll need to make sure that you have your Gemini API Key set in your shell under the alias `GOOGLE_API_KEY`.\n\n```md-code__content\nimport instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    )\n)\n\nresp = client.chat.completions.create(\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Extract Jason is 25 years old.\",\\\n        }\\\n    ],\n    response_model=User,\n)\n\nprint(resp)\n#> name='Jason' age=25\n\n```\n\nWe can achieve a similar thing with the VertexAI SDK. For this to work, you'll need to authenticate to VertexAI.\n\nThere are some instructions [here](https://cloud.google.com/vertex-ai/docs/authentication) but the easiest way I found was to simply download the GCloud cli and run `gcloud auth application-default login`.\n\n```md-code__content\nimport instructor\nimport vertexai  # type: ignore\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nfrom pydantic import BaseModel\n\nvertexai.init()\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-1.5-pro-preview-0409\"),\n)\n\nresp = client.chat.completions.create(\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Extract Jason is 25 years old.\",\\\n        }\\\n    ],\n    response_model=User,\n)\n\nprint(resp)\n#> name='Jason' age=25\n\n```\n\n## [Why Instructor is the best way to get JSON from LLMs](https://python.useinstructor.com/blog/2024/06/15/zero-cost-abstractions/)\n\nLarge Language Models (LLMs) like GPT are incredibly powerful, but getting them to return well-formatted JSON can be challenging. This is where the Instructor library shines. Instructor allows you to easily map LLM outputs to JSON data using Python type annotations and Pydantic models.\n\nInstructor makes it easy to get structured data like JSON from LLMs like GPT-3.5, GPT-4, GPT-4-Vision, and open-source models including [Mistral/Mixtral](https://python.useinstructor.com/integrations/together/), [Ollama](https://python.useinstructor.com/integrations/ollama/), and [llama-cpp-python](https://python.useinstructor.com/integrations/llama-cpp-python/).\n\nIt stands out for its simplicity, transparency, and user-centric design, built on top of Pydantic. Instructor helps you manage [validation context](https://python.useinstructor.com/concepts/reask_validation/), retries with [Tenacity](https://python.useinstructor.com/concepts/retrying/), and streaming [Lists](https://python.useinstructor.com/concepts/lists/) and [Partial](https://python.useinstructor.com/concepts/partial/) responses.\n\n- Instructor provides support for a wide range of programming languages, including:\n- [Python](https://python.useinstructor.com)\n- [TypeScript](https://js.useinstructor.com)\n- [Ruby](https://ruby.useinstructor.com)\n- [Go](https://go.useinstructor.com)\n- [Elixir](https://hex.pm/packages/instructor)\n\n## [Enhancing RAG with Time Filters Using Instructor](https://python.useinstructor.com/blog/2024/06/06/enhancing-rag-with-time-filters-using-instructor/)\n\nRetrieval-augmented generation (RAG) systems often need to handle queries with time-based constraints, like \"What new features were released last quarter?\" or \"Show me support tickets from the past week.\" Effective time filtering is crucial for providing accurate, relevant responses.\n\nInstructor is a Python library that simplifies integrating large language models (LLMs) with data sources and APIs. It allows defining structured output models using Pydantic, which can be used as prompts or to parse LLM outputs.\n\n## [Why Instructor is the Best Library for Structured LLM Outputs](https://python.useinstructor.com/blog/2024/03/05/zero-cost-abstractions/)\n\nLarge language models (LLMs) like GPTs are incredibly powerful, but working with their open-ended text outputs can be challenging. This is where the Instructor library shines - it allows you to easily map LLM outputs to structured data using Python type annotations.\n\n## [Seamless Support with Langsmith](https://python.useinstructor.com/blog/2024/02/18/seamless-support-with-langsmith/)\n\nIts a common misconception that LangChain's [LangSmith](https://www.langchain.com/langsmith) is only compatible with LangChain's models. In reality, LangSmith is a unified DevOps platform for developing, collaborating, testing, deploying, and monitoring LLM applications. In this blog we will explore how LangSmith can be used to enhance the OpenAI client alongside `instructor`.\n\n## [Generators and LLM Streaming](https://python.useinstructor.com/blog/2023/11/26/python-generators-and-llm-streaming/)\n\nLatency is crucial, especially in eCommerce and newer chat applications like ChatGPT. Streaming is the solution that enables us to enhance the user experience without the need for faster response times.\n\nAnd what makes streaming possible? Generators!\n\n## [Async Processing OpenAI using `asyncio` and `Instructor` with Python](https://python.useinstructor.com/blog/2023/11/13/learn-async/)\n\nToday, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using `instructor` and learn how to use `asyncio.gather` and `asyncio.as_completed` for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using `asyncio.Semaphore`.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/llm-techniques/",
      "ogUrl": "https://python.useinstructor.com/blog/category/llm-techniques/",
      "title": "LLM Techniques - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/llm-techniques/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/llm-techniques.png",
      "ogTitle": "LLM Techniques - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/llm-techniques.png",
      "og:title": "LLM Techniques - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/llm-techniques/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/llm-techniques.png",
      "twitter:title": "LLM Techniques - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/google/#google)\n\n## [Do I Still Need Instructor with Google's New OpenAI Integration?](https://python.useinstructor.com/blog/2024/11/10/do-i-still-need-instructor-with-googles-new-openai-integration/)\n\nGoogle recently launched OpenAI client compatibility for Gemini.\n\nWhile this is a significant step forward for developers by simplifying Gemini model interactions, **you absolutely still need instructor**.\n\nIf you're unfamiliar with instructor, we provide a simple interface to get structured outputs from LLMs across different providers.\n\nThis makes it easy to switch between providers, get reliable outputs from language models and ultimately build production grade LLM applications.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/google/",
      "ogUrl": "https://python.useinstructor.com/blog/category/google/",
      "title": "Google - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/google/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/google.png",
      "ogTitle": "Google - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/google.png",
      "og:title": "Google - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/google/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/google.png",
      "twitter:title": "Google - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/llm-observability/#llm-observability)\n\n## [Parea for Observing, Testing & Fine-tuning of Instructor](https://python.useinstructor.com/blog/2024/07/17/parea-for-observing-testing--fine-tuning-of-instructor/)\n\n[Parea](https://www.parea.ai) is a platform that enables teams to monitor, collaborate, test & label for LLM applications. In this blog we will explore how Parea can be used to enhance the OpenAI client alongside `instructor` and debug + improve `instructor` calls. Parea has some features which makes it particularly useful for `instructor`:\n\n- it automatically groups any LLM calls due to reties under a single trace\n- it automatically tracks any validation error counts & fields that occur when using `instructor`\n- it provides a UI to label JSON responses by filling out a form instead of editing JSON objects\n\nConfigure Parea\n\nBefore starting this tutorial, make sure that you've registered for a [Parea](https://www.parea.ai) account. You'll also need to create an [API key](https://docs.parea.ai/api-reference/authentication).\n\n### [Example: Writing Emails with URLs from Instructor Docs](https://python.useinstructor.com/blog/2024/07/17/parea-for-observing-testing--fine-tuning-of-instructor/\\#example-writing-emails-with-urls-from-instructor-docs)\n\nWe will demonstrate Parea by using `instructor` to write emails which only contain URLs from the `instructor` docs. We'll need to install our dependencies before proceeding so simply run the command below.\n\n## [Why Logfire is a perfect fit for FastAPI + Instructor](https://python.useinstructor.com/blog/2024/05/03/fastapi-open-telemetry-and-instructor/)\n\nLogfire is a new tool that provides key insight into your application with Open Telemtry. Instead of using ad-hoc print statements, Logfire helps to profile every part of your application and is integrated directly into Pydantic and FastAPI, two popular libraries amongst Instructor users.\n\nIn short, this is the secret sauce to help you get your application to the finish line and beyond. We'll show you how to easily integrate Logfire into FastAPI, one of the most popular choices amongst users of Instructor using two examples\n\n1. Data Extraction from a single User Query\n2. Using `asyncio` to process multiple users in parallel\n3. Streaming multiple objects using an `Iterable` so that they're avaliable on demand\n\n## [Logfire](https://python.useinstructor.com/blog/2024/05/01/instructor-logfire/)\n\n### [Introduction](https://python.useinstructor.com/blog/2024/05/01/instructor-logfire/\\#introduction)\n\nLogfire is a new observability platform coming from the creators of Pydantic. It integrates almost seamlessly with many of your favourite libraries such as Pydantic, HTTPx and Instructor. In this article, we'll show you how to use Logfire with Instructor to gain visibility into the performance of your entire application.\n\nWe'll walk through the following examples\n\n1. Classifying scam emails using Instructor\n2. Performing simple validation using the `llm_validator`\n3. Extracting data into a markdown table from an infographic with GPT4V\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/llm-observability/",
      "ogUrl": "https://python.useinstructor.com/blog/category/llm-observability/",
      "title": "LLM Observability - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/llm-observability/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/llm-observability.png",
      "ogTitle": "LLM Observability - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/llm-observability.png",
      "og:title": "LLM Observability - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/llm-observability/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/llm-observability.png",
      "twitter:title": "LLM Observability - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/data-analysis/#data-analysis)\n\n## [Using Structured Outputs to convert messy tables into tidy data](https://python.useinstructor.com/blog/2024/11/21/using-structured-outputs-to-convert-messy-tables-into-tidy-data/)\n\n### [Why is this a problem?](https://python.useinstructor.com/blog/2024/11/21/using-structured-outputs-to-convert-messy-tables-into-tidy-data/\\#why-is-this-a-problem)\n\nMessy data exports are a common problem. Whether it's multiple headers in the table, implicit relationships that make analysis a pain or even just merged cells, using `instructor` with structured outputs makes it easy to convert messy tables into tidy data, even if all you have is just an image of the table as we'll see below.\n\nLet's look at the following table as an example. It makes analysis unnecessarily difficult because it hides data relationships through empty cells and implicit repetition. If we were using it for data analysis, cleaning it manually would be a huge nightmare.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/data-analysis/",
      "ogUrl": "https://python.useinstructor.com/blog/category/data-analysis/",
      "title": "Data Analysis - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/data-analysis/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/data-analysis.png",
      "ogTitle": "Data Analysis - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/data-analysis.png",
      "og:title": "Data Analysis - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/data-analysis/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/data-analysis.png",
      "twitter:title": "Data Analysis - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/data-validation/#data-validation)\n\n## [Good LLM Validation is Just Good Validation](https://python.useinstructor.com/blog/2023/10/23/good-llm-validation-is-just-good-validation/)\n\n> What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here.\n\nValidation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like `Pydantic` and `Instructor`. We validate these outputs using a validation function which conforms to the structure seen below.\n\n```md-code__content\ndef validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n\n```\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/data-validation/",
      "ogUrl": "https://python.useinstructor.com/blog/category/data-validation/",
      "title": "Data Validation - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/data-validation/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/data-validation.png",
      "ogTitle": "Data Validation - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/data-validation.png",
      "og:title": "Data Validation - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/data-validation/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/data-validation.png",
      "twitter:title": "Data Validation - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/category/llm/#llm)\n\n## [Building an LLM-based Reranker for your RAG pipeline](https://python.useinstructor.com/blog/2024/10/23/building-an-llm-based-reranker-for-your-rag-pipeline/)\n\nAre you struggling with irrelevant search results in your Retrieval-Augmented Generation (RAG) pipeline?\n\nImagine having a powerful tool that can intelligently reassess and reorder your search results, significantly improving their relevance to user queries.\n\nIn this blog post, we'll show you how to create an LLM-based reranker using Instructor and Pydantic. This approach will:\n\n- Enhance the accuracy of your search results\n- Leverage the power of large language models (LLMs)\n- Utilize structured outputs for precise information retrieval\n\nBy the end of this tutorial, you'll be able to implement a llm reranker to label your synthetic data for fine-tuning a traditional reranker, or to build out an evaluation pipeline for your RAG system. Let's dive in!\n\n## [Building a Pairwise LLM Judge with Instructor and Pydantic](https://python.useinstructor.com/blog/2024/10/17/building-a-pairwise-llm-judge-with-instructor-and-pydantic/)\n\nIn this blog post, we'll explore how to create a pairwise LLM judge using Instructor and Pydantic. This judge will evaluate the relevance between a question and a piece of text, demonstrating a practical application of structured outputs in language model interactions.\n\n### [Introduction](https://python.useinstructor.com/blog/2024/10/17/building-a-pairwise-llm-judge-with-instructor-and-pydantic/\\#introduction)\n\nEvaluating text relevance is a common task in natural language processing and information retrieval. By leveraging large language models (LLMs) and structured outputs, we can create a system that judges the similarity or relevance between a question and a given text.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/category/llm/",
      "ogUrl": "https://python.useinstructor.com/blog/category/llm/",
      "title": "LLM - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/category/llm/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/category/llm.png",
      "ogTitle": "LLM - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/category/llm.png",
      "og:title": "LLM - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/category/llm/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/category/llm.png",
      "twitter:title": "LLM - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/page/5/#subscribe-to-our-newsletter-for-updates-and-tips)\n\n## [AI Engineer Keynote: Pydantic is all you need](https://python.useinstructor.com/blog/2023/11/02/ai-engineer-keynote-pydantic-is-all-you-need/)\n\n[![Pydantic is all you need](https://img.youtube.com/vi/yj-wSRJwrrc/0.jpg)](https://www.youtube.com/watch?v=yj-wSRJwrrc)\n\n[Click here to watch the full talk](https://www.youtube.com/watch?v=yj-wSRJwrrc)\n\n## [Good LLM Validation is Just Good Validation](https://python.useinstructor.com/blog/2023/10/23/good-llm-validation-is-just-good-validation/)\n\n> What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here.\n\nValidation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like `Pydantic` and `Instructor`. We validate these outputs using a validation function which conforms to the structure seen below.\n\n```md-code__content\ndef validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n\n```\n\n## [Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation](https://python.useinstructor.com/blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/)\n\n### [Introduction](https://python.useinstructor.com/blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/\\#introduction)\n\nGet ready to dive deep into the world of fine-tuning task specific language models with Python functions. We'll explore how the `instructor.instructions` streamlines this process, making the task you want to distil more efficient and powerful while preserving its original functionality and backwards compatibility.\n\nIf you want to see the full example checkout [examples/distillation](https://github.com/jxnl/instructor/tree/main/examples/distilations)\n\n## [RAG is more than just embedding search](https://python.useinstructor.com/blog/2023/09/17/rag-is-more-than-just-embedding-search/)\n\nWith the advent of large language models (LLM), retrieval augmented generation (RAG) has become a hot topic. However throughout the past year of [helping startups](https://jxnl.co) integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware.\n\nWhat is RAG?\n\nRetrieval augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized.\n\n![RAG](https://python.useinstructor.com/blog/img/dumb_rag.png)\n\nSimple RAG that embedded the user query and makes a search.\n\nSo let's kick things off by examining what I like to call the 'Dumb' RAG Model—a basic setup that's more common than you'd think.\n\n## [Generating Structured Output / JSON from LLMs](https://python.useinstructor.com/blog/2023/09/11/generating-structured-output--json-from-llms/)\n\nLanguage models have seen significant growth. Using them effectively often requires complex frameworks. This post discusses how Instructor simplifies this process using Pydantic.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/page/5/",
      "ogUrl": "https://python.useinstructor.com/blog/page/5/",
      "title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/page/5/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/page/5/index.png",
      "ogTitle": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/page/5/index.png",
      "og:title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/page/5/",
      "statusCode": 200,
      "description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/page/5/index.png",
      "twitter:title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "og:description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/page/3/#subscribe-to-our-newsletter-for-updates-and-tips)\n\n## [Should I Be Using Structured Outputs?](https://python.useinstructor.com/blog/2024/08/20/should-i-be-using-structured-outputs/)\n\nOpenAI recently announced Structured Outputs which ensures that generated responses match any arbitrary provided JSON Schema. In their [announcement article](https://openai.com/index/introducing-structured-outputs-in-the-api/), they acknowledged that it had been inspired by libraries such as `instructor`.\n\n### [Main Challenges](https://python.useinstructor.com/blog/2024/08/20/should-i-be-using-structured-outputs/\\#main-challenges)\n\nIf you're building complex LLM workflows, you've likely considered OpenAI's Structured Outputs as a potential replacement for `instructor`.\n\nBut before you do so, three key challenges remain:\n\n1. **Limited Validation And Retry Logic**: Structured Outputs ensure adherence to the schema but not useful content. You might get perfectly formatted yet unhelpful responses\n2. **Streaming Challenges**: Parsing raw JSON objects from streamed responses with the sdk is error-prone and inefficient\n3. **Unpredictable Latency Issues** : Structured Outputs suffers from random latency spikes that might result in an almost 20x increase in response time\n\nAdditionally, adopting Structured Outputs locks you into OpenAI's ecosystem, limiting your ability to experiment with diverse models or providers that might better suit specific use-cases.\n\nThis vendor lock-in increases vulnerability to provider outages, potentially causing application downtime and SLA violations, which can damage user trust and impact your business reputation.\n\nIn this article, we'll show how `instructor` addresses many of these challenges with features such as automatic reasking when validation fails, automatic support for validated streaming data and more.\n\n## [Parea for Observing, Testing & Fine-tuning of Instructor](https://python.useinstructor.com/blog/2024/07/17/parea-for-observing-testing--fine-tuning-of-instructor/)\n\n[Parea](https://www.parea.ai) is a platform that enables teams to monitor, collaborate, test & label for LLM applications. In this blog we will explore how Parea can be used to enhance the OpenAI client alongside `instructor` and debug + improve `instructor` calls. Parea has some features which makes it particularly useful for `instructor`:\n\n- it automatically groups any LLM calls due to reties under a single trace\n- it automatically tracks any validation error counts & fields that occur when using `instructor`\n- it provides a UI to label JSON responses by filling out a form instead of editing JSON objects\n\nConfigure Parea\n\nBefore starting this tutorial, make sure that you've registered for a [Parea](https://www.parea.ai) account. You'll also need to create an [API key](https://docs.parea.ai/api-reference/authentication).\n\n### [Example: Writing Emails with URLs from Instructor Docs](https://python.useinstructor.com/blog/2024/07/17/parea-for-observing-testing--fine-tuning-of-instructor/\\#example-writing-emails-with-urls-from-instructor-docs)\n\nWe will demonstrate Parea by using `instructor` to write emails which only contain URLs from the `instructor` docs. We'll need to install our dependencies before proceeding so simply run the command below.\n\n## [Analyzing Youtube Transcripts with Instructor](https://python.useinstructor.com/blog/2024/07/11/youtube-transcripts/)\n\n### [Extracting Chapter Information](https://python.useinstructor.com/blog/2024/07/11/youtube-transcripts/\\#extracting-chapter-information)\n\nCode Snippets\n\nAs always, the code is readily available in our `examples/youtube` folder in our repo for your reference in the `run.py` file.\n\nIn this post, we'll show you how to summarise Youtube video transcripts into distinct chapters using `instructor` before exploring some ways you can adapt the code to different applications.\n\nBy the end of this article, you'll be able to build an application as per the video below.\n\n![](https://python.useinstructor.com/img/youtube.gif)\n\n## [Why Instructor is the best way to get JSON from LLMs](https://python.useinstructor.com/blog/2024/06/15/zero-cost-abstractions/)\n\nLarge Language Models (LLMs) like GPT are incredibly powerful, but getting them to return well-formatted JSON can be challenging. This is where the Instructor library shines. Instructor allows you to easily map LLM outputs to JSON data using Python type annotations and Pydantic models.\n\nInstructor makes it easy to get structured data like JSON from LLMs like GPT-3.5, GPT-4, GPT-4-Vision, and open-source models including [Mistral/Mixtral](https://python.useinstructor.com/integrations/together/), [Ollama](https://python.useinstructor.com/integrations/ollama/), and [llama-cpp-python](https://python.useinstructor.com/integrations/llama-cpp-python/).\n\nIt stands out for its simplicity, transparency, and user-centric design, built on top of Pydantic. Instructor helps you manage [validation context](https://python.useinstructor.com/concepts/reask_validation/), retries with [Tenacity](https://python.useinstructor.com/concepts/retrying/), and streaming [Lists](https://python.useinstructor.com/concepts/lists/) and [Partial](https://python.useinstructor.com/concepts/partial/) responses.\n\n- Instructor provides support for a wide range of programming languages, including:\n- [Python](https://python.useinstructor.com)\n- [TypeScript](https://js.useinstructor.com)\n- [Ruby](https://ruby.useinstructor.com)\n- [Go](https://go.useinstructor.com)\n- [Elixir](https://hex.pm/packages/instructor)\n\n## [Enhancing RAG with Time Filters Using Instructor](https://python.useinstructor.com/blog/2024/06/06/enhancing-rag-with-time-filters-using-instructor/)\n\nRetrieval-augmented generation (RAG) systems often need to handle queries with time-based constraints, like \"What new features were released last quarter?\" or \"Show me support tickets from the past week.\" Effective time filtering is crucial for providing accurate, relevant responses.\n\nInstructor is a Python library that simplifies integrating large language models (LLMs) with data sources and APIs. It allows defining structured output models using Pydantic, which can be used as prompts or to parse LLM outputs.\n\n## [Why Logfire is a perfect fit for FastAPI + Instructor](https://python.useinstructor.com/blog/2024/05/03/fastapi-open-telemetry-and-instructor/)\n\nLogfire is a new tool that provides key insight into your application with Open Telemtry. Instead of using ad-hoc print statements, Logfire helps to profile every part of your application and is integrated directly into Pydantic and FastAPI, two popular libraries amongst Instructor users.\n\nIn short, this is the secret sauce to help you get your application to the finish line and beyond. We'll show you how to easily integrate Logfire into FastAPI, one of the most popular choices amongst users of Instructor using two examples\n\n1. Data Extraction from a single User Query\n2. Using `asyncio` to process multiple users in parallel\n3. Streaming multiple objects using an `Iterable` so that they're avaliable on demand\n\n## [Logfire](https://python.useinstructor.com/blog/2024/05/01/instructor-logfire/)\n\n### [Introduction](https://python.useinstructor.com/blog/2024/05/01/instructor-logfire/\\#introduction)\n\nLogfire is a new observability platform coming from the creators of Pydantic. It integrates almost seamlessly with many of your favourite libraries such as Pydantic, HTTPx and Instructor. In this article, we'll show you how to use Logfire with Instructor to gain visibility into the performance of your entire application.\n\nWe'll walk through the following examples\n\n1. Classifying scam emails using Instructor\n2. Performing simple validation using the `llm_validator`\n3. Extracting data into a markdown table from an infographic with GPT4V\n\n## [Announcing instructor=1.0.0](https://python.useinstructor.com/blog/2024/04/01/announce-instructor-v1/)\n\nOver the past 10 months, we've build up instructor with the [principle](https://python.useinstructor.com/why/) of 'easy to try, and easy to delete'. We accomplished this by patching the openai client with the `instructor` package and adding new arguments like `response_model`, `max_retries`, and `validation_context`. As a result I truely believe isntructor is the [best way](https://python.useinstructor.com/blog/2024/03/05/zero-cost-abstractions/) to get structured data out of llm apis.\n\nBut as a result, we've been a bit stuck on getting typing to work well while giving you more control at development time. I'm excited to launch version 1.0.0 which cleans up the api w.r.t. typing without compromising the ease of use.\n\n## [Matching Language in Multilingual Summarization Tasks](https://python.useinstructor.com/blog/2024/03/28/matching-language-summaries/)\n\nWhen asking language models to summarize text, there's a risk that the generated summary ends up in English, even if the source text is in another language. This is likely due to the instructions being provided in English, biasing the model towards English output.\n\nIn this post, we explore techniques to ensure the language of the generated summary matches the language of the source text. We leverage Pydantic for data validation and the `langdetect` library for language identification.\n\n## [Structured Outputs with Anthropic](https://python.useinstructor.com/blog/2024/03/20/structured-outputs-with-anthropic/)\n\nA special shoutout to [Shreya](https://twitter.com/shreyaw_) for her contributions to the anthropic support. As of now, all features are operational with the exception of streaming support.\n\nFor those eager to experiment, simply patch the client with `ANTHROPIC_JSON`, which will enable you to leverage the `anthropic` client for making requests.\n\n```\npip install instructor[anthropic]\n\n```\n\nMissing Features\n\nJust want to acknowledge that we know that we are missing partial streaming and some better re-asking support for XML. We are working on it and will have it soon.\n\n```\nfrom pydantic import BaseModel\nfrom typing import List\nimport anthropic\nimport instructor\n\n# Patching the Anthropics client with the instructor for enhanced capabilities\nanthropic_client = instructor.from_openai(\n    create=anthropic.Anthropic().messages.create,\n    mode=instructor.Mode.ANTHROPIC_JSON\n)\n\nclass Properties(BaseModel):\n    name: str\n    value: str\n\nclass User(BaseModel):\n    name: str\n    age: int\n    properties: List[Properties]\n\nuser_response = anthropic_client(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=1024,\n    max_retries=0,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Create a user for a model with a name, age, and properties.\",\\\n        }\\\n    ],\n    response_model=User,\n)  # type: ignore\n\nprint(user_response.model_dump_json(indent=2))\n\"\"\"\n{\n    \"name\": \"John\",\n    \"age\": 25,\n    \"properties\": [\\\n        {\\\n            \"key\": \"favorite_color\",\\\n            \"value\": \"blue\"\\\n        }\\\n    ]\n}\n\n```\n\nWe're encountering challenges with deeply nested types and eagerly invite the community to test, provide feedback, and suggest necessary improvements as we enhance the anthropic client's support.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/page/3/",
      "ogUrl": "https://python.useinstructor.com/blog/page/3/",
      "title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/page/3/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/page/3/index.png",
      "ogTitle": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/page/3/index.png",
      "og:title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/page/3/",
      "statusCode": 200,
      "description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/page/3/index.png",
      "twitter:title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "og:description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/archive/2024/#2024)\n\n## [Using Structured Outputs to convert messy tables into tidy data](https://python.useinstructor.com/blog/2024/11/21/using-structured-outputs-to-convert-messy-tables-into-tidy-data/)\n\n### [Why is this a problem?](https://python.useinstructor.com/blog/2024/11/21/using-structured-outputs-to-convert-messy-tables-into-tidy-data/\\#why-is-this-a-problem)\n\nMessy data exports are a common problem. Whether it's multiple headers in the table, implicit relationships that make analysis a pain or even just merged cells, using `instructor` with structured outputs makes it easy to convert messy tables into tidy data, even if all you have is just an image of the table as we'll see below.\n\nLet's look at the following table as an example. It makes analysis unnecessarily difficult because it hides data relationships through empty cells and implicit repetition. If we were using it for data analysis, cleaning it manually would be a huge nightmare.\n\n## [Structured Outputs with Writer now supported](https://python.useinstructor.com/blog/2024/11/19/writer-support/)\n\nWe're excited to announce that `instructor` now supports [Writer](https://writer.com)'s enterprise-grade LLMs, including their latest Palmyra X 004 model. This integration enables structured outputs and enterprise AI workflows with Writer's powerful language models.\n\n### [Getting Started](https://python.useinstructor.com/blog/2024/11/19/writer-support/\\#getting-started)\n\nFirst, make sure that you've signed up for an account on [Writer](https://app.writer.com/aistudio/signup?utm_campaign=devrel) and obtained an API key using this [quickstart guide](https://dev.writer.com/api-guides/quickstart). Once you've done so, install `instructor` with Writer support by running `pip install instructor[writer]` in your terminal.\n\nMake sure to set the `WRITER_API_KEY` environment variable with your Writer API key or pass it as an argument to the `Writer` constructor.\n\n## [Eliminating Hallucinations with Structured Outputs using Gemini](https://python.useinstructor.com/blog/2024/11/15/eliminating-hallucinations-with-structured-outputs-using-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to generate accurate citations from PDFs. This approach ensures that answers are grounded in the actual content of the PDF, reducing the risk of hallucinations.\n\nWe'll be using the Nvidia 10k report for this example which you can download at this [link](https://d18rn0p25nwr6d.cloudfront.net/CIK-0001045810/78501ce3-7816-4c4d-8688-53dd140df456.pdf).\n\n## [PDF Processing with Structured Outputs with Gemini](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to analyse the [Gemini 1.5 Pro Paper](https://github.com/google-gemini/generative-ai-python/blob/0e5c5f25fe4ce266791fa2afb20d17dee780ca9e/third_party/test.pdf) and extract a structured summary.\n\n### [The Problem](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#the-problem)\n\nProcessing PDFs programmatically has always been painful. The typical approaches all have significant drawbacks:\n\n- **PDF parsing libraries** require complex rules and break easily\n- **OCR solutions** are slow and error-prone\n- **Specialized PDF APIs** are expensive and require additional integration\n- **LLM solutions** often need complex document chunking and embedding pipelines\n\nWhat if we could just hand a PDF to an LLM and get structured data back? With Gemini's multimodal capabilities and Instructor's structured output handling, we can do exactly that.\n\n### [Quick Setup](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#quick-setup)\n\nFirst, install the required packages:\n\n```\npip install \"instructor[google-generativeai]\"\n\n```\n\nThen, here's all the code you need:\n\n```\nimport instructor\nimport google.generativeai as genai\nfrom google.ai.generativelanguage_v1beta.types.file import File\nfrom pydantic import BaseModel\nimport time\n\n# Initialize the client\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    )\n)\n\n# Define your output structure\nclass Summary(BaseModel):\n    summary: str\n\n# Upload the PDF\nfile = genai.upload_file(\"path/to/your.pdf\")\n\n# Wait for file to finish processing\nwhile file.state != File.State.ACTIVE:\n    time.sleep(1)\n    file = genai.get_file(file.name)\n    print(f\"File is still uploading, state: {file.state}\")\n\nprint(f\"File is now active, state: {file.state}\")\nprint(file)\n\nresp = client.chat.completions.create(\n    messages=[\\\n        {\"role\": \"user\", \"content\": [\"Summarize the following file\", file]},\\\n    ],\n    response_model=Summary,\n)\n\nprint(resp.summary)\n\n```\n\nExpand to see Raw Results\n\n```\nsummary=\"Gemini 1.5 Pro is a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. It achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Gemini 1.5 Pro is built to handle extremely long contexts; it has the ability to recall and reason over fine-grained information from up to at least 10M tokens. This scale is unprecedented among contemporary large language models (LLMs), and enables the processing of long-form mixed-modality inputs including entire collections of documents, multiple hours of video, and almost five days long of audio. Gemini 1.5 Pro surpasses Gemini 1.0 Pro and performs at a similar level to 1.0 Ultra on a wide array of benchmarks while requiring significantly less compute to train. It can recall information amidst distractor context, and it can learn to translate a new language from a single set of linguistic documentation. With only instructional materials (a 500-page reference grammar, a dictionary, and ≈ 400 extra parallel sentences) all provided in context, Gemini 1.5 Pro is capable of learning to translate from English to Kalamang, a Papuan language with fewer than 200 speakers, and therefore almost no online presence.\"\n\n```\n\n### [Benefits](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#benefits)\n\nThe combination of Gemini and Instructor offers several key advantages over traditional PDF processing approaches:\n\n**Simple Integration** \\- Unlike traditional approaches that require complex document processing pipelines, chunking strategies, and embedding databases, you can directly process PDFs with just a few lines of code. This dramatically reduces development time and maintenance overhead.\n\n**Structured Output** \\- Instructor's Pydantic integration ensures you get exactly the data structure you need. The model's outputs are automatically validated and typed, making it easier to build reliable applications. If the extraction fails, Instructor automatically handles the retries for you with support for [custom retry logic using tenacity](https://python.useinstructor.com/concepts/retrying/).\n\n**Multimodal Support** \\- Gemini's multimodal capabilities mean this same approach works for various file types. You can process images, videos, and audio files all in the same api request. Check out our [multimodal processing guide](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/) to see how we extract structured data from travel videos.\n\n### [Conclusion](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#conclusion)\n\nWorking with PDFs doesn't have to be complicated.\n\nBy combining Gemini's multimodal capabilities with Instructor's structured output handling, we can transform complex document processing into simple, Pythonic code.\n\nNo more wrestling with parsing rules, managing embeddings, or building complex pipelines – just define your data model and let the LLM do the heavy lifting.\n\nIf you liked this, give `instructor` a try today and see how much easier structured outputs makes working with LLMs become. [Get started with Instructor today!](https://python.useinstructor.com/)\n\n## [Do I Still Need Instructor with Google's New OpenAI Integration?](https://python.useinstructor.com/blog/2024/11/10/do-i-still-need-instructor-with-googles-new-openai-integration/)\n\nGoogle recently launched OpenAI client compatibility for Gemini.\n\nWhile this is a significant step forward for developers by simplifying Gemini model interactions, **you absolutely still need instructor**.\n\nIf you're unfamiliar with instructor, we provide a simple interface to get structured outputs from LLMs across different providers.\n\nThis makes it easy to switch between providers, get reliable outputs from language models and ultimately build production grade LLM applications.\n\n## [Building an LLM-based Reranker for your RAG pipeline](https://python.useinstructor.com/blog/2024/10/23/building-an-llm-based-reranker-for-your-rag-pipeline/)\n\nAre you struggling with irrelevant search results in your Retrieval-Augmented Generation (RAG) pipeline?\n\nImagine having a powerful tool that can intelligently reassess and reorder your search results, significantly improving their relevance to user queries.\n\nIn this blog post, we'll show you how to create an LLM-based reranker using Instructor and Pydantic. This approach will:\n\n- Enhance the accuracy of your search results\n- Leverage the power of large language models (LLMs)\n- Utilize structured outputs for precise information retrieval\n\nBy the end of this tutorial, you'll be able to implement a llm reranker to label your synthetic data for fine-tuning a traditional reranker, or to build out an evaluation pipeline for your RAG system. Let's dive in!\n\n## [Structured Outputs with Multimodal Gemini](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to analyze [travel videos](https://www.youtube.com/watch?v=_R8yhW_H9NQ) and extract structured recommendations. This powerful combination allows us to process multimodal inputs (video) and generate structured outputs using Pydantic models. This post was done in collaboration with [Kino.ai](https://kino.ai), a company that uses instructor to do structured extraction from multimodal inputs to improve search for film makers.\n\n### [Setting Up the Environment](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/\\#setting-up-the-environment)\n\nFirst, let's set up our environment with the necessary libraries:\n\n```\n\n```\n\n## [Structured Outputs and Prompt Caching with Anthropic](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-and-prompt-caching-with-anthropic/)\n\nAnthropic's ecosystem now offers two powerful features for AI developers: structured outputs and prompt caching. These advancements enable more efficient use of large language models (LLMs). This guide demonstrates how to leverage these features with the Instructor library to enhance your AI applications.\n\n### [Structured Outputs with Anthropic and Instructor](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-and-prompt-caching-with-anthropic/\\#structured-outputs-with-anthropic-and-instructor)\n\nInstructor now offers seamless integration with Anthropic's powerful language models, allowing developers to easily create structured outputs using Pydantic models. This integration simplifies the process of extracting specific information from AI-generated responses.\n\n## [Flashcard generator with Instructor + Burr](https://python.useinstructor.com/blog/2024/10/18/youtube-flashcards/)\n\nFlashcards help break down complex topics and learn anything from biology to a new language or lines for a play. This blog will show how to use LLMs to generate flashcards and kickstart your learning!\n\n**Instructor** lets us get structured outputs from LLMs reliably, and [Burr](https://github.com/dagworks-inc/burr) helps create an LLM application that's easy to understand and debug. It comes with **Burr UI**, a free, open-source, and local-first tool for observability, annotations, and more!\n\n## [Audio Support in OpenAI's Chat Completions API](https://python.useinstructor.com/blog/2024/10/17/audio-support-in-openais-chat-completions-api/)\n\nOpenAI has recently introduced audio support in their Chat Completions API, opening up exciting new possibilities for developers working with audio and text interactions. This feature is powered by the new `gpt-4o-audio-preview` model, which brings advanced voice capabilities to the familiar Chat Completions API interface.\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/archive/2024/",
      "ogUrl": "https://python.useinstructor.com/blog/archive/2024/",
      "title": "2024 - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/archive/2024/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/archive/2024.png",
      "ogTitle": "2024 - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/archive/2024.png",
      "og:title": "2024 - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/archive/2024/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/archive/2024.png",
      "twitter:title": "2024 - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/blog/?q=#subscribe-to-our-newsletter-for-updates-and-tips)\n\n## [Using Structured Outputs to convert messy tables into tidy data](https://python.useinstructor.com/blog/2024/11/21/using-structured-outputs-to-convert-messy-tables-into-tidy-data/)\n\n### [Why is this a problem?](https://python.useinstructor.com/blog/2024/11/21/using-structured-outputs-to-convert-messy-tables-into-tidy-data/\\#why-is-this-a-problem)\n\nMessy data exports are a common problem. Whether it's multiple headers in the table, implicit relationships that make analysis a pain or even just merged cells, using `instructor` with structured outputs makes it easy to convert messy tables into tidy data, even if all you have is just an image of the table as we'll see below.\n\nLet's look at the following table as an example. It makes analysis unnecessarily difficult because it hides data relationships through empty cells and implicit repetition. If we were using it for data analysis, cleaning it manually would be a huge nightmare.\n\n## [Structured Outputs with Writer now supported](https://python.useinstructor.com/blog/2024/11/19/writer-support/)\n\nWe're excited to announce that `instructor` now supports [Writer](https://writer.com/)'s enterprise-grade LLMs, including their latest Palmyra X 004 model. This integration enables structured outputs and enterprise AI workflows with Writer's powerful language models.\n\n### [Getting Started](https://python.useinstructor.com/blog/2024/11/19/writer-support/\\#getting-started)\n\nFirst, make sure that you've signed up for an account on [Writer](https://app.writer.com/aistudio/signup?utm_campaign=devrel) and obtained an API key using this [quickstart guide](https://dev.writer.com/api-guides/quickstart). Once you've done so, install `instructor` with Writer support by running `pip install instructor[writer]` in your terminal.\n\nMake sure to set the `WRITER_API_KEY` environment variable with your Writer API key or pass it as an argument to the `Writer` constructor.\n\n## [Eliminating Hallucinations with Structured Outputs using Gemini](https://python.useinstructor.com/blog/2024/11/15/eliminating-hallucinations-with-structured-outputs-using-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to generate accurate citations from PDFs. This approach ensures that answers are grounded in the actual content of the PDF, reducing the risk of hallucinations.\n\nWe'll be using the Nvidia 10k report for this example which you can download at this [link](https://d18rn0p25nwr6d.cloudfront.net/CIK-0001045810/78501ce3-7816-4c4d-8688-53dd140df456.pdf).\n\n## [PDF Processing with Structured Outputs with Gemini](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to analyse the [Gemini 1.5 Pro Paper](https://github.com/google-gemini/generative-ai-python/blob/0e5c5f25fe4ce266791fa2afb20d17dee780ca9e/third_party/test.pdf) and extract a structured summary.\n\n### [The Problem](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#the-problem)\n\nProcessing PDFs programmatically has always been painful. The typical approaches all have significant drawbacks:\n\n- **PDF parsing libraries** require complex rules and break easily\n- **OCR solutions** are slow and error-prone\n- **Specialized PDF APIs** are expensive and require additional integration\n- **LLM solutions** often need complex document chunking and embedding pipelines\n\nWhat if we could just hand a PDF to an LLM and get structured data back? With Gemini's multimodal capabilities and Instructor's structured output handling, we can do exactly that.\n\n### [Quick Setup](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#quick-setup)\n\nFirst, install the required packages:\n\n```md-code__content\npip install \"instructor[google-generativeai]\"\n\n```\n\nThen, here's all the code you need:\n\n```md-code__content\nimport instructor\nimport google.generativeai as genai\nfrom google.ai.generativelanguage_v1beta.types.file import File\nfrom pydantic import BaseModel\nimport time\n\n# Initialize the client\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    )\n)\n\n# Define your output structure\nclass Summary(BaseModel):\n    summary: str\n\n# Upload the PDF\nfile = genai.upload_file(\"path/to/your.pdf\")\n\n# Wait for file to finish processing\nwhile file.state != File.State.ACTIVE:\n    time.sleep(1)\n    file = genai.get_file(file.name)\n    print(f\"File is still uploading, state: {file.state}\")\n\nprint(f\"File is now active, state: {file.state}\")\nprint(file)\n\nresp = client.chat.completions.create(\n    messages=[\\\n        {\"role\": \"user\", \"content\": [\"Summarize the following file\", file]},\\\n    ],\n    response_model=Summary,\n)\n\nprint(resp.summary)\n\n```\n\nExpand to see Raw Results\n\n```md-code__content\nsummary=\"Gemini 1.5 Pro is a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. It achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Gemini 1.5 Pro is built to handle extremely long contexts; it has the ability to recall and reason over fine-grained information from up to at least 10M tokens. This scale is unprecedented among contemporary large language models (LLMs), and enables the processing of long-form mixed-modality inputs including entire collections of documents, multiple hours of video, and almost five days long of audio. Gemini 1.5 Pro surpasses Gemini 1.0 Pro and performs at a similar level to 1.0 Ultra on a wide array of benchmarks while requiring significantly less compute to train. It can recall information amidst distractor context, and it can learn to translate a new language from a single set of linguistic documentation. With only instructional materials (a 500-page reference grammar, a dictionary, and ≈ 400 extra parallel sentences) all provided in context, Gemini 1.5 Pro is capable of learning to translate from English to Kalamang, a Papuan language with fewer than 200 speakers, and therefore almost no online presence.\"\n\n```\n\n### [Benefits](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#benefits)\n\nThe combination of Gemini and Instructor offers several key advantages over traditional PDF processing approaches:\n\n**Simple Integration** \\- Unlike traditional approaches that require complex document processing pipelines, chunking strategies, and embedding databases, you can directly process PDFs with just a few lines of code. This dramatically reduces development time and maintenance overhead.\n\n**Structured Output** \\- Instructor's Pydantic integration ensures you get exactly the data structure you need. The model's outputs are automatically validated and typed, making it easier to build reliable applications. If the extraction fails, Instructor automatically handles the retries for you with support for [custom retry logic using tenacity](https://python.useinstructor.com/concepts/retrying/).\n\n**Multimodal Support** \\- Gemini's multimodal capabilities mean this same approach works for various file types. You can process images, videos, and audio files all in the same api request. Check out our [multimodal processing guide](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/) to see how we extract structured data from travel videos.\n\n### [Conclusion](https://python.useinstructor.com/blog/2024/11/11/pdf-processing-with-structured-outputs-with-gemini/\\#conclusion)\n\nWorking with PDFs doesn't have to be complicated.\n\nBy combining Gemini's multimodal capabilities with Instructor's structured output handling, we can transform complex document processing into simple, Pythonic code.\n\nNo more wrestling with parsing rules, managing embeddings, or building complex pipelines – just define your data model and let the LLM do the heavy lifting.\n\nIf you liked this, give `instructor` a try today and see how much easier structured outputs makes working with LLMs become. [Get started with Instructor today!](https://python.useinstructor.com/)\n\n## [Do I Still Need Instructor with Google's New OpenAI Integration?](https://python.useinstructor.com/blog/2024/11/10/do-i-still-need-instructor-with-googles-new-openai-integration/)\n\nGoogle recently launched OpenAI client compatibility for Gemini.\n\nWhile this is a significant step forward for developers by simplifying Gemini model interactions, **you absolutely still need instructor**.\n\nIf you're unfamiliar with instructor, we provide a simple interface to get structured outputs from LLMs across different providers.\n\nThis makes it easy to switch between providers, get reliable outputs from language models and ultimately build production grade LLM applications.\n\n## [Building an LLM-based Reranker for your RAG pipeline](https://python.useinstructor.com/blog/2024/10/23/building-an-llm-based-reranker-for-your-rag-pipeline/)\n\nAre you struggling with irrelevant search results in your Retrieval-Augmented Generation (RAG) pipeline?\n\nImagine having a powerful tool that can intelligently reassess and reorder your search results, significantly improving their relevance to user queries.\n\nIn this blog post, we'll show you how to create an LLM-based reranker using Instructor and Pydantic. This approach will:\n\n- Enhance the accuracy of your search results\n- Leverage the power of large language models (LLMs)\n- Utilize structured outputs for precise information retrieval\n\nBy the end of this tutorial, you'll be able to implement a llm reranker to label your synthetic data for fine-tuning a traditional reranker, or to build out an evaluation pipeline for your RAG system. Let's dive in!\n\n## [Structured Outputs with Multimodal Gemini](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/)\n\nIn this post, we'll explore how to use Google's Gemini model with Instructor to analyze [travel videos](https://www.youtube.com/watch?v=_R8yhW_H9NQ) and extract structured recommendations. This powerful combination allows us to process multimodal inputs (video) and generate structured outputs using Pydantic models. This post was done in collaboration with [Kino.ai](https://kino.ai/), a company that uses instructor to do structured extraction from multimodal inputs to improve search for film makers.\n\n### [Setting Up the Environment](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-with-multimodal-gemini/\\#setting-up-the-environment)\n\nFirst, let's set up our environment with the necessary libraries:\n\n```md-code__content\n\n```\n\n## [Structured Outputs and Prompt Caching with Anthropic](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-and-prompt-caching-with-anthropic/)\n\nAnthropic's ecosystem now offers two powerful features for AI developers: structured outputs and prompt caching. These advancements enable more efficient use of large language models (LLMs). This guide demonstrates how to leverage these features with the Instructor library to enhance your AI applications.\n\n### [Structured Outputs with Anthropic and Instructor](https://python.useinstructor.com/blog/2024/10/23/structured-outputs-and-prompt-caching-with-anthropic/\\#structured-outputs-with-anthropic-and-instructor)\n\nInstructor now offers seamless integration with Anthropic's powerful language models, allowing developers to easily create structured outputs using Pydantic models. This integration simplifies the process of extracting specific information from AI-generated responses.\n\n## [Flashcard generator with Instructor + Burr](https://python.useinstructor.com/blog/2024/10/18/youtube-flashcards/)\n\nFlashcards help break down complex topics and learn anything from biology to a new language or lines for a play. This blog will show how to use LLMs to generate flashcards and kickstart your learning!\n\n**Instructor** lets us get structured outputs from LLMs reliably, and [Burr](https://github.com/dagworks-inc/burr) helps create an LLM application that's easy to understand and debug. It comes with **Burr UI**, a free, open-source, and local-first tool for observability, annotations, and more!\n\n## [Audio Support in OpenAI's Chat Completions API](https://python.useinstructor.com/blog/2024/10/17/audio-support-in-openais-chat-completions-api/)\n\nOpenAI has recently introduced audio support in their Chat Completions API, opening up exciting new possibilities for developers working with audio and text interactions. This feature is powered by the new `gpt-4o-audio-preview` model, which brings advanced voice capabilities to the familiar Chat Completions API interface.",
    "metadata": {
      "url": "https://python.useinstructor.com/blog/?q=",
      "ogUrl": "https://python.useinstructor.com/blog/",
      "title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/blog/",
      "robots": "noindex",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/blog/index.png",
      "ogTitle": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/blog/index.png",
      "og:title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "viewport": [
        "width=device-width,initial-scale=1",
        "width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"
      ],
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/blog/?q=",
      "statusCode": 200,
      "description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/blog/index.png",
      "twitter:title": "Subscribe to Our Newsletter for AI Updates and Tips - Instructor",
      "og:description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Subscribe to our newsletter for AI updates, tips, and insights into the latest features and advancements in AI technology."
    }
  }
]