[
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/decomposition/plan_and_solve/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/decomposition/plan_and_solve.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/decomposition/plan_and_solve.md \"View source of this page\")\n\n# Ditch Vanilla Chain Of Thought\n\nPlan and Solve[1](https://arxiv.org/pdf/2305.04091) improves the use of an improved Zero-Shot Chain Of Thought (CoT) prompt which adds more detailed instructions to the prompt given to these large language models.\n\nPlan and Solve Prompt\n\n\\[User Prompt\\]\n\n**Let’s first understand the problem, extract relevant variables and their corresponding numerals, and make a complete plan.Then, let’s carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.**\n\n\\[Model Response\\]\n\n**Therefore the answer(arabic numerals) is**\n\nThis is a two step process which guides the LLM to pay more attention to calculation and intermediate results to ensure that they are correctly performed as much as possible.\n\n1. **Generate Reasoning**: In the first step we prompt the model with the user's query and prime the model using plan and solve prompting to explicitly devise a plan for solving a problem before generating an intermediate reasoning process\n2. **Extract Answer** : Once we've obtained the model's reasoning, we then extract the answer from a new prompt which includes the model's chain of thought.\n\n![](https://python.useinstructor.com/img/plan_and_solve.png)\n\nWe can implement this using `instructor` as seen below.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\n\nclass Reasoning(BaseModel):\n    chain_of_thought: str\n\nclass Response(BaseModel):\n    correct_answer: str\n\ndef generate_reasoning(query: str):\n    return client.chat.completions.create(\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                <user query>\\\n                {query}\\\n                </user query>\\\n\\\n                Let's first understand the problem,\\\n                extract relevant variables and their\\\n                corresponding numerals, and make a\\\n                complete plan. Then, let's carry out\\\n                the plan, calculate intermediate\\\n                variables (pay attention to correct\\\n                numerical calculation and commonsense),\\\n                solve the problem step by step, and\\\n                show the answer.\\\n                \"\"\",\\\n            },\\\n        ],\n        response_model=Reasoning,\n        model=\"gpt-4o\",\n    )\n\ndef extract_answer(query: str, reasoning: Reasoning):\n    return client.chat.completions.create(\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                <user query>\\\n                    {query}\\\n                </user query>\\\n\\\n                Let's first understand the problem,\\\n                extract relevant variables and their\\\n                corresponding numerals, and make a\\\n                complete plan. Then, let's carry out\\\n                the plan, calculate intermediate\\\n                variables (pay attention to correct\\\n                numerical calculation and commonsense),\\\n                solve the problem step by step, and\\\n                show the answer.\\\n\\\n                <reasoning>\\\n                {reasoning.chain_of_thought}\\\n                </reasoning>\\\n\\\n                Therefore the answer (arabic numerals) is\\\n                \"\"\",\\\n            }\\\n        ],\n        model=\"gpt-4o\",\n        response_model=Response,\n    )\n\nif __name__ == \"__main__\":\n    query = (\n        \"In a dance class of 20 students, 20% enrolled \"\n        \"in contemporary dance, 25% of the remaining \"\n        \"enrolled in jazz dance and the rest enrolled \"\n        \"in hip-hop dance. What percentage of the entire \"\n        \"students enrolled in hip-hop dance?\"\n    )\n\n    reasoning = generate_reasoning(query)\n    print(reasoning.model_dump_json(indent=2))\n    \"\"\"\n    {\n    \"chain_of_thought\": \"Let's first break down the\n    problem:\\n\\n1. Total number of students = 20\\n2.\n    Percentage enrolled in contemporary dance = 20%\\n\\n\n    Step-by-Step Plan:\\n1. Calculate the number of\n    students enrolled in contemporary dance.\\n2.\n    Calculate the remaining students after contemporary\n    dance enrollment.\\n3. Calculate the percentage and\n    number of students from the remaining who enrolled in\n    jazz dance.\\n4. Determine the remaining students who\n    enrolled in hip-hop dance.\\n5. Finally, calculate the\n    percentage of the entire students who enrolled in\n    hip-hop dance.\\n\\nLet's carry out the plan:\\n\\n1.\n    Number of students enrolled in contemporary dance =\n    20% of 20 = (20/100) * 20 = 4\\n2. Remaining students\n    after contemporary = 20 - 4 = 16\\n3. Percentage of\n    remaining students enrolled in jazz dance = 25%\\n\n    Number of students enrolled in jazz dance = 25% of 16\n    = (25/100) * 16 = 4\\n4. Remaining students after\n    contemporary and jazz = 16 - 4 = 12\\n5. The number of\n    students enrolled in hip-hop dance = 12\\n6.\n    Percentage of entire students enrolled in hip-hop =\n    (Number of hip-hop students / Total students) *\n    100\\n   Percentage = (12 / 20) * 100 = 60%\\n\\nThus,\n    60% of the entire students enrolled in hip-hop dance.\"\n    }\n    \"\"\"\n\n    response = extract_answer(query, reasoning)\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"correct_answer\": \"60\"\n    }\n    \"\"\"\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/decomposition/plan_and_solve/\\#references \"Permanent link\")\n\n1: [Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models](https://arxiv.org/pdf/2305.04091)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/decomposition/plan_and_solve/",
      "ogUrl": "https://python.useinstructor.com/prompting/decomposition/plan_and_solve/",
      "title": "Ditch Vanilla Chain Of Thought - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/decomposition/plan_and_solve/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/plan_and_solve.png",
      "ogTitle": "Ditch Vanilla Chain Of Thought - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/plan_and_solve.png",
      "og:title": "Ditch Vanilla Chain Of Thought - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/decomposition/plan_and_solve/",
      "statusCode": 200,
      "description": "Plan and Solve involves the use of an improved zero-shot CoT prompt. This generates more robust reasoning processes than standard Zero-Shot CoT on multiple reasoning datasets",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Plan and Solve involves the use of an improved zero-shot CoT prompt. This generates more robust reasoning processes than standard Zero-Shot CoT on multiple reasoning datasets",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/plan_and_solve.png",
      "twitter:title": "Ditch Vanilla Chain Of Thought - Instructor",
      "og:description": "Plan and Solve involves the use of an improved zero-shot CoT prompt. This generates more robust reasoning processes than standard Zero-Shot CoT on multiple reasoning datasets",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Plan and Solve involves the use of an improved zero-shot CoT prompt. This generates more robust reasoning processes than standard Zero-Shot CoT on multiple reasoning datasets"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/ensembling/diverse/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/ensembling/diverse.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/ensembling/diverse.md \"View source of this page\")\n\n# Verify Responses over Majority Voting\n\nDiverse Verifier On Reasoning Step (DiVeRSe)[1](https://aclanthology.org/2023.acl-long.291/) is a prompting technique which provides two main improvements\n\n1. **Diverse Prompts** : They generate multiple variations of the same prompt by varying the examples used in each prompt\n2. **Verification** : They use a finetuned `Deberta-V3-Large` to determine the quality of a generated response. Instead of using majority voting, they use their model to score each generated response from 0 to 1. They then aggregate these scores for each unique answer to determine the best generated solution.\n\nIn the paper itself, they also train a step-wise verifier that is able to score each individual reasoning step. This enables much more fine-grained predictions but is challenging to obtain training data for.\n\nWe can implement this in `instructor`. However, instead of using a `deberta-v3-large` model, we'll be using gpt-4o to score its own outputs and generate a quality score.\n\n```md-code__content\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\nfrom typing import Literal\nfrom textwrap import dedent\nimport asyncio\nfrom collections import defaultdict\n\nclient = instructor.from_openai(AsyncOpenAI())\n\nclass Response(BaseModel):\n    chain_of_thought: str\n    answer: int\n\nclass Grading(BaseModel):\n    grade: Literal[\"Poor\", \"Average\", \"Good\", \"Excellent\"]\n\n    def get_score(self):\n        mapping = {\n            \"Poor\": 0.25,\n            \"Average\": 0.5,\n            \"Good\": 0.75,\n            \"Excellent\": 1,\n        }\n        return mapping[self.grade]\n\nasync def generate_response(query: str, examples: list[str]):\n    formatted_examples = \"\\n\".join(examples)\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": dedent(\\\n                    f\"\"\"\\\n                You are a world class AI that excels at answering\\\n                user queries in a succint and accurate manner.\\\n\\\n                <query>\\\n                {query}\\\n                </query>\\\n\\\n                <examples>\\\n                {formatted_examples}\\\n                </examples>\\\n                \"\"\"\\\n                ),\\\n            }\\\n        ],\n        response_model=Response,\n    )\n\nasync def score_response(query: str, response: Response) -> tuple[Response, Grading]:\n    return (\n        response,\n        await client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[\\\n                {\\\n                    \"role\": \"user\",\\\n                    \"content\": dedent(\\\n                        f\"\"\"\\\n                You are a world class AI that excels at grading\\\n                responses to a user query in a succint and clear\\\n                manner.\\\n\\\n                <query>\\\n                {query}\\\n                </query>\\\n\\\n                <response>\\\n                {response}\\\n                </response>\\\n                \"\"\"\\\n                    ),\\\n                }\\\n            ],\n            response_model=Grading,\n        ),\n    )\n\nasync def generate_response_batch(\n    query: str, examples: list[str], n_examples_per_batch: int\n):\n    batches: list[list[str]] = []\n    for i in range(0, len(examples), n_examples_per_batch):\n        batches.append(examples[i : i + n_examples_per_batch])\n\n    coros = [generate_response(query, example_batch) for example_batch in batches]\n    return await asyncio.gather(*coros)\n\nasync def score_responses(\n    query: str, responses: list[Response]\n) -> list[tuple[Response, Grading]]:\n    coros = [score_response(query, response) for response in responses]\n    return await asyncio.gather(*coros)\n\nif __name__ == \"__main__\":\n    examples = [\\\n        \"\"\"\\\n        Q: James decides to run 3 sprints 3 times a week.\\\n        He runs 60 meters each sprint. How many total\\\n        meters does he run a week?\\\n        A: James decides to run 3 sprints 3 times a week.\\\n        He runs 60 meters each sprint. So he runs 60 meters\\\n        x 3 sprints x 3 times a week. That is 60 meters x 9.\\\n        The answer is 540.\\\n        \"\"\",\\\n        \"\"\"\\\n        Q: Brandon's iPhone is four times as old as Ben's\\\n        iPhone. Ben's iPhone is two times older than Suzy's\\\n        iPhone. If Suzy's iPhone is 1 year old, how old is\\\n        Brandon's iPhone?\\\n        A: Brandon's iPhone is 4 times as old as Ben's\\\n        iPhone. Ben's iPhone is 2 times older than Suzy's\\\n        iPhone. So Brandon's iPhone is 4 x 2 = 8 times older\\\n        than Suzy's iPhone. Suzy's iPhone is 1 year old. So\\\n        Brandon's iPhone is 8 x 1 = 8 years old. The answer\\\n        is 8.\\\n        \"\"\",\\\n        \"\"\"\\\n        Q: Jean has 30 lollipops. Jean eats 2 of the\\\n        lollipops. With the remaining lollipops, Jean wants\\\n        to package 2 lollipops in one bag. How many bags can\\\n        Jean fill?\\\n        A: Jean started with 30 lollipops. She ate 2 of\\\n        them. So she has 28 lollipops left. She wants to\\\n        package 2 lollipops in one bag. So she can package\\\n        28 / 2 = 14 bags. The answer is 14.\\\n        \"\"\",\\\n        \"\"\"\\\n        Q: Weng earns $12 an hour for babysitting.\\\n        Yesterday, she just did 50 minutes of babysitting.\\\n        How much did she earn?\\\n        A: Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\\n        Working 50 minutes, she earned 0.2 x 50 =\\\n        $<<0.2*50=10>>10. The answer is 10\\\n        \"\"\",\\\n    ]\n\n    query = \"\"\"Betty is saving money for a new wallet which\n    costs $100. Betty has only half of the money she needs.\n    Her parents decided to give her $15 for that purpose,\n    and her grandparents twice as much as her parents. How\n    much more money does Betty need to buy the wallet?\"\"\"\n\n    generated_responses = asyncio.run(generate_response_batch(query, examples, 1))\n\n    scored_responses = asyncio.run(score_responses(query, generated_responses))\n\n    scores: dict[int, float] = defaultdict(int)\n\n    for response, grade in scored_responses:\n        scores[response.answer] += grade.get_score()\n\n    print(scores)\n    #> defaultdict(<class 'int'>, {5: 3.5})\n\n    answer = max(scores, key=scores.get)\n    print(answer)\n    #> 5\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/ensembling/diverse/\\#references \"Permanent link\")\n\n1: [Making Language Models Better Reasoners with Step-Aware Verifier](https://aclanthology.org/2023.acl-long.291/)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/ensembling/diverse/",
      "ogUrl": "https://python.useinstructor.com/prompting/ensembling/diverse/",
      "title": "Verify Responses over Majority Voting - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/ensembling/diverse/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/diverse.png",
      "ogTitle": "Verify Responses over Majority Voting - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/diverse.png",
      "og:title": "Verify Responses over Majority Voting - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/ensembling/diverse/",
      "statusCode": 200,
      "description": "Diverse creates multiple prompts for a given problem before performing self-consistency for each. It then generates multiple reaosning paths before choosing the best final response",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Diverse creates multiple prompts for a given problem before performing self-consistency for each. It then generates multiple reaosning paths before choosing the best final response",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/diverse.png",
      "twitter:title": "Verify Responses over Majority Voting - Instructor",
      "og:description": "Diverse creates multiple prompts for a given problem before performing self-consistency for each. It then generates multiple reaosning paths before choosing the best final response",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Diverse creates multiple prompts for a given problem before performing self-consistency for each. It then generates multiple reaosning paths before choosing the best final response"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/few_shot/cosp/#consistency-based-self-adaptive-prompting-cosp)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/few_shot/cosp.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/few_shot/cosp.md \"View source of this page\")\n\n# Consistency Based Self Adaptive Prompting (COSP) [¶](https://python.useinstructor.com/prompting/few_shot/cosp/\\#consistency-based-self-adaptive-prompting-cosp \"Permanent link\")\n\nCOSP is a technique that aims to improve few-shot learning by selecting high-quality examples based on the consistency and confidence of model responses. This approach helps create more effective prompts by identifying examples that the model can process reliably.\n\n## Overview [¶](https://python.useinstructor.com/prompting/few_shot/cosp/\\#overview \"Permanent link\")\n\nThe COSP process involves two main stages:\n\n1. **Example Generation**: Generate multiple responses for potential examples\n\n2. Run each example through the model multiple times\n\n3. Collect responses and confidence scores\n\n4. **Example Selection**: Select the best examples based on entropy and repetitiveness\n\n5. Calculate entropy of responses to measure consistency\n6. Evaluate repetitiveness to ensure reliability\n\n## How COSP Works [¶](https://python.useinstructor.com/prompting/few_shot/cosp/\\#how-cosp-works \"Permanent link\")\n\n### Stage 1: Example Generation [¶](https://python.useinstructor.com/prompting/few_shot/cosp/\\#stage-1-example-generation \"Permanent link\")\n\nFor each potential example in your dataset:\n\n1. Generate multiple responses (typically 3-5)\n2. Calculate the entropy of these responses\n3. Measure the repetitiveness across responses\n\n```md-code__content\nfrom typing import List\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclass Response(BaseModel):\n    content: str = Field(description=\"The model's response to the prompt\")\n    confidence: float = Field(description=\"Confidence score between 0 and 1\")\n\nclient = instructor.from_openai(OpenAI())\n\ndef generate_responses(prompt: str, n: int = 3) -> List[Response]:\n    responses = []\n    for _ in range(n):\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            response_model=Response\n        )\n        responses.append(response)\n    return responses\n\n```\n\n### Stage 2: Example Selection [¶](https://python.useinstructor.com/prompting/few_shot/cosp/\\#stage-2-example-selection \"Permanent link\")\n\nCalculate metrics for each example:\n\n1. **Entropy**: Measure response variability\n2. **Repetitiveness**: Check response consistency\n\n```md-code__content\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef calculate_metrics(responses: List[Response]) -> tuple[float, float]:\n    # Calculate entropy\n    confidences = [r.confidence for r in responses]\n    entropy_score = entropy(confidences)\n\n    # Calculate repetitiveness\n    unique_responses = len(set(r.content for r in responses))\n    repetitiveness = 1 - (unique_responses / len(responses))\n\n    return entropy_score, repetitiveness\n\n```\n\n## Implementation Example [¶](https://python.useinstructor.com/prompting/few_shot/cosp/\\#implementation-example \"Permanent link\")\n\nHere's a complete example of COSP implementation:\n\n```md-code__content\nfrom typing import List, Tuple\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\nimport numpy as np\nfrom scipy.stats import entropy\n\nclass Example(BaseModel):\n    text: str\n    score: float = Field(description=\"Combined quality score\")\n    entropy: float = Field(description=\"Entropy of responses\")\n    repetitiveness: float = Field(description=\"Repetitiveness of responses\")\n\nclass COSPSelector:\n    def __init__(self, client: OpenAI, n_samples: int = 3):\n        self.client = instructor.from_openai(client)\n        self.n_samples = n_samples\n\n    def generate_responses(self, prompt: str) -> List[Response]:\n        return [\\\n            self.client.chat.completions.create(\\\n                model=\"gpt-4\",\\\n                messages=[{\"role\": \"user\", \"content\": prompt}],\\\n                response_model=Response\\\n            )\\\n            for _ in range(self.n_samples)\\\n        ]\n\n    def calculate_metrics(self, responses: List[Response]) -> Tuple[float, float]:\n        confidences = [r.confidence for r in responses]\n        entropy_score = entropy(confidences)\n\n        unique_responses = len(set(r.content for r in responses))\n        repetitiveness = 1 - (unique_responses / len(responses))\n\n        return entropy_score, repetitiveness\n\n    def select_examples(self, candidates: List[str], k: int) -> List[Example]:\n        examples = []\n\n        for text in candidates:\n            responses = self.generate_responses(text)\n            entropy_score, repetitiveness = self.calculate_metrics(responses)\n\n            # Combined score (lower is better)\n            score = entropy_score - repetitiveness\n\n            examples.append(Example(\n                text=text,\n                score=score,\n                entropy=entropy_score,\n                repetitiveness=repetitiveness\n            ))\n\n        # Sort by score (lower is better) and select top k\n        return sorted(examples, key=lambda x: x.score)[:k]\n\n```\n\n## Usage Example [¶](https://python.useinstructor.com/prompting/few_shot/cosp/\\#usage-example \"Permanent link\")\n\n```md-code__content\n# Initialize COSP selector\nclient = OpenAI()\nselector = COSPSelector(client)\n\n# Candidate examples\ncandidates = [\\\n    \"The quick brown fox jumps over the lazy dog\",\\\n    \"Machine learning is a subset of artificial intelligence\",\\\n    \"Python is a high-level programming language\",\\\n    # ... more examples\\\n]\n\n# Select best examples\nbest_examples = selector.select_examples(candidates, k=3)\n\n# Use selected examples in your prompt\nselected_texts = [ex.text for ex in best_examples]\nprompt = f\"\"\"Use these examples to guide your response:\n\nExamples:\n{chr(10).join(f'- {text}' for text in selected_texts)}\n\nNow, please respond to: [your query here]\n\"\"\"\n\n```\n\n## Benefits of COSP [¶](https://python.useinstructor.com/prompting/few_shot/cosp/\\#benefits-of-cosp \"Permanent link\")\n\n1. **Improved Consistency**: By selecting examples with low entropy and high repetitiveness\n2. **Better Performance**: More reliable few-shot learning\n3. **Automated Selection**: No manual example curation needed\n4. **Quality Metrics**: Quantifiable measure of example quality\n\n## Limitations [¶](https://python.useinstructor.com/prompting/few_shot/cosp/\\#limitations \"Permanent link\")\n\n1. **Computational Cost**: Requires multiple API calls per example\n2. **Time Overhead**: Selection process can be slow for large candidate sets\n3. **Model Dependency**: Performance may vary across different models\n\n## Related Techniques [¶](https://python.useinstructor.com/prompting/few_shot/cosp/\\#related-techniques \"Permanent link\")\n\n- [Universal Self Prompting (USP)](https://python.useinstructor.com/prompting/ensembling/usp/)\n- Chain of Thought Prompting\n- Self-Consistency\n\n## References [¶](https://python.useinstructor.com/prompting/few_shot/cosp/\\#references \"Permanent link\")\n\n1. Original COSP Paper: [arXiv:2305.14121](https://arxiv.org/abs/2305.14121)\n2. Related Work: [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/few_shot/cosp/",
      "ogUrl": "https://python.useinstructor.com/prompting/few_shot/cosp/",
      "title": "Consistent Based Examples - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/few_shot/cosp/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/few_shot/cosp.png",
      "ogTitle": "Consistent Based Examples - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/few_shot/cosp.png",
      "og:title": "Consistent Based Examples - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/few_shot/cosp/",
      "statusCode": 200,
      "description": "Consistency Based Self Adaptive Prompting (COSP) is a technique that uses entropy and repetitiveness to select high-quality examples for few-shot learning.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Consistency Based Self Adaptive Prompting (COSP) is a technique that uses entropy and repetitiveness to select high-quality examples for few-shot learning.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/few_shot/cosp.png",
      "twitter:title": "Consistent Based Examples - Instructor",
      "og:description": "Consistency Based Self Adaptive Prompting (COSP) is a technique that uses entropy and repetitiveness to select high-quality examples for few-shot learning.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Consistency Based Self Adaptive Prompting (COSP) is a technique that uses entropy and repetitiveness to select high-quality examples for few-shot learning."
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/decomposition/recurs_of_thought.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/decomposition/recurs_of_thought.md \"View source of this page\")\n\n# Recurs.-of-Thought\n\n\\[wip\\]\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/decomposition/recurs_of_thought/",
      "ogUrl": "https://python.useinstructor.com/prompting/decomposition/recurs_of_thought/",
      "title": "Recurs.-of-Thought - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/decomposition/recurs_of_thought/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/recurs_of_thought.png",
      "ogTitle": " - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/recurs_of_thought.png",
      "og:title": " - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/decomposition/recurs_of_thought/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/recurs_of_thought.png",
      "twitter:title": " - Instructor",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": []
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/self_criticism/cumulative_reason/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/self_criticism/cumulative_reason.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/self_criticism/cumulative_reason.md \"View source of this page\")\n\n# Break Down Reasoning Into Multiple Steps\n\nCumulative Reasoning[1](https://arxiv.org/pdf/2308.04371) aims to generate better outputs by dividing the reasoning process into three separate steps\n\n1. **Propose** : A LLM first suggests potential steps based on the current context, initiating the reasoning cycle\n2. **Verify** : We then assess the proposer's suggestions for accuracy, incorporating valid steps into the ongoing context\n3. **Report** : We then determine the appropriate moment to conclude the reasoning process\n\nBy first generating potential steps and separating out each portions of the reasoning process, we are able to obtain significant improvements in logical inference tasks and mathematical problems.\n\nWe can implement this using `instructor` as seen below\n\n```md-code__content\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field\nfrom textwrap import dedent\nfrom typing import Literal\nimport asyncio\n\nclient = instructor.from_openai(AsyncOpenAI())\n\nclass Proposition(BaseModel):\n    premise1: str\n    premise2: str\n    reasoning: str\n    proposition: str\n\nclass ProposerOutput(BaseModel):\n    reasoning: str\n    valid_propositions: list[Proposition] = Field(\n        description=\"Concise list of Propositions that are derived from the premises that are relevant to the hypothesis. Note that each Proposition is derived from two given premises at most\",\n        min_length=4,\n    )\n    prediction: Literal[\"False\", \"True\", \"Unknown\"]\n\nclass VerifiedProposition(BaseModel):\n    proposition: str\n    reasoning: str\n    is_valid: bool\n\nclass ReporterOutput(BaseModel):\n    reasoning: str\n    is_valid_hypothesis: bool\n\nasync def generate_propositions(premises: list[str], hypothesis: str) -> ProposerOutput:\n    formatted_premises = \"\\n- \".join(premises)\n    return await client.chat.completions.create(\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": dedent(\\\n                    \"\"\"\\\n                Suppose you are one of the greatest AI\\\n                scientists, logicians, and mathematicians.\\\n\\\n                Let us think step by step. Please use\\\n                First-Order Logic (FOL) to deduce a list\\\n                of Propositions. Each Proposition is\\\n                derived from two given Premises and\\\n                should be logically correct. Most\\\n                importantly, each Proposition should\\\n                not duplicate the two premises that it\\\n                is derived from. Please make sure your\\\n                reasoning is directly deduced from the\\\n                Premises and Propositions rather than\\\n                introducing unsourced common knowledge\\\n                and unsourced information by common\\\n                sense reasoning.\\\n                \"\"\"\\\n                ),\\\n            },\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": dedent(\\\n                    f\"\"\"\\\n                Premises:\\\n                {formatted_premises}\\\n\\\n                We want to deduce more Propositions to\\\n                determine the correctness of the following\\\n                Hypothesis:\\\n                Hypothesis: {hypothesis}\\\n                \"\"\"\\\n                ),\\\n            },\\\n        ],\n        response_model=ProposerOutput,\n        model=\"gpt-4o\",\n    )\n\nasync def verify_propositions(\n    premise_evaluation: ProposerOutput,\n) -> list[VerifiedProposition]:\n    async def create_verification_task(proposition: Proposition) -> VerifiedProposition:\n        return await client.chat.completions.create(\n            messages=[\\\n                {\\\n                    \"role\": \"system\",\\\n                    \"content\": \"\"\"\\\n                    Suppose you are one of the greatest AI\\\n                    scientists, logicians, and mathematicians.\\\n                    Let us think step by step. Please use\\\n                    First-Order Logic (FOL) to determine\\\n                    whether the deduction of two given\\\n                    Premises to a Proposition is valid or not,\\\n                    and reply with True or False.\\\n                    \"\"\",\\\n                },\\\n                {\\\n                    \"role\": \"user\",\\\n                    \"content\": f\"\"\"\\\n                    Premises:\\\n                    {proposition.premise1}\\\n                    {proposition.premise2}\\\n\\\n                    Proposition:\\\n                    {proposition.proposition}\\\n                    \"\"\",\\\n                },\\\n            ],\n            response_model=VerifiedProposition,\n            model=\"gpt-4o\",\n        )\n\n    tasks = [\\\n        create_verification_task(proposition)\\\n        for proposition in premise_evaluation.valid_propositions\\\n    ]\n\n    return await asyncio.gather(*tasks)\n\nasync def final_evaluation(\n    verification_result: list[str], hypothesis: str, premises: list[str]\n) -> ReporterOutput:\n    formatted_premises = \"\\n- \".join(premises)\n    formatted_propositions = \"\\n- \".join(verification_result)\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"\"\"\\\n                Suppose you are one of the greatest AI\\\n                scientists, logicians, and mathematicians.\\\n                Let us think step by step. Read and analyze\\\n                the “Premises” first, then use First-Order\\\n                Logic (FOL) to judge whether the “Hypothesis”\\\n                is True, False, or Unknown. Please make sure\\\n                your reasoning is directly deduced from the\\\n                \"Premises\" and \"Propositions\" rather than\\\n                introducing unsourced common knowledge and\\\n                unsourced information by common sense\\\n                reasoning.\\\n                \"\"\",\\\n            },\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                Premises:\\\n                {formatted_premises}\\\n\\\n                Hypothesis: {hypothesis}\\\n                \"\"\",\\\n            },\\\n            {\\\n                \"role\": \"assistant\",\\\n                \"content\": f\"\"\"\\\n                Let's think step by step. From the premises,\\\n                we can deduce the following propositions:\\\n                {formatted_propositions}\\\n\\\n                Recall the Hypothesis: {hypothesis}\\\n                \"\"\",\\\n            },\\\n        ],\n        response_model=ReporterOutput,\n    )\n\nif __name__ == \"__main__\":\n    hypothesis = \"Hyraxes lay eggs\"\n    premises = [\\\n        \"The only types of mammals that lay eggs are platypuses and echidnas\",\\\n        \"Platypuses are not hyrax\",\\\n        \"Echidnas are not hyrax\",\\\n        \"No mammals are invertebrates\",\\\n        \"All animals are either vertebrates or invertebrates\",\\\n        \"Mammals are animals\",\\\n        \"Hyraxes are mammals\",\\\n        \"Grebes lay eggs\",\\\n        \"Grebes are not platypuses and also not echidnas\",\\\n    ]\n    premise_evaluation = asyncio.run(generate_propositions(premises, hypothesis))\n\n    verification_result = asyncio.run(verify_propositions(premise_evaluation))\n\n    filtered_propositions = [\\\n        proposition.proposition\\\n        for proposition in verification_result\\\n        if proposition.is_valid\\\n    ]\n\n    reporter_output = asyncio.run(\n        final_evaluation(filtered_propositions, hypothesis, premises)\n    )\n    print(reporter_output.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"reasoning\": \"Based on the premises provided, the\n      only mammals that lay eggs are platypuses and\n      echidnas. Hyraxes are mammals but are explicitly\n      stated as not being platypuses or echidnas. Hence,\n      there is no basis in the premises to conclude that\n      hyraxes lay eggs. \\n\\nTherefore, the hypothesis that\n      hyraxes lay eggs is False.\",\n      \"is_valid_hypothesis\": false\n    }\n    \"\"\"\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/self_criticism/cumulative_reason/\\#references \"Permanent link\")\n\n1: [Cumulative Reasoning with Large Language Models](https://arxiv.org/pdf/2308.04371)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/self_criticism/cumulative_reason/",
      "ogUrl": "https://python.useinstructor.com/prompting/self_criticism/cumulative_reason/",
      "title": "Break Down Reasoning Into Multiple Steps - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/self_criticism/cumulative_reason/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/cumulative_reason.png",
      "ogTitle": "Break Down Reasoning Into Multiple Steps - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/cumulative_reason.png",
      "og:title": "Break Down Reasoning Into Multiple Steps - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/self_criticism/cumulative_reason/",
      "statusCode": 200,
      "description": "Cumulative Reasoning breaks the reasoning process into three separate steps so that our model has enough room to reason and filter out the reasoning steps at each point, thus improving model performance",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Cumulative Reasoning breaks the reasoning process into three separate steps so that our model has enough room to reason and filter out the reasoning steps at each point, thus improving model performance",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/cumulative_reason.png",
      "twitter:title": "Break Down Reasoning Into Multiple Steps - Instructor",
      "og:description": "Cumulative Reasoning breaks the reasoning process into three separate steps so that our model has enough room to reason and filter out the reasoning steps at each point, thus improving model performance",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Cumulative Reasoning breaks the reasoning process into three separate steps so that our model has enough room to reason and filter out the reasoning steps at each point, thus improving model performance"
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/decomposition/program_of_thought.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/decomposition/program_of_thought.md \"View source of this page\")\n\n# Generate Python for Intermediate Steps\n\nProgram of Thought aims to leverage an external python interpreter in order to generate intermediate reasoning steps. This helps us to achieve a greater degree of performance in mathematical and programming-related tasks by grounding our final response in deterministic code.\n\n![](https://python.useinstructor.com/img/pot.jpeg)\n\nWe can implement it in `instructor` as seen below\n\n```md-code__content\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field, field_validator\nimport instructor\nfrom textwrap import dedent\nfrom typing import Literal\n\nclient = instructor.from_openai(OpenAI())\n\nprefix = \"\"\"\n# Answer this question by implementing a solver()\n# function, use for loop if necessary.\ndef solver():\n    # Let's write a Python program step by step,\n    # and then return the answer\n    # Firstly, we need to define the following\n    # variable:\n\"\"\".strip()\n\ndef execute_program(code: str):\n    code = code.strip() + \"\\nans = solver()\"\n    print(code)\n    \"\"\"\n    # Answer this question by implementing a\n    # solver() function, use for loop if necessary.\n    def solver():\n        # Let's write a Python program step by step,\n        # and then return the answer\n        # Firstly, we need to define the following\n        # variable:\n        selling_price = 360\n        profit_percentage = 20\n\n        # To find the cost price, use the formula:\n        # cost_price = selling_price / (1 + profit_percentage / 100)\n        cost_price = selling_price / (1 + profit_percentage / 100)\n\n        return cost_price\n\n    # Running the solver function to get the cost price\n    result = solver()\n    print(result)\n    ans = solver()\n    \"\"\"\n    exec(code)\n    locals_ = locals()\n    return locals_.get(\"ans\")\n\nclass Prediction(BaseModel):\n    choice: Literal[\"A\", \"B\", \"C\", \"D\", \"E\"]\n\nclass ProgramExecution(BaseModel):\n    program_code: str = Field(\n        description=\"\"\"Program Code that\n    once executed contains the final answer\"\"\"\n    )\n\n    @field_validator(\"program_code\")\n    @classmethod\n    def ensure_valid_code(cls, v: str) -> str:\n        if not v.startswith(prefix):\n            raise ValueError(\n                f\"\"\"Program Code must begin with the desired\n                prefix of {prefix}\"\"\"\n            )\n\n        answer = execute_program(v)\n        if not answer:\n            raise ValueError(\n                f\"\"\"Make sure to return the answer to the\n                question within the solver function\"\"\"\n            )\n\n        return str(answer)\n\ndef generate_intermediate_reasoning(query: str):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": dedent(\\\n                    f\"\"\"\\\n                You are a world class AI system that excels\\\n                at answering user queries in a systematic\\\n                and detailed manner. You are about to be\\\n                passed a user query to respond to. Make sure\\\n                to generate a valid program that can be\\\n                executed to answer the user query.\\\n\\\n                Make sure to begin your generated program\\\n                with the following prefix\\\n\\\n                {prefix}\\\n                \"\"\"\\\n                ),\\\n            },\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": query,\\\n            },\\\n        ],\n        response_model=ProgramExecution,\n    )\n\ndef generate_prediction(\n    predicted_answer: str, options: list[str], query: str\n) -> Prediction:\n    formatted_options = \",\".join(options)\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Prediction,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": dedent(\\\n                    f\"\"\"\\\n                Find the closest options based on the\\\n                question and prediction.\\\n\\\n                Question: {query}\\\n                Prediction: {predicted_answer}\\\n                Options: [{formatted_options}]\\\n                \"\"\"\\\n                ),\\\n            }\\\n        ],\n    )\n\nif __name__ == \"__main__\":\n    query = \"\"\"A trader sold an article at a profit of 20%\n    for Rs.360. What is the cost price of the article?\"\"\"\n    reasoning = generate_intermediate_reasoning(query)\n    options = [\"A)270\", \"B)300\", \"C)280\", \"D)320\", \"E)315\"]\n    print(reasoning.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"program_code\": \"300.0\"\n    }\n    \"\"\"\n\n    prediction = generate_prediction(reasoning.program_code, options, query)\n    print(prediction.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"choice\": \"B\"\n    }\n    \"\"\"\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/decomposition/program_of_thought/",
      "ogUrl": "https://python.useinstructor.com/prompting/decomposition/program_of_thought/",
      "title": "Generate Python for Intermediate Steps - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/decomposition/program_of_thought/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/program_of_thought.png",
      "ogTitle": "Generate Python for Intermediate Steps - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/program_of_thought.png",
      "og:title": "Generate Python for Intermediate Steps - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/decomposition/program_of_thought/",
      "statusCode": 200,
      "description": "Program Of Thought",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Program Of Thought",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/program_of_thought.png",
      "twitter:title": "Generate Python for Intermediate Steps - Instructor",
      "og:description": "Program Of Thought",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Program Of Thought"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/decomposition/skeleton_of_thought/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/decomposition/skeleton_of_thought.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/decomposition/skeleton_of_thought.md \"View source of this page\")\n\n# Generate in Parallel\n\nHow do we decrease the latency of an LLM pipeline?\n\nSkelelton-of-Thought is a technique which prompts an LLM to generate a skeleton outline of the response, then completes each point in the skeleton in parallel. The parallelism can be achieved by parallel API calls or batched decoding.\n\nBelow is an example of an implementation using parallel API calls with `instructor`:\n\n```md-code__content\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\nimport asyncio\n\nclient = instructor.from_openai(AsyncOpenAI())\n\nclass Point(BaseModel):\n    index: int\n    description: str\n\nclass Skeleton(BaseModel):\n    points: list[Point]\n\nclass Response(BaseModel):\n    response: str\n\nasync def get_skeleton(question):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Skeleton,\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                You’re an organizer responsible for only giving the skeleton (not the full content) for answering the question.\\\n                Provide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question.\\\n                Instead of writing a full sentence, each skeleton point should be very short with only 3∼5 words.\\\n                Generally, the skeleton should have 3∼10 points.\\\n\\\n                Now, please provide the skeleton for the following question.\\\n\\\n                <question>\\\n                {question}\\\n                </question>\\\n\\\n                Skeleton:\\\n                \"\"\",\\\n            }\\\n        ],\n    )\n\nasync def expand_point(question, skeleton, point_index):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                You’re responsible for continuing the writing of one and only one point in the overall answer to the following question.\\\n\\\n                <question>\\\n                {question}\\\n                </question>\\\n\\\n                The skeleton of the answer is:\\\n\\\n                <skeleton>\\\n                {skeleton}\\\n                </skeleton>\\\n\\\n                Continue and only continue the writing of point {point_index}.\\\n                Write it **very shortly** in 1∼2 sentence and do not continue with other points!\\\n                \"\"\",\\\n            }\\\n        ],\n    )\n\nasync def main():\n    query = \"Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.\"\n\n    # Step 1: Get the skeleton\n    skeleton = await get_skeleton(query)\n\n    for point in skeleton.points:\n        print(point)\n        #> index=1 description='Introduction to Hawaii trip'\n        #> index=2 description='Arrival and first impressions'\n        #> index=3 description='Traditional Hawaiian cuisine'\n        #> index=4 description='Exploring local markets'\n        #> index=5 description='Visit to historic sites'\n        #> index=6 description='Experience a Hawaiian luau'\n        #> index=7 description='Day at the beach'\n        #> index=8 description='Hiking adventures'\n        #> index=9 description='Scenic viewpoints'\n        #> index=10 description='Closing remarks and tips'\n\n    # Step 2: Expand on each point in parallel\n    tasks = [expand_point(query, skeleton, point.index) for point in skeleton.points]\n    responses = await asyncio.gather(*tasks)\n\n    for response in responses:\n        print(response.response)\n        \"\"\"\n        Hawaii—a paradise of golden beaches, lush landscapes, and vibrant culture—beckoned us with the promise of adventure and unforgettable experiences. Our journey began the moment we landed on this magical archipelago, ready to explore its unique blend of natural beauty and rich traditions.\n        \"\"\"\n        \"\"\"\n        The moment we landed in Hawaii, we were greeted with warm aloha spirit, lush tropical landscapes, and the gentle aroma of hibiscus flowers in the air.\n        \"\"\"\n        \"\"\"\n        The traditional Hawaiian cuisine was an exotic delight; from savoring the rich flavors of poke bowls to indulging in the sweet taste of haupia, every bite was a unique cultural experience.\n        \"\"\"\n        \"\"\"\n        Exploring local markets was a vibrant and delightful experience, where the air was filled with the scent of exotic fruits, freshly-made poke, and sounds of local musicians. We discovered unique handicrafts and interacted with friendly vendors eager to share their stories and traditions.\n        \"\"\"\n        \"\"\"\n        A visit to Pearl Harbor is a poignant reminder of the past, offering a chance to pay respects and learn about the events that shaped history. Walking through the USS Arizona Memorial and exploring the interactive exhibits was both humbling and enlightening.\n        \"\"\"\n        \"\"\"\n        Point 6: Experience a Hawaiian luau - Attending a traditional Hawaiian luau was unforgettable, filled with vibrant dances, soulful music, and a feast of mouthwatering dishes cooked in an imu (underground oven). It was a magical evening that immersed us in the heart of Hawaiian culture.\n        \"\"\"\n        \"\"\"\n        A day at the beach in Hawaii was pure bliss. The crystal-clear waters and soft sands were the perfect backdrop for both relaxation and adventure, from sunbathing to snorkeling.\n        \"\"\"\n        \"\"\"\n        Hiking adventures in Hawaii offer a unique chance to connect with nature, with trails leading to stunning waterfalls and lush rainforests. Don’t miss out on the Na Pali Coast's breathtaking hikes!\n        \"\"\"\n        \"\"\"\n        One of the highlights of my trip was visiting the scenic viewpoints such as the Na Pali Coast and Haleakalā National Park, offering breathtaking panoramic views that are perfect for photography aficionados and nature lovers alike.\n        \"\"\"\n        \"\"\"\n        As you plan your trip, don't forget to pack plenty of sunscreen and a camera to capture every magical moment. Hawaii offers a unique blend of relaxation and adventure that's sure to leave you with unforgettable memories.\n        \"\"\"\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/decomposition/skeleton_of_thought/\\#references \"Permanent link\")\n\n1: [Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation](https://arxiv.org/abs/2307.15337)\n\n\\*: [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/decomposition/skeleton_of_thought/",
      "ogUrl": "https://python.useinstructor.com/prompting/decomposition/skeleton_of_thought/",
      "title": "Generate in Parallel - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/decomposition/skeleton_of_thought/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/skeleton_of_thought.png",
      "ogTitle": "Generate in Parallel - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/skeleton_of_thought.png",
      "og:title": "Generate in Parallel - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/decomposition/skeleton_of_thought/",
      "statusCode": 200,
      "description": "Skelelton-of-Thought is a technique which prompts an LLM to generate a skeleton outline of the response, then completes each point in the skeleton in parallel.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Skelelton-of-Thought is a technique which prompts an LLM to generate a skeleton outline of the response, then completes each point in the skeleton in parallel.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/skeleton_of_thought.png",
      "twitter:title": "Generate in Parallel - Instructor",
      "og:description": "Skelelton-of-Thought is a technique which prompts an LLM to generate a skeleton outline of the response, then completes each point in the skeleton in parallel.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Skelelton-of-Thought is a technique which prompts an LLM to generate a skeleton outline of the response, then completes each point in the skeleton in parallel."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/#prompting-guide)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/index.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/index.md \"View source of this page\")\n\n# Prompting Guide [¶](https://python.useinstructor.com/prompting/\\#prompting-guide \"Permanent link\")\n\nPrompting requires an understanding of techniques to enhance model performance.\n\nThe team at [Learn Prompting](https://learnprompting.org) released The [Prompt Report](https://trigaten.github.io/Prompt_Survey_Site) in collaboration with researchers from OpenAI, Microsoft, and Google. This report surveys over 1,500 prompting papers and condenses the findings into a list of 58 distinct prompting techniques.\n\nHere are examples of the 58 prompting techniques\\* using `instructor`.\n\nPrompting techniques are separated into the following categories: - [Prompting Guide](https://python.useinstructor.com/prompting/#prompting-guide) \\- [Zero-Shot](https://python.useinstructor.com/prompting/#zero-shot) \\- [Few-Shot](https://python.useinstructor.com/prompting/#few-shot) \\- [Thought Generation](https://python.useinstructor.com/prompting/#thought-generation) \\- [Zero Shot](https://python.useinstructor.com/prompting/#zero-shot-1) \\- [Few Shot](https://python.useinstructor.com/prompting/#few-shot-1) \\- [Ensembling](https://python.useinstructor.com/prompting/#ensembling) \\- [Self-Criticism](https://python.useinstructor.com/prompting/#self-criticism) \\- [Decomposition](https://python.useinstructor.com/prompting/#decomposition)\n\nClick links to learn about each method and how to apply them in prompts.\n\n## Zero-Shot [¶](https://python.useinstructor.com/prompting/\\#zero-shot \"Permanent link\")\n\nHow do we increase the performance of our model without any examples?\n\n1. [Use Emotional Language](https://python.useinstructor.com/prompting/zero_shot/emotion_prompting/)\n2. [Assign a Role](https://python.useinstructor.com/prompting/zero_shot/role_prompting/)\n3. [Define a Style](https://python.useinstructor.com/prompting/zero_shot/style_prompting/)\n4. [Auto-Refine The Prompt](https://python.useinstructor.com/prompting/zero_shot/s2a/)\n5. [Simulate a Perspective](https://python.useinstructor.com/prompting/zero_shot/simtom/)\n6. [Clarify Ambiguous Information](https://python.useinstructor.com/prompting/zero_shot/rar/)\n7. [Ask Model To Repeat Query](https://python.useinstructor.com/prompting/zero_shot/re2/)\n8. [Generate Follow-Up Questions](https://python.useinstructor.com/prompting/zero_shot/self_ask/)\n\n## Few-Shot [¶](https://python.useinstructor.com/prompting/\\#few-shot \"Permanent link\")\n\nHow do we choose effective examples to include in our prompt?\n\n1. [Auto-Generate Examples](https://python.useinstructor.com/prompting/few_shot/example_generation/sg_icl/)\n2. [Re-Order Examples](https://python.useinstructor.com/prompting/few_shot/example_ordering/)\n3. [Choose Examples Similar to the Query (KNN)](https://python.useinstructor.com/prompting/few_shot/exemplar_selection/knn/)\n4. [Choose Examples Similar to the Query (Vote-K)](https://python.useinstructor.com/prompting/few_shot/exemplar_selection/vote_k/)\n\n## Thought Generation [¶](https://python.useinstructor.com/prompting/\\#thought-generation \"Permanent link\")\n\nHow do we encourage our model to mimic human-like reasoning?\n\n## Zero Shot [¶](https://python.useinstructor.com/prompting/\\#zero-shot-1 \"Permanent link\")\n\n1. [Auto-Generate Chain-Of-Thought Examples](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_zero_shot/analogical_prompting/)\n2. [First Ask a Higher-Level Question](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_zero_shot/step_back_prompting/)\n3. [Encourage Analysis](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_zero_shot/thread_of_thought/)\n4. [Encourage Structural Reasoning](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_zero_shot/tab_cot/)\n\n## Few Shot [¶](https://python.useinstructor.com/prompting/\\#few-shot-1 \"Permanent link\")\n\n1. [Annotate Only Uncertain Examples](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/active_prompt/)\n2. [Choose Diverse Examples](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/auto_cot/)\n3. [Choose Complex Examples](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/complexity_based/)\n4. [Include Incorrect Demonstrations](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/contrastive/)\n5. [Choose Similar, Auto-Generated, High-Certainty Chain-Of-Thought Reasonings](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/memory_of_thought/)\n6. [Choose the Most Certain Reasoning](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/uncertainty_routed_cot/)\n7. [Generate Template-Based Prompts](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/prompt_mining/)\n\n## Ensembling [¶](https://python.useinstructor.com/prompting/\\#ensembling \"Permanent link\")\n\nHow can we use multiple prompts and aggregate their responses?\n\n01. [Build a Set of Consistent, Diverse Examples](https://python.useinstructor.com/prompting/ensembling/cosp/)\n02. [Batch In-Context Examples](https://python.useinstructor.com/prompting/ensembling/dense/)\n03. [Verify Individual Reasoning Steps](https://python.useinstructor.com/prompting/ensembling/diverse/)\n04. [Maximize Information Between Input and Output](https://python.useinstructor.com/prompting/ensembling/max_mutual_information/)\n05. [Merge Multiple Chains-Of-Thought](https://python.useinstructor.com/prompting/ensembling/meta_cot/)\n06. [Use Specialized Experts](https://python.useinstructor.com/prompting/ensembling/more/)\n07. [Choose The Most Consistent Reasoning](https://python.useinstructor.com/prompting/ensembling/self_consistency/)\n08. [Choose The Most Consistent Reasioning (Universal)](https://python.useinstructor.com/prompting/ensembling/universal_self_consistency/)\n09. [Use Task-Specific Example Selection](https://python.useinstructor.com/prompting/ensembling/usp/)\n10. [Paraphrase The Prompt](https://python.useinstructor.com/prompting/ensembling/prompt_paraphrasing/)\n\n## Self-Criticism [¶](https://python.useinstructor.com/prompting/\\#self-criticism \"Permanent link\")\n\nHow can a model verify or critique its own response?\n\n1. [Generate Verification Questions](https://python.useinstructor.com/prompting/self_criticism/chain_of_verification/)\n2. [Ask If the Answer is Correct](https://python.useinstructor.com/prompting/self_criticism/self_calibration/)\n3. [Generate Feedback and Auto-Improve](https://python.useinstructor.com/prompting/self_criticism/self_refine/)\n4. [Score Multiple Candidate Solutions](https://python.useinstructor.com/prompting/self_criticism/self_verification/)\n5. [Reconstruct The Problem](https://python.useinstructor.com/prompting/self_criticism/reversecot/)\n6. [Generate Possible Steps](https://python.useinstructor.com/prompting/self_criticism/cumulative_reason/)\n\n## Decomposition [¶](https://python.useinstructor.com/prompting/\\#decomposition \"Permanent link\")\n\nHow can we break down complex problems? How do we solve subproblems?\n\n1. [Implement Subproblems As Functions](https://python.useinstructor.com/prompting/decomposition/decomp/)\n2. [Use Natural and Symbolic Language](https://python.useinstructor.com/prompting/decomposition/faithful_cot/)\n3. [Solve Increasingly Complex Subproblems](https://python.useinstructor.com/prompting/decomposition/least_to_most/)\n4. [Generate a Plan](https://python.useinstructor.com/prompting/decomposition/plan_and_solve/)\n5. [Use Code As Reasoning](https://python.useinstructor.com/prompting/decomposition/program_of_thought/)\n6. [Recursively Solve Subproblems](https://python.useinstructor.com/prompting/decomposition/recurs_of_thought/)\n7. [Generate a Skeleton](https://python.useinstructor.com/prompting/decomposition/skeleton_of_thought/)\n8. [Search Through Subproblems](https://python.useinstructor.com/prompting/decomposition/tree-of-thought/)\n\n\\*: [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/",
      "ogUrl": "https://python.useinstructor.com/prompting/",
      "title": "Comprehensive Guide to Prompting Techniques - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/index.png",
      "ogTitle": "Comprehensive Guide to Prompting Techniques - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/index.png",
      "og:title": "Comprehensive Guide to Prompting Techniques - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/",
      "statusCode": 200,
      "description": "Explore 58 effective prompting techniques categorized for enhanced model performance in AI prompts.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Explore 58 effective prompting techniques categorized for enhanced model performance in AI prompts.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/index.png",
      "twitter:title": "Comprehensive Guide to Prompting Techniques - Instructor",
      "og:description": "Explore 58 effective prompting techniques categorized for enhanced model performance in AI prompts.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Explore 58 effective prompting techniques categorized for enhanced model performance in AI prompts."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/zero_shot/style_prompting/#implementation)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/zero_shot/style_prompting.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/zero_shot/style_prompting.md \"View source of this page\")\n\n# Define A Style\n\nHow can we constrain model outputs through prompting alone?\n\nTo contrain a model's response to fit the boundaries of our task, we can specify a style.\n\nStylistic constraints can include:\n\n- **writing style**: write a _flowery_ poem\n- **tone**: write a _dramatic_ poem\n- **mood**: write a _happy_ poem\n- **genre**: write a _mystery_ poem\n\n## Implementation [¶](https://python.useinstructor.com/prompting/zero_shot/style_prompting/\\#implementation \"Permanent link\")\n\n```md-code__content\nimport instructor\nfrom pydantic import BaseModel\nimport openai\n\nclass Email(BaseModel):\n    subject: str\n    message: str\n\nclient = instructor.from_openai(openai.OpenAI())\n\ndef generate_email(subject, to, sender, tone):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                Write an email about {subject} to {to} from {sender}.\\\n                The email should be {tone}.\\\n                \"\"\",\\\n            }\\\n        ],\n        response_model=Email,\n    )\n\nif __name__ == \"__main__\":\n    email = generate_email(\n        subject=\"invitation to all-hands on Monday at 6pm\",\n        to=\"John Smith\",\n        sender=\"Jane Doe\",\n        tone=\"formal\",\n    )\n\n    print(email.subject)\n    #> Invitation to All-Hands Meeting\n    print(email.message)\n    \"\"\"\n    Dear Mr. Smith,\n\n    I hope this message finds you well. I am writing to formally invite you to our upcoming all-hands meeting scheduled for Monday at 6:00 PM. This meeting is an important opportunity for us to come together, discuss key updates, and align on our strategic goals.\n\n    Please confirm your availability at your earliest convenience. Your presence and contributions to the discussion would be greatly valued.\n\n    Thank you and I look forward to your confirmation.\n\n    Warm regards,\n\n    Jane Doe\n    \"\"\"\n\n```\n\n## Stylistic Constraint Examples [¶](https://python.useinstructor.com/prompting/zero_shot/style_prompting/\\#stylistic-constraint-examples \"Permanent link\")\n\n| Constraint | Possible Phrases |\n| --- | --- |\n| Writing Style | Functional, Flowery, Candid, Prosaic, Ornate, Poetic |\n| Tone | Dramatic, Humorous, Optimistic, Sad, Formal, Informal |\n| Mood | Angry, Fearful, Happy, Sad |\n| Genre | Historical Fiction, Literary Fiction, Science Fiction, Mystery, Dystopian, Horror |\n\nMore Stylistic Constraints\n\nTo see even more examples of these stylistic constraints and additional constraints ( **characterization**, **pacing**, and **plot**), check out [this](https://arxiv.org/abs/2302.09185) paper.\n\n## References [¶](https://python.useinstructor.com/prompting/zero_shot/style_prompting/\\#references \"Permanent link\")\n\n1: [Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints](https://arxiv.org/abs/2302.09185)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/zero_shot/style_prompting/",
      "ogUrl": "https://python.useinstructor.com/prompting/zero_shot/style_prompting/",
      "title": "Style Prompting - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/zero_shot/style_prompting/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/style_prompting.png",
      "ogTitle": "Style Prompting - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/style_prompting.png",
      "og:title": "Style Prompting - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/zero_shot/style_prompting/",
      "statusCode": 200,
      "description": "To contrain a model's response to fit the boundaries of our task, we can specify a style.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "To contrain a model's response to fit the boundaries of our task, we can specify a style.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/style_prompting.png",
      "twitter:title": "Style Prompting - Instructor",
      "og:description": "To contrain a model's response to fit the boundaries of our task, we can specify a style.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "To contrain a model's response to fit the boundaries of our task, we can specify a style."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/ensembling/universal_self_consistency/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/ensembling/universal_self_consistency.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/ensembling/universal_self_consistency.md \"View source of this page\")\n\n# Use LLMs to Combine Different Responses\n\nUniversal Self Consistency[1](https://arxiv.org/pdf/2311.17311) aims to extend self-consistency by using a second LLM model to judge the quality of individual responses. Therefore instead of choosing the final answer based on the most frequently occuring value among each reasoning chain, we instead prompt the model to choose the most consistent answer for us relative to the prompt.\n\n![](https://python.useinstructor.com/img/universal_self_consistency.png)\n\nThis enables us to support a greater variety of different response formats and answer, leading to greater diversity of outputs and hence higher accuracy.\n\nWe can implement this in `instructor` as seen below.\n\n```md-code__content\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field, ValidationInfo, field_validator\nimport instructor\nfrom textwrap import dedent\nimport asyncio\n\nclient = instructor.from_openai(AsyncOpenAI())\n\nclass Response(BaseModel):\n    chain_of_thought: str\n    answer: str\n\nclass SelectedResponse(BaseModel):\n    most_consistent_response_id: int = Field(\n        description=\"\"\"The ID of the most consistent response that\n        was provided\"\"\"\n    )\n\n    @field_validator(\"most_consistent_response_id\")\n    @classmethod\n    def validate_id(cls, v: int, info: ValidationInfo):\n        context = info.context\n        number_responses = context.get(\"number_responses\", float(\"inf\"))\n\n        if v > number_responses:\n            raise ValueError(\n                f\"\"\"Most consistent response ID {v} is greater than the\n                number of responses {number_responses}. Please return a\n                valid id between 0 and {number_responses-1}\"\"\"\n            )\n        return v\n\nasync def generate_response(query: str) -> Response:\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[{\"role\": \"user\", \"content\": query}],\n    )\n\nasync def generate_batch_responses(query: str, no_responses: int):\n    coros = [generate_response(query) for _ in range(no_responses)]\n    return await asyncio.gather(*coros)\n\nasync def select_consistent_response(responses: list[Response], query: str):\n    formatted_responses = \"\\n\".join(\n        [\\\n            f\"Response {idx}: {response.chain_of_thought}. {response.answer}\"\\\n            for idx, response in enumerate(responses)\\\n        ]\n    )\n\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=SelectedResponse,\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": dedent(\\\n                    f\"\"\"\\\n                <user query>\\\n                {query}\\\n                </user query>\\\n\\\n                {formatted_responses}\\\n\\\n                Evaluate these responses.\\\n                Select the most consistent response based on majority\\\n                consensus\\\n                \"\"\"\\\n                ),\\\n            }\\\n        ],\n        validation_context={\"number_responses\": len(responses)},\n    )\n\nif __name__ == \"__main__\":\n    query = \"\"\"The three-digit number 'ab5' is divisible by 3. How many different\n     three-digit numbers can 'ab5' represent?\"\"\"\n    responses = asyncio.run(generate_batch_responses(query, 3))\n\n    for response in responses:\n        print(response.model_dump_json(indent=2))\n        \"\"\"\n        {\n          \"chain_of_thought\": \"A number is divisible by 3 if\n          the sum of its digits is divisible by 3. Given the\n          number 'ab5', we need to check how many different\n          values of 'a' and 'b', where both are digits (0-9)\n          can make the sum divisible by 3.\\n\\nThe sum of the\n          digits is a + b + 5.\\n\\nWe need to find pairs (a, b)\n          such that (a + b + 5) % 3 == 0.\",\n          \"answer\": \"30\"\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"chain_of_thought\": \"A number is divisible by 3 if\n          the sum of its digits is divisible by 3. Let's\n          denote the digits a and b. The number 'ab5' has\n          digits a, b, and 5. Therefore, the sum of the\n          digits is a + b + 5. Since the number is divisible\n          by 3, a + b + 5 must be divisible by 3.\\n\\nNow,\n          since a and b are single digits (0-9), we need to\n          find pairs (a, b) such that a + b + 5 is divisible\n          by 3. We will evaluate all possible combinations of\n          values for a and b to count how many valid pairs\n          (a, b) exist.\\n\\nLet's start by considering b's\n          values:\\n1. If b = 0, then a + 5 must be divisible\n          by 3.\\n2. If b = 1, then a + 6 must be divisible by\n          3.\\n3. If b = 2, then a + 7 must be divisible by\n          3.\\n4. If b = 3, then a + 8 must be divisible by\n          3.\\n5. If b = 4, then a + 9 must be divisible by\n          3.\\n6. If b = 5, then a + 10 must be divisible by\n          3.\\n7. If b = 6, then a + 11 must be divisible by\n          3.\\n8. If b = 7, then a + 12 must be divisible by\n          3.\\n9. If b = 8, then a + 13 must be divisible by\n          3.\\n10. If b = 9, then a + 14 must be divisible by\n          3.\\n\\nWe will find all corresponding a values for\n          each b and count the valid combinations.\\n\",\n          \"answer\": \"There are 30 different three-digit\n          numbers that 'ab5' can represent.\"\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"chain_of_thought\": \"A number is divisible by 3 if\n          the sum of its digits is divisible by 3. The given\n          number is in the form 'ab5', where 'a' and 'b' are\n          digits from 0 to 9. To find the total number of\n          different three-digit numbers that 'ab5' can\n          represent, we need to determine all possible digit\n          combinations for 'a' and 'b' such that 'a + b + 5'\n          is divisible by 3.\",\n          \"answer\": \"30\"\n        }\n        \"\"\"\n\n    selected_response = asyncio.run(select_consistent_response(responses, query))\n    print(selected_response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"most_consistent_response_id\": 0\n    }\n    \"\"\"\n\n    print(\n        responses[selected_response.most_consistent_response_id].model_dump_json(\n            indent=2\n        )\n    )\n    \"\"\"\n    {\n      \"chain_of_thought\": \"A number is divisible by 3 if the sum of its digits is divisible by 3. Given the number 'ab5', we need to\n      check how many different values of 'a' and 'b', where both are digits (0-9) can make the sum divisible by 3.\\n\\nThe sum of the\n      digits is a + b + 5.\\n\\nWe need to find pairs (a, b) such that (a + b + 5) % 3 == 0.\",\n      \"answer\": \"30\"\n    }\n    \"\"\"\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/ensembling/universal_self_consistency/\\#references \"Permanent link\")\n\n1: [Universal Self-Consistency For Large Language Model Generation](https://arxiv.org/pdf/2311.17311)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/ensembling/universal_self_consistency/",
      "ogUrl": "https://python.useinstructor.com/prompting/ensembling/universal_self_consistency/",
      "title": "Use LLMs to Combine Different Responses - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/ensembling/universal_self_consistency/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/universal_self_consistency.png",
      "ogTitle": "Use LLMs to Combine Different Responses - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/universal_self_consistency.png",
      "og:title": "Use LLMs to Combine Different Responses - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/ensembling/universal_self_consistency/",
      "statusCode": 200,
      "description": "Universal Self Consistency aims to extend Self-Consistency by using Large Language Models themselves to select the most consistent answer among multiple candidates",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Universal Self Consistency aims to extend Self-Consistency by using Large Language Models themselves to select the most consistent answer among multiple candidates",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/universal_self_consistency.png",
      "twitter:title": "Use LLMs to Combine Different Responses - Instructor",
      "og:description": "Universal Self Consistency aims to extend Self-Consistency by using Large Language Models themselves to select the most consistent answer among multiple candidates",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Universal Self Consistency aims to extend Self-Consistency by using Large Language Models themselves to select the most consistent answer among multiple candidates"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/decomposition/faithful_cot/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/decomposition/faithful_cot.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/decomposition/faithful_cot.md \"View source of this page\")\n\n# Leverage Task Specific Systems\n\nFaithful Chain of Thought[1](https://arxiv.org/pdf/2301.13379) improves the faithfulness of reasoning chains generated by Language Models by breaking it up into two stages\n\n1. **Translation** : We first translate a user query into a series of reasoning steps. These are a task specific set of steps that we can execute deterministically.\n2. **Problem Solving**: We execute our steps and arrive at a final answer that we can derive. This ensures that our Chain Of Thought is able to derive a answer that is consistent with the reasoning steps.\n\nThey list a few examples in the paper of what these task-specific steps could be\n\n1. **Math Word Problems** : Python Code that can be executed by an interpreter to derive a final answer\n2. **Multi-Hop QA** : This is a multi-step reasoning process. To solve this, they use a mix of python and Datalog ( which is a relation and log programming language ) to arrive at a final answer\n3. **Planning** : When trying to generate a plan to solve a user query, they generate a list of symbolic goals in a Programming Language and then call a PDDL Planner to obtain a plan to solve the user's query\n\n![](https://python.useinstructor.com/img/faithful_cot_example.png)\n\nIn the example below, we show how you can use a LLM to generate python code that can be executed by an Interpreter to arrive at a final answer.\n\nWe can implement it in `instructor` as seen below\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(OpenAI())\n\nclass ReasoningStep(BaseModel):\n    id: int = Field(description=\"Unique ID\")\n    rationale: list[str] = Field(\n        description=\"\"\"Specific sections from prior reasoning\n        steps or the context that ground this reasoning step\"\"\"\n    )\n    dependencies: list[int] = Field(\n        description=\"\"\"IDs of prior reasoning steps that this\n        reasoning step depends on\"\"\"\n    )\n    eval_string: str = Field(\n        description=\"\"\"Python Code to execute to generate the\n        final evaluation\"\"\"\n    )\n\ndef generate_reasoning_steps(query: str) -> list[ReasoningStep]:\n    return client.chat.completions.create(\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"\"\"\\\n                You are a world class AI who excels at\\\n                generating reasoning steps to answer a\\\n                question. You will be given a question\\\n                and you will generate a list of reasoning\\\n                steps that are needed to answer the\\\n                question.\\\n\\\n                At each point you should either\\\n                - declare a variable to be referenced\\\n                later on\\\n                - combine multiple variables together to\\\n                generate a new result that you should\\\n                store in another variable\\\n\\\n                The final answer should be stored in a\\\n                variable called `answer`.\\\n                \"\"\",\\\n            },\\\n            {\"role\": \"user\", \"content\": query},\\\n        ],\n        model=\"gpt-4o\",\n        response_model=list[ReasoningStep],\n    )\n\nif __name__ == \"__main__\":\n    steps = generate_reasoning_steps(\n        \"\"\"If there are 3 cars in the parking lot and 2 more\n        cars arrive, how many cars are in the parking lot\n        after another 2 more arrive?\"\"\"\n    )\n\n    code = \"\\n\".join([step.eval_string for step in steps])\n    print(code)\n    \"\"\"\n    initial_cars = 3\n    arriving_cars = 2\n    cars_after_first_arrival = initial_cars + arriving_cars\n    final_car_count = cars_after_first_arrival + 2\n    answer = final_car_count\n    \"\"\"\n    exec(code)\n\n    local_vars = {}\n    exec(code, {}, local_vars)\n    print(local_vars.get(\"answer\"))\n    #> 7\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/decomposition/faithful_cot/\\#references \"Permanent link\")\n\n1: [Faithful Chain-of-Thought Reasoning](https://arxiv.org/pdf/2301.13379)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/decomposition/faithful_cot/",
      "ogUrl": "https://python.useinstructor.com/prompting/decomposition/faithful_cot/",
      "title": "Leverage Task Specific Systems - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/decomposition/faithful_cot/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/faithful_cot.png",
      "ogTitle": "Leverage Task Specific Systems - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/faithful_cot.png",
      "og:title": "Leverage Task Specific Systems - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/decomposition/faithful_cot/",
      "statusCode": 200,
      "description": "Faithful Chain of Thought aims to use multiple reasoning steps to improve the quality of the final outputs",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Faithful Chain of Thought aims to use multiple reasoning steps to improve the quality of the final outputs",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/faithful_cot.png",
      "twitter:title": "Leverage Task Specific Systems - Instructor",
      "og:description": "Faithful Chain of Thought aims to use multiple reasoning steps to improve the quality of the final outputs",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Faithful Chain of Thought aims to use multiple reasoning steps to improve the quality of the final outputs"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/zero_shot/simtom/#implementation)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/zero_shot/simtom.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/zero_shot/simtom.md \"View source of this page\")\n\n# Simulate A Perspective\n\nHow can we encourage the model to focus on relevant information?\n\nSimToM (Simulated Theory of Mind) is a two-step prompting technique that encourages a model to consider a specific perspective.\n\nThis can be useful for complex questions with multiple entities. For example, if the prompt contains information about two individuals, we can ask the model to answer our query from the perspective of one of the individuals.\n\nThis is implemented in two steps. Given an entity:\n\n1. Identify and isolate information relevant to the entity\n2. Ask the model to answer the query from the entity's perspective\n\nSample Template\n\n**Step 1**: Given the following context, list the facts that < _entity_ \\> would know. Context: < _context_ >\n\n**Step 2**: You are < _entity_ >. Answer the following question based only on these facts you know: < _facts_ >. Question: < _query_ >\n\n## Implementation [¶](https://python.useinstructor.com/prompting/zero_shot/simtom/\\#implementation \"Permanent link\")\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel, Field\nfrom typing import Iterable\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass KnownFact(BaseModel):\n    fact: str = Field(description=\"A fact that the given entity would know\")\n\nclass Response(BaseModel):\n    location: str\n\ndef generate_known_facts(entity, context, query) -> Iterable[KnownFact]:\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Iterable[KnownFact],\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"Given the following context, list\\\n                the facts that {entity} would know:\\\n\\\n                Context:\\\n                {context}\\\n                {query}\\\n\\\n                List only the facts relevant to {entity}.\\\n                \"\"\",\\\n            }\\\n        ],\n    )\n\ndef answer_question_based_on_facts(entity, query, known_facts) -> Response:\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"\"\"You are {entity}. Answer the following question\\\n                based only on these facts you know:\\\n                {\" \".join([str(fact) for fact in known_facts])}\"\"\",\\\n            },\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"Question: {query}\",\\\n            },\\\n        ],\n    )\n\nif __name__ == \"__main__\":\n    entity = \"Alice\"\n    context = \"\"\"Alice puts the book on the table.\n        Alice leaves the room.\n        Bob moves the book to the shelf.\n        \"\"\"\n    query = f\"Where does {entity} think the book is?\"\n\n    known_facts = generate_known_facts(entity, context, query)\n    response = answer_question_based_on_facts(entity, query, known_facts)\n\n    for fact in known_facts:\n        print(fact)\n        #> fact='Alice puts the book on the table.'\n        #> fact='Alice leaves the room. Bob moves the book to the shelf.'\n    print(response.location)\n    #> On the table\n\n```\n\n## References [¶](https://python.useinstructor.com/prompting/zero_shot/simtom/\\#references \"Permanent link\")\n\n1: [Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities](https://arxiv.org/abs/2311.10227)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/zero_shot/simtom/",
      "ogUrl": "https://python.useinstructor.com/prompting/zero_shot/simtom/",
      "title": "SimToM (Simulated Theory of Mind) - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/zero_shot/simtom/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/simtom.png",
      "ogTitle": "SimToM (Simulated Theory of Mind) - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/simtom.png",
      "og:title": "SimToM (Simulated Theory of Mind) - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/zero_shot/simtom/",
      "statusCode": 200,
      "description": "SimToM (Simulated Theory of Mind) is a two-step prompting technique that encourages a model to consider a specific perspective.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "SimToM (Simulated Theory of Mind) is a two-step prompting technique that encourages a model to consider a specific perspective.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/simtom.png",
      "twitter:title": "SimToM (Simulated Theory of Mind) - Instructor",
      "og:description": "SimToM (Simulated Theory of Mind) is a two-step prompting technique that encourages a model to consider a specific perspective.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "SimToM (Simulated Theory of Mind) is a two-step prompting technique that encourages a model to consider a specific perspective."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/ensembling/prompt_paraphrasing/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/ensembling/prompt_paraphrasing.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/ensembling/prompt_paraphrasing.md \"View source of this page\")\n\n# Use Translation for Paraphrasing\n\nLarge Language Models are sensitive to the way that they are prompted. When prompted incorrectly, they might perform much worse despite having the information or capability to respond to the prompt. We can help find semantically similar prompts by performing back translation - where we translate our prompts to another language and back to encourage more diversity in the rephrased prompts.\n\nPrompt paraphrasing [1](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00324/96460/How-Can-We-Know-What-Language-Models-Know). provides some ways for us to improve on the phrasing of our prompts to do so.\n\nWe can implement this using `instructor` as seen below.\n\n```md-code__content\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\nimport random\n\nclient = instructor.from_openai(AsyncOpenAI())\n\nclass TranslatedPrompt(BaseModel):\n    translation: str\n\nasync def translate_prompt(prompt: str, from_language: str, to_language: str):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"\"\"\\\n                You are an expert translation assistant.\\\n                You are going to be given a prompt and\\\n                asked to translate it from {from_language}\\\n                to {to_language}. Paraphrase and use\\\n                synonyms where possible, especially for\\\n                the examples.\\\n                \"\"\",\\\n            },\\\n            {\"role\": \"user\", \"content\": f\"Prompt: {prompt}\"},\\\n        ],\n        response_model=TranslatedPrompt,\n    )\n\nasync def generate_permutation(prompt: str, language: str) -> str:\n    tranlated_prompt = await translate_prompt(prompt, \"english\", language)\n    backtranslated_prompt = await translate_prompt(\n        tranlated_prompt.translation, language, \"english\"\n    )\n    return backtranslated_prompt.translation\n\nasync def generate_prompts(\n    prompt: str, languages: list[str], permutations: int\n) -> list[str]:\n    coros = [\\\n        generate_permutation(prompt, random.choice(languages))\\\n        for _ in range(permutations)\\\n    ]\n    return await asyncio.gather(*coros)\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    prompt = \"\"\"\n    You are an expert system that excels at Sentiment\n    Analysis of User Reviews.\n\n    Here are a few examples to refer to:\n\n    1. That was a fantastic experience I had! I'm\n    definitely recommending this to all my friends\n    // Positive\n    2. I think it was a passable evening. I don't think\n    there was anything remarkable or off-putting for me.\n    // Negative\n    3. I'm horrified at the state of affairs in this new\n    restaurant // Negative\n\n    Sentence: This was a fantastic experience!\n    \"\"\"\n    languages = [\"french\", \"spanish\", \"chinese\"]\n    permutations = 2\n\n    generated_prompts = asyncio.run(generate_prompts(prompt, languages, permutations))\n    for prompt in generated_prompts:\n        print(prompt)\n        \"\"\"\n        You are an expert system specializing in user review sentiment analysis. Here are a few examples to guide you: 1. It was an exceptional experience! I will definitely recommend it to all my friends // Positive 2. I think it was a mediocre evening. There wasn't anything outstanding or particularly bad for me // Negative 3. I am horrified by the condition of things in this new restaurant // Negative Sentence: It was an amazing experience!\n        \"\"\"\n        \"\"\"\n        You are an expert system that excels in User Review Sentiment Analysis.\n\n        Here are some reference examples:\n\n        1. I had an amazing experience! I will definitely recommend it to all my friends.\n        // Positive\n        2. I think it was an average evening. I don’t believe there was anything remarkable or unpleasant about it for me.\n        // Negative\n        3. I am horrified by the situation at this new restaurant.\n        // Negative\n\n        Sentence: This was a fantastic experience!\n        \"\"\"\n        \"\"\"\n        You are an expert system skilled in conducting user\n        review sentiment analysis.\n\n        Here are some examples for reference:\n\n        1. That was an awesome experience! I'll definitely\n        recommend it to all my friends // Positive\n        2. I think it was an okay evening. I don't find\n        anything particularly outstanding or unpleasant.\n        // Neutral\n        3. I am very shocked by the condition of this new\n        restaurant // Negative\n\n        Sentence: This was a wonderful experience!\n        \"\"\"\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/ensembling/prompt_paraphrasing/\\#references \"Permanent link\")\n\n1: [How Can We Know What Language Models Know?](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00324/96460/How-Can-We-Know-What-Language-Models-Know)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/ensembling/prompt_paraphrasing/",
      "ogUrl": "https://python.useinstructor.com/prompting/ensembling/prompt_paraphrasing/",
      "title": "Use Translation for Paraphrasing - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/ensembling/prompt_paraphrasing/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/prompt_paraphrasing.png",
      "ogTitle": "Use Translation for Paraphrasing - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/prompt_paraphrasing.png",
      "og:title": "Use Translation for Paraphrasing - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/ensembling/prompt_paraphrasing/",
      "statusCode": 200,
      "description": "Use Large Language Models to perform back translation in order to improve prompt performance",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Use Large Language Models to perform back translation in order to improve prompt performance",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/prompt_paraphrasing.png",
      "twitter:title": "Use Translation for Paraphrasing - Instructor",
      "og:description": "Use Large Language Models to perform back translation in order to improve prompt performance",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Use Large Language Models to perform back translation in order to improve prompt performance"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/zero_shot/role_prompting/#implementation)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/zero_shot/role_prompting.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/zero_shot/role_prompting.md \"View source of this page\")\n\n# Assign a Role\n\nHow can we increase a model's performance on open-ended tasks?\n\nRole prompting, or persona prompting, assigns a role to the model. Roles can be:\n\n- **specific to the query**: _You are a talented writer. Write me a poem._\n- **general/social**: _You are a helpful AI assistant. Write me a poem._\n\n## Implementation [¶](https://python.useinstructor.com/prompting/zero_shot/role_prompting/\\#implementation \"Permanent link\")\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass Response(BaseModel):\n    poem: str\n\ndef role_prompting(query, role):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"{role} {query}\",\\\n            },\\\n        ],\n    )\n\nif __name__ == \"__main__\":\n    query = \"Write me a short poem about coffee.\"\n    role = \"You are a renowned poet.\"\n\n    response = role_prompting(query, role)\n    print(response.poem)\n    \"\"\"\n    In the morning's gentle light,\n    A brew of warmth, dark and bright.\n    Awakening dreams, so sweet,\n    In every sip, the day we greet.\n\n    Through the steam, stories spin,\n    A liquid muse, caffeine within.\n    Moments pause, thoughts unfold,\n    In coffee's embrace, we find our gold.\n    \"\"\"\n\n```\n\nMore Role Prompting\n\nTo read about a systematic approach to choosing roles, check out [RoleLLM](https://arxiv.org/abs/2310.00746).\n\nFor more examples of social roles, check out [this](https://arxiv.org/abs/2311.10054) evaluation of social roles in system prompts..\n\nTo read about using more than one role, check out [Multi-Persona Self-Collaboration](https://arxiv.org/abs/2307.05300).\n\n## References [¶](https://python.useinstructor.com/prompting/zero_shot/role_prompting/\\#references \"Permanent link\")\n\n1: [RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Lanuage Models](https://arxiv.org/abs/2310.00746)\n\n2: [Is \"A Helpful Assistant\" the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts](https://arxiv.org/abs/2311.10054)\n\n3: [Unleashing the Emergent Cognitive Synergy in Large Lanuage Models: A Task-Solving Agent through Multi-Persona Self-Collaboration](https://arxiv.org/abs/2307.05300)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/zero_shot/role_prompting/",
      "ogUrl": "https://python.useinstructor.com/prompting/zero_shot/role_prompting/",
      "title": "Role Prompting - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/zero_shot/role_prompting/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/role_prompting.png",
      "ogTitle": "Role Prompting - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/role_prompting.png",
      "og:title": "Role Prompting - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/zero_shot/role_prompting/",
      "statusCode": 200,
      "description": "Role prompting, or persona prompting, assigns a role to the model.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Role prompting, or persona prompting, assigns a role to the model.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/role_prompting.png",
      "twitter:title": "Role Prompting - Instructor",
      "og:description": "Role prompting, or persona prompting, assigns a role to the model.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Role prompting, or persona prompting, assigns a role to the model."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/zero_shot/s2a/#implementation)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/zero_shot/s2a.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/zero_shot/s2a.md \"View source of this page\")\n\n# Auto-Refine The Prompt\n\nHow do we remove irrelevant information from the prompt?\n\nThe S2A (System 2 Attention) technique auto-refines a prompt by asking the model to rewrite the prompt to include only _relevant_ information. We implement this in two steps:\n\n1. Ask the model to rewrite the prompt\n2. Pass the rewritten prompt back to the model\n\n## Implementation [¶](https://python.useinstructor.com/prompting/zero_shot/s2a/\\#implementation \"Permanent link\")\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass Step1(BaseModel):\n    relevant_context: str = Field(..., description=\"Relevant context\")\n    user_query: str = Field(..., description=\"The question from the user\")\n\nclass Step2(BaseModel):\n    answer: int\n\ndef rewrite_prompt(query):\n    rewritten_prompt = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Step1,\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                    Given the following text by a user, extract the part\\\n                    that is actually relevant to their question. Please\\\n                    include the actual question or query that the user\\\n                    is asking.\\\n\\\n                    Text by user:\\\n                    {query}\\\n                    \"\"\",\\\n            }\\\n        ],\n    )\n    return rewritten_prompt\n\ndef generate_final_response(rewritten_prompt):\n    final_response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Step2,\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"{rewritten_prompt.relevant_context}\\\n                    Question: {rewritten_prompt.user_query}\"\"\",\\\n            }\\\n        ],\n    )\n    return final_response\n\nif __name__ == \"__main__\":\n    query = \"\"\"Mary has 3 times as much candy as Megan.\n        Mary then adds 10 more pieces of candy to her collection.\n        Max is 5 years older than Mary.\n        If Megan has 5 pieces of candy, how many does Mary have in total?\n        \"\"\"\n\n    # Step 1: Rewrite the prompt\n    rewritten_prompt = rewrite_prompt(query)\n    print(rewritten_prompt.relevant_context)\n    \"\"\"\n    Mary has 3 times as much candy as Megan. Mary then adds 10 more pieces of candy to her collection. If Megan has 5 pieces of candy, how many does Mary have in total?\n    \"\"\"\n    print(rewritten_prompt.user_query)\n    #> how many does Mary have in total?\n\n    # Step 2: Generate the final response\n    final_response = generate_final_response(rewritten_prompt)\n    print(final_response.answer)\n    #> 25\n\n```\n\n## References [¶](https://python.useinstructor.com/prompting/zero_shot/s2a/\\#references \"Permanent link\")\n\n1: [System 2 Attention (is something you might need too)](https://arxiv.org/abs/2311.11829)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/zero_shot/s2a/",
      "ogUrl": "https://python.useinstructor.com/prompting/zero_shot/s2a/",
      "title": "System 2 Attention (S2A) - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/zero_shot/s2a/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/s2a.png",
      "ogTitle": "System 2 Attention (S2A) - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/s2a.png",
      "og:title": "System 2 Attention (S2A) - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/zero_shot/s2a/",
      "statusCode": 200,
      "description": "The S2A (System 2 Attention) technique auto-refines a prompt by asking the model to rewrite the prompt to include only relevant information.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "The S2A (System 2 Attention) technique auto-refines a prompt by asking the model to rewrite the prompt to include only relevant information.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/s2a.png",
      "twitter:title": "System 2 Attention (S2A) - Instructor",
      "og:description": "The S2A (System 2 Attention) technique auto-refines a prompt by asking the model to rewrite the prompt to include only relevant information.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "The S2A (System 2 Attention) technique auto-refines a prompt by asking the model to rewrite the prompt to include only relevant information."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/zero_shot/emotion_prompting/#implementation)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/zero_shot/emotion_prompting.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/zero_shot/emotion_prompting.md \"View source of this page\")\n\n# Use Emotional Language\n\nDo language models respond to emotional stimuli?\n\nAdding phrases with emotional significance to humans can help enhance the performance of a language model. This includes phrases such as:\n\n- This is very important to my career.\n- Take pride in your work.\n- Are you sure?\n\nInfo\n\nFor more examples of emotional stimuli to use in prompts, look into [EmotionPrompt](https://arxiv.org/abs/2307.11760) \\-\\- a set of prompts inspired by well-established human psychological phenomena.\n\n## Implementation [¶](https://python.useinstructor.com/prompting/zero_shot/emotion_prompting/\\#implementation \"Permanent link\")\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel\nfrom typing import Iterable\n\nclass Album(BaseModel):\n    name: str\n    artist: str\n    year: int\n\nclient = instructor.from_openai(openai.OpenAI())\n\ndef emotion_prompting(query, stimuli):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Iterable[Album],\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                {query}\\\n                {stimuli}\\\n                \"\"\",\\\n            }\\\n        ],\n    )\n\nif __name__ == \"__main__\":\n    query = \"Provide me with a list of 3 musical albums from the 2000s.\"\n    stimuli = \"This is very important to my career.\"\n\n    albums = emotion_prompting(query, stimuli)\n\n    for album in albums:\n        print(album)\n        #> name='Kid A' artist='Radiohead' year=2000\n        #> name='The Marshall Mathers LP' artist='Eminem' year=2000\n        #> name='The College Dropout' artist='Kanye West' year=2004\n\n```\n\n## References [¶](https://python.useinstructor.com/prompting/zero_shot/emotion_prompting/\\#references \"Permanent link\")\n\n1: [Large Language Models Understand and Can be Enhanced by Emotional Stimuli](https://arxiv.org/abs/2307.11760)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/zero_shot/emotion_prompting/",
      "ogUrl": "https://python.useinstructor.com/prompting/zero_shot/emotion_prompting/",
      "title": "Emotion Prompting - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/zero_shot/emotion_prompting/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/emotion_prompting.png",
      "ogTitle": "Emotion Prompting - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/emotion_prompting.png",
      "og:title": "Emotion Prompting - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/zero_shot/emotion_prompting/",
      "statusCode": 200,
      "description": "Adding phrases with emotional significance to humans can help enhance the performance of a language model.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Adding phrases with emotional significance to humans can help enhance the performance of a language model.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/emotion_prompting.png",
      "twitter:title": "Emotion Prompting - Instructor",
      "og:description": "Adding phrases with emotional significance to humans can help enhance the performance of a language model.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Adding phrases with emotional significance to humans can help enhance the performance of a language model."
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/ensembling/more.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/ensembling/more.md \"View source of this page\")\n\n# Combine Different Specialized LLMs\n\nLanguage Models struggle to generalize across question types that require distinct reasoning abilities. By combining a variety of different specialized language models, we can improve the quality of our responses. This is done through a technique called Mixture Of Reasoning Experts (MoRE).\n\nIn the original paper, they utilise four different experts\n\n1. Factual Expert : This is a model that is augmented by a RAG prompting pipeline. WHen it recieves a query, it retrieves the top 10 most relevant passages from Wikipedia and appends them to the prompt right before the question.\n\n2. Multihop Expert : This is an expert that has manually written rationales after each demo to elicit multi-step reasoning processes for the questions\n\n3. Math Expert : This is an expert that has manually written explanations for the GSM8k Dataset to bias the model towards different reasoning steps\n\n4. Commonsense expert: This is an expert that is provided with 10 different facts that are generated by a Codex model which are appended to the prompt right before the question\n\n\n![](https://python.useinstructor.com/img/more.png)\n\nOnce each expert has genearted a response, they then use a random forest classifier to score it from 0 to 1. This is then used for selecting the final answer and determining if we've generated a sufficiently good answer ( Since we have the option to abstain at each point )\n\nWe can implement a simplified version of MoRE with `instructor` with a few modifications.\n\n```md-code__content\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom textwrap import dedent\n\nclient = instructor.from_openai(OpenAI())\n\nclass MultihopExpert(BaseModel):\n    chain_of_thought: str\n    answer: str\n\nclass FactualExpert(BaseModel):\n    answer: str\n\nclass ModelScore(BaseModel):\n    score: float = Field(ge=0, lt=1)\n\ndef query_factual_expert(query: str, evidence: list[str]):\n    formatted_evidence = \"\\n-\".join(evidence)\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=FactualExpert,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": dedent(\\\n                    f\"\"\"\\\n                <query>\\\n                {query}\\\n                </query>\\\n\\\n                <evidences>\\\n                {formatted_evidence}\\\n                </evidences>\\\n                \"\"\"\\\n                ),\\\n            }\\\n        ],\n    )\n\ndef query_multihop_expert(query: str):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=MultihopExpert,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": dedent(\\\n                    f\"\"\"\\\n                <query>\\\n                {query}\\\n                </query>\\\n                \"\"\"\\\n                ),\\\n            }\\\n        ],\n    )\n\ndef score_answer(query: str, answer: str):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ModelScore,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"\"\"You are a helpful assistant that scores\\\n                answers based on well they are able to answer a\\\n                specific user query\"\"\",\\\n            },\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                <user query>\\\n                {query}\\\n                </user query>\\\n\\\n                <response>\\\n                {answer}\\\n                </response>\\\n                \"\"\",\\\n            },\\\n        ],\n    )\n\nif __name__ == \"__main__\":\n    query = \"\"\"Who's the original singer of Help Me Make It\n    Through The Night?\"\"\"\n    evidences = [\\\n        \"\"\"Help Me Make It Through The Night is a country\\\n        music ballad written and composed by Kris Kristofferson\\\n        and released on his 1970 album 'Kristofferson'\"\"\"\\\n    ]\n\n    threshold = 0.8\n\n    factual_expert_output = query_factual_expert(query, evidences)\n    print(factual_expert_output.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"answer\": \"The original singer of 'Help Me Make It Through the\n      Night' is Kris Kristofferson, who released it on his 1970 album\n      'Kristofferson'.\"\n    }\n    \"\"\"\n\n    multihop_expert_output = query_multihop_expert(query)\n    print(multihop_expert_output.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"chain_of_thought\": \"To identify the original singer of 'Help Me\n      Make It Through The Night,' I need to look for the person who\n      first recorded and released the song.\",\n      \"answer\": \"The original singer of 'Help Me Make It Through\n      The Night' is Kris Kristofferson.\"\n    }\n    \"\"\"\n\n    factual_expert_score = score_answer(query, factual_expert_output.answer)\n    multihop_expert_score = score_answer(query, multihop_expert_output.answer)\n\n    if max(factual_expert_score.score, multihop_expert_score.score) < threshold:\n        answer = \"Abstaining from responding\"\n    elif factual_expert_score.score > multihop_expert_score.score:\n        answer = factual_expert_output.answer\n    else:\n        answer = multihop_expert_output.answer\n\n    print(answer)\n    \"\"\"\n    The original singer of 'Help Me Make It Through the Night' is Kris\n    Kristofferson, who released it on his 1970 album 'Kristofferson'.\n    \"\"\"\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/ensembling/more/",
      "ogUrl": "https://python.useinstructor.com/prompting/ensembling/more/",
      "title": "Combine Different Specialized LLMs - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/ensembling/more/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/more.png",
      "ogTitle": "Combine Different Specialized LLMs - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/more.png",
      "og:title": "Combine Different Specialized LLMs - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/ensembling/more/",
      "statusCode": 200,
      "description": "MoRE creates a set of diverse reasoning experts by using different specialized prompts for different reasoning types. THe best answer from all experts is then selected using an agreement score",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "MoRE creates a set of diverse reasoning experts by using different specialized prompts for different reasoning types. THe best answer from all experts is then selected using an agreement score",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/more.png",
      "twitter:title": "Combine Different Specialized LLMs - Instructor",
      "og:description": "MoRE creates a set of diverse reasoning experts by using different specialized prompts for different reasoning types. THe best answer from all experts is then selected using an agreement score",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "MoRE creates a set of diverse reasoning experts by using different specialized prompts for different reasoning types. THe best answer from all experts is then selected using an agreement score"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/ensembling/meta_cot/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/ensembling/meta_cot.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/ensembling/meta_cot.md \"View source of this page\")\n\n# Combine Multiple Reasoning Chains\n\nMeta Chain Of Thought (Meta COT) [1](https://arxiv.org/pdf/2304.13007). involves the use of multiple reasoning chains to generate a response to a given query. This helps our model evaluate multiple potential reasoning paths and from there, determine a more accurate answer.\n\nWe can implement this using `instructor` as seen below.\n\n```md-code__content\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field\nimport asyncio\nfrom typing import Optional\n\nclient = instructor.from_openai(AsyncOpenAI())\n\nclass ReasoningAndResponse(BaseModel):\n    intermediate_reasoning: str = Field(\n        description=\"\"\"\n    Intermediate reasoning steps\"\"\"\n    )\n    correct_answer: str\n\nclass MaybeResponse(BaseModel):\n    result: Optional[ReasoningAndResponse]\n    error: Optional[bool]\n    error_message: Optional[str] = Field(\n        description=\"\"\"Informative explanation of why\n        the reasoning chain was unable to generate\n        a result\"\"\"\n    )\n\nclass QueryDecomposition(BaseModel):\n    queries: list[str] = Field(\n        description=\"\"\"A list of queries that need to be\n        answered in order to derive the final answer\"\"\"\n    )\n\nasync def generate_queries(query: str):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"\"\"You are a helpful assistant that\\\n                decomposes a query into multiple sub-queries.\"\"\",\\\n            },\\\n            {\"role\": \"user\", \"content\": query},\\\n        ],\n        response_model=QueryDecomposition,\n    )\n\nasync def generate_reasoning_chain(query: str) -> MaybeResponse:\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"\"\"\\\n                Given a question and a context,\\\n                answer the question step-by-step.\\\n\\\n                Indicate the intermediate reasoning\\\n                steps.\\\n                \"\"\",\\\n            },\\\n            {\"role\": \"user\", \"content\": query},\\\n        ],\n        response_model=MaybeResponse,\n    )\n\nasync def batch_reasoning_chains(\n    queries: list[str],\n) -> list[MaybeResponse]:\n    coros = [generate_reasoning_chain(query) for query in queries]\n    results = await asyncio.gather(*coros)\n    return results\n\nasync def generate_response(query: str, context: list[MaybeResponse]):\n    formatted_context = \"\\n\".join(\n        [\\\n            f\"\"\"\\\n            {item.result.intermediate_reasoning}\\\n            {item.result.correct_answer}\\\n            \"\"\"\\\n            for item in context\\\n            if not item.error and item.result\\\n        ]\n    )\n\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"\"\"\\\n                Given a question and a context,\\\n                answer the question step-by-step.\\\n\\\n                If you are unsure, answer Unknown.\\\n                \"\"\",\\\n            },\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                    <question>\\\n                    {query}\\\n                    </question>\\\n                    <context>\\\n                    {formatted_context}\\\n                    </context>\\\n                    \"\"\",\\\n            },\\\n        ],\n        response_model=ReasoningAndResponse,\n    )\n\nif __name__ == \"__main__\":\n    query = \"\"\"Would Arnold Schwarzenegger have been\n    able to deadlift an adult Black rhinoceros at his\n    peak strength?\"\"\"\n    decomposed_queries = asyncio.run(generate_queries(query))\n\n    for generated_query in decomposed_queries.queries:\n        print(generated_query)\n        #> How much weight could Arnold Schwarzenegger\n        #> deadlift at his peak strength?\n        #> What is the average weight of an adult Black\n        #> rhinoceros?\n\n    chains = asyncio.run(batch_reasoning_chains(decomposed_queries.queries))\n\n    for chain in chains:\n        print(chain.model_dump_json(indent=2))\n        \"\"\"\n        {\n          \"result\": {\n            \"intermediate_reasoning\": \"Determining Arnold\n            Schwarzenegger's peak deadlift involves\n            researching historical records, interviews,\n            and Arnold’s competitive powerlifting\n            results.\",\n            \"correct_answer\": \"Arnold Schwarzenegger's\n            peak deadlift was reportedly 710 lbs (322\n            kg).\"\n          },\n          \"error\": false,\n          \"error_message\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"result\": {\n            \"intermediate_reasoning\": \"To determine the\n            average weight of an adult Black rhinoceros,\n            I need to consult reliable sources such as\n            wildlife encyclopedias, zoological databases,\n            or scientific articles. Commonly, the average\n            weight of adult Black rhinoceros ranges\n            between 800 to 1,400 kg.\",\n            \"correct_answer\": \"The average weight of an\n            adult Black rhinoceros ranges between 800 to\n            1,400 kg.\"\n          },\n          \"error\": false,\n          \"error_message\": null\n        }\n        \"\"\"\n\n    response = asyncio.run(generate_response(query, chains))\n\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"intermediate_reasoning\": \"Arnold Schwarzenegger's\n      peak deadlift was 710 lbs (322 kg). The average\n      weight of an adult Black rhinoceros ranges between\n      800 to 1,400 kg (1764 to 3086 lbs). Even at the\n      lower end of the rhinoceros weight range (800 kg\n      or 1764 lbs), it exceeds Arnold Schwarzenegger's\n      peak deadlift capacity of 710 lbs (322 kg).\n      Therefore, Arnold Schwarzenegger would not have\n      been able to deadlift an adult Black rhinoceros at\n      his peak strength.\",\n      \"correct_answer\": \"No\"\n    }\n    \"\"\"\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/ensembling/meta_cot/\\#references \"Permanent link\")\n\n1: [Answering Questions by Meta-Reasoning over Multiple Chains of Thought](https://arxiv.org/pdf/2304.13007)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/ensembling/meta_cot/",
      "ogUrl": "https://python.useinstructor.com/prompting/ensembling/meta_cot/",
      "title": "Combine Multiple Reasoning Chains - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/ensembling/meta_cot/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/meta_cot.png",
      "ogTitle": "Combine Multiple Reasoning Chains - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/meta_cot.png",
      "og:title": "Combine Multiple Reasoning Chains - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/ensembling/meta_cot/",
      "statusCode": 200,
      "description": "Meta Chain Of Thought involves decomposing an initial query into multiple sub questions. We then aggregate the response from each of these chains as context before prompting another LLM to generate a response",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Meta Chain Of Thought involves decomposing an initial query into multiple sub questions. We then aggregate the response from each of these chains as context before prompting another LLM to generate a response",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/meta_cot.png",
      "twitter:title": "Combine Multiple Reasoning Chains - Instructor",
      "og:description": "Meta Chain Of Thought involves decomposing an initial query into multiple sub questions. We then aggregate the response from each of these chains as context before prompting another LLM to generate a response",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Meta Chain Of Thought involves decomposing an initial query into multiple sub questions. We then aggregate the response from each of these chains as context before prompting another LLM to generate a response"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/ensembling/usp/#usp-process)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/ensembling/usp.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/ensembling/usp.md \"View source of this page\")\n\n# Use Task Specific Evaluation Metrics\n\nUniversal Self Prompting is a two stage process similar to [Consistency Based Self Adaptive Prompting (COSP)](https://python.useinstructor.com/prompting/few_shot/cosp/). Here is a breakdown of the two stages.\n\n1. **Generate Examples** : LLMs are prompted to generate a collection of candidate responses using a test dataset\n2. **Answer Query** : We then select a few of these model-generated responses as examples to prompt the LLM to obtain a final prediction.\n\nNote here that the final answer is obtained using a single forward pass with greedy decoding.\n\n## USP Process [¶](https://python.useinstructor.com/prompting/ensembling/usp/\\#usp-process \"Permanent link\")\n\n![](https://python.useinstructor.com/img/universal_self_adaptive_prompting.png)\n\nLet's see how this works in greater detail.\n\n### Generate Few Shot Examples [¶](https://python.useinstructor.com/prompting/ensembling/usp/\\#generate-few-shot-examples \"Permanent link\")\n\nWe first prompt our model to generate responses for a given set of prompts. Instead of measuring the entropy and repetitiveness as in COSP, we use one of three possible methods to measure the quality of the generated responses. These methods are decided based on the three categories supported.\n\nThis category has to be specified by a user ahead of time.\n\nNote that for Short Form and Long Form generation, we generate mmm different samples. This is not the case for classification tasks.\n\n- **Classification** : Classification Tasks are evaluated using the normalized probability of each label using the raw logits from the LLM.\n\nFCLS(p(j)∣d(j)):=−∑c∈CP(c∣d(j))log⁡P(c∣d(j)) F\\_{CLS}(p^{(j)}\\|d^{(j)}) := -\\\\sum\\_{c \\\\in C} P(c\\|d^{(j)}) \\\\log P(c\\|d^{(j)}) FCLS​(p(j)∣d(j)):=−c∈C∑​P(c∣d(j))logP(c∣d(j))\n\nIn short, we take the raw logit for each token corresponding to the label, use a softmax to normalize each of them and then sum across the individual probabilities and their log probs. We also try to sample enough queries such that we have a balanced number of predictions across each class ( so that our model doesn't have a bias towards specific classes )\n\n- **Short Form Generation**: This is done by using a similar formula to COSP but without the normalizing term\n\nH(x(i)∣{y^j(i)}j=1m)=∑α=1up^(y^α(i))log⁡p^(y^α(i))log⁡m, \\\\mathcal{H}\\\\left(x^{(i)} \\\\mid \\\\left\\\\{\\\\hat{y}\\_j^{(i)}\\\\right\\\\}\\_{j=1}^m\\\\right) = \\\\frac{\\\\sum\\_{\\\\alpha=1}^u \\\\hat{p}\\\\left(\\\\hat{y}\\_{\\\\alpha}^{(i)}\\\\right) \\\\log \\\\hat{p}\\\\left(\\\\hat{y}\\_{\\\\alpha}^{(i)}\\\\right)}{\\\\log m}, H(x(i)∣{y^​j(i)​}j=1m​)=logm∑α=1u​p^​(y^​α(i)​)logp^​(y^​α(i)​)​,\n\n- **Long Form Generation**: This is done by using the average pairwise ROUGE score between all pairs of the mmm responses.\n\nWhat is key here is that depending on the task specified by the user, we have a task-specific form of evaluation. This eventually allows us to better evaluate our individual generated examples. Samples of tasks for each category include\n\n1. **Classification**: Natural Language Inference, Topic Classification and Sentiment Analysis\n2. **Short Form Generation** : Question Answering and Sentence Completion\n3. **Long Form Generation** : Text Summarization and Machine Translation\n\nThis helps to ultimately improve the performance of these large language models across different types of tasks.\n\n### Generate Single Response [¶](https://python.useinstructor.com/prompting/ensembling/usp/\\#generate-single-response \"Permanent link\")\n\nOnce we've selected our examples, the second step is relatively simple. We just need to append a few of our chosen examples that score best on our chosen metric to append to our solution.\n\n## Implementation [¶](https://python.useinstructor.com/prompting/ensembling/usp/\\#implementation \"Permanent link\")\n\nWe've implemented a classification example below that tries to sample across different classes in a balanced manner before generating a response using a single inference call.\n\nWe bias this sampling towards samples that the model is more confident towards by using a confidence label.\n\n```md-code__content\nfrom pydantic import BaseModel\nfrom typing import Literal\nfrom instructor import from_openai\nfrom openai import AsyncOpenAI\nimport asyncio\nfrom collections import defaultdict\n\nclass Classification(BaseModel):\n    chain_of_thought: str\n    label: Literal[\"Happy\", \"Angry\", \"Sadness\"]\n    confidence: Literal[\\\n        \"Uncertain\", \"Somewhat Confident\", \"Confident\", \"Highly Confident\"\\\n    ]\n\n    def confidence_score(self) -> int:\n        confidence_order = {\n            \"Highly Confident\": 4,\n            \"Confident\": 3,\n            \"Somewhat Confident\": 2,\n            \"Uncertain\": 1,\n        }\n        return confidence_order[self.confidence]\n\nclient = from_openai(AsyncOpenAI())\n\nasync def generate_prediction(query: str):\n    return (\n        await client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\\\n                {\\\n                    \"role\": \"user\",\\\n                    \"content\": f\"\"\"Classify the following query {query} into\\\n                    one of the following categories: Happy, Angry, Sadness\"\"\",\\\n                }\\\n            ],\n            response_model=Classification,\n        ),\n        query,\n    )\n\nasync def generate_predictions(queries: list[str]) -> list[tuple[Classification, str]]:\n    return await asyncio.gather(*[generate_prediction(query) for query in queries])\n\ndef get_balanced_sample(predictions: list[tuple[Classification, str]], k: int):\n    label_to_queries: dict[str, list[tuple[Classification, str]]] = defaultdict(list)\n\n    for prediction in predictions:\n        label_to_queries[prediction[0].label].append(prediction)\n\n    num_classes = len(label_to_queries)\n    num_samples_per_class = k // num_classes\n\n    res: list[str] = []\n    for label, label_queries in label_to_queries.items():\n        label_queries = sorted(\n            label_queries, key=lambda x: x[0].confidence_score(), reverse=True\n        )\n        label_queries = [\\\n            label_queries[1] for label_queries in label_queries[:num_samples_per_class]\\\n        ]\n        res.extend([f\"{query} ({label})\" for query in label_queries])\n\n    return res\n\nasync def generate_response_with_examples(query: str, examples: list[str]):\n    formatted_examples = \"\\n\".join(examples)\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Classification,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"\"\"\\\n                You are a helpful assistant that classifies queries into one of the following categories: Happy, Angry, Sadness.\\\n\\\n                Here are some samples of queries and their categories:\\\n\\\n                <examples>\\\n                {formatted_examples}\\\n                </examples>\\\n\\\n                Here is a user query to classify\\\n\\\n                <query>\\\n                {query}\\\n                </query>\\\n                \"\"\",\\\n            },\\\n        ],\n    )\n\nif __name__ == \"__main__\":\n    examples = [\\\n        \"\"\"\\\n        i do feel that running is a divine experience and\\\n        that i can expect to have some type of spiritual\\\n        encounter\\\n        \"\"\",\\\n        \"\"\"\\\n        i get giddy over feeling elegant in a perfectly\\\n        fitted pencil skirt\\\n        \"\"\",\\\n        \"\"\"\\\n        i plan to share my everyday life stories traveling\\\n        adventures inspirations and handmade creations with\\\n        you and hope you will also feel inspired\\\n        \"\"\",\\\n        \"\"\"\\\n        i need to feel the dough to make sure its just\\\n        perfect\\\n        \"\"\",\\\n        \"\"\"\\\n        i found myself feeling a little discouraged that\\\n        morning\\\n        \"\"\",\\\n        \"i didnt really feel that embarrassed\",\\\n        \"i feel like a miserable piece of garbage\",\\\n        \"\"\"\\\n        i feel like throwing away the shitty piece of shit\\\n        paper\\\n        \"\"\",\\\n        \"\"\"\\\n        i feel irritated and rejected without anyone doing\\\n        anything or saying anything\\\n        \"\"\",\\\n        \"i feel angered and firey\",\\\n        \"\"\"\\\n        im feeling bitter today my mood has been strange the\\\n        entire day so i guess its that\\\n        \"\"\",\\\n        \"i just feel really violent right now\",\\\n        \"i know there are days in which you feel distracted\",\\\n    ]\n\n    labels = asyncio.run(generate_predictions(examples))\n    balanced_sample = get_balanced_sample(labels, 3)\n    for sample in balanced_sample:\n        print(sample)\n        \"\"\"\n        i do feel that running is a divine experience and that i can\n        expect to have some type of spiritual encounter (Happy)\n        \"\"\"\n        #> i feel like a miserable piece of garbage (Sadness)\n        #> i feel like throwing away the shitty piece of shit paper (Angry)\n\n    response = asyncio.run(\n        generate_response_with_examples(\n            \"\"\"\n            i feel furious that right to life advocates can\n            and do tell me how to live and die through\n            lobbying and supporting those politicians\n            sympathic to their views\n            \"\"\",\n            balanced_sample,\n        )\n    )\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"chain_of_thought\": \"The user expresses feelings of\n      anger and frustration specifically directed at right\n      to life advocates. The language used, such as\n      'furious,' indicates a high level of emotion\n      associated with anger.\",\n      \"label\": \"Angry\",\n      \"confidence\": \"Highly Confident\"\n    }\n    \"\"\"\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/ensembling/usp/",
      "ogUrl": "https://python.useinstructor.com/prompting/ensembling/usp/",
      "title": "Use Task Specific Evaluation Metrics - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/ensembling/usp/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/usp.png",
      "ogTitle": "Use Task Specific Evaluation Metrics - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/usp.png",
      "og:title": "Use Task Specific Evaluation Metrics - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/ensembling/usp/",
      "statusCode": 200,
      "description": "Universal Self Prompting is a technique that aims to use unlabeled data to generate exemplars and a more complicated scoring function to select them.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Universal Self Prompting is a technique that aims to use unlabeled data to generate exemplars and a more complicated scoring function to select them.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/usp.png",
      "twitter:title": "Use Task Specific Evaluation Metrics - Instructor",
      "og:description": "Universal Self Prompting is a technique that aims to use unlabeled data to generate exemplars and a more complicated scoring function to select them.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Universal Self Prompting is a technique that aims to use unlabeled data to generate exemplars and a more complicated scoring function to select them."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/decomposition/least_to_most/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/decomposition/least_to_most.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/decomposition/least_to_most.md \"View source of this page\")\n\n# Solve simpler subproblems\n\nGiven a complex problem, how can we encourage an LLM to solve simpler subproblems?\n\nLeast-to-Most is a prompting technique that breaks a complex problem down into a series of increasingly complex subproblems.\n\nSubproblems Example\n\n**original problem**: Adam is twice as old as Mary. Adam will be 11 in 1 year. How old is Mary?\n\n**subproblems**: (1) How old is Adam now? (2) What is half of Adam's current age?\n\nThese subproblems are solved sequentially, allowing the answers from earlier (simpler) subproblems to inform the LLM while solving later (more complex) subproblems.\n\n```md-code__content\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nfrom typing import Iterable\n\nclass Subquestion(BaseModel):\n    question: str\n\nclass Answer(BaseModel):\n    answer: int\n\nclass SubquestionWithAnswers(BaseModel):\n    question: str\n    answer: int\n\nclient = instructor.from_openai(OpenAI())\n\ndef decompose(question):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Iterable[Subquestion],\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"Break this question down into subquestions to solve sequentially: {question}\",\\\n            }\\\n        ],\n    )\n\ndef solve(question, solved_questions, original_question):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Answer,\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                    <original_question>\\\n                    {original_question}\\\n                    </original_question>\\\n\\\n                    <solved_subquestions>\\\n                    {solved_questions}\\\n                    </solved_subquestions>\\\n\\\n                    Solve this next subquestion: {question}\\\n                    \"\"\",\\\n            }\\\n        ],\n    ).answer\n\nif __name__ == \"__main__\":\n    question = \"Four years ago, Kody was only half as old as Mohamed. If Mohamed is currently twice 30 years old, how old is Kody?\"\n\n    # Stage 1: Decompose Question into Subquestions\n    subquestions = decompose(question)\n\n    # Stage 2: Sequentially Solve Subquestions\n    solved_questions = []\n    for subquestion in subquestions:\n        solved_questions.append(\n            SubquestionWithAnswers(\n                question=subquestion.question,\n                answer=solve(subquestion, solved_questions, question),\n            )\n        )\n\n    # Print\n    for item in solved_questions:\n        print(f\"{item.question} {item.answer}\")\n        #> How old is Mohamed currently? 60\n        #> How old was Mohamed four years ago? 56\n        #> How old was Kody four years ago if he was half as old as Mohamed? 28\n        #> How old is Kody currently? 32\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/decomposition/least_to_most/\\#references \"Permanent link\")\n\n1: [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)\n\n\\*: [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/decomposition/least_to_most/",
      "ogUrl": "https://python.useinstructor.com/prompting/decomposition/least_to_most/",
      "title": "Solve simpler subproblems - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/decomposition/least_to_most/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/least_to_most.png",
      "ogTitle": "Solve simpler subproblems - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/least_to_most.png",
      "og:title": "Solve simpler subproblems - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/decomposition/least_to_most/",
      "statusCode": 200,
      "description": "Least-to-Most is a prompting technique that breaks a complex problem down into a series of increasingly complex subproblems.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Least-to-Most is a prompting technique that breaks a complex problem down into a series of increasingly complex subproblems.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/least_to_most.png",
      "twitter:title": "Solve simpler subproblems - Instructor",
      "og:description": "Least-to-Most is a prompting technique that breaks a complex problem down into a series of increasingly complex subproblems.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Least-to-Most is a prompting technique that breaks a complex problem down into a series of increasingly complex subproblems."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/ensembling/self_consistency/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/ensembling/self_consistency.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/ensembling/self_consistency.md \"View source of this page\")\n\n# Generate Multiple Candidate Responses\n\nBy generating multiple candidate responses in parallel and choosing the most common answer among them, we can get a more accurate answer. This is known as Self-Consistency [1](https://arxiv.org/pdf/2203.11171)\n\nWe can implement this using `instructor` as seen below.\n\n```md-code__content\nimport instructor\nfrom pydantic import BaseModel, Field\nfrom openai import AsyncOpenAI\nimport asyncio\nfrom collections import Counter\nfrom textwrap import dedent\n\nclass SelfConsistencyResponse(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"reasoning behind the final correct answer\"\n    )\n    correct_answer: int\n\nclient = instructor.from_openai(AsyncOpenAI())\n\nasync def generate_self_consistent_response(prompt: str):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"\"\"You are an intelligent question\\\n                answering AI system that excels at answering\\\n                user queries. Make sure to generate a\\\n                comprehensive explanation of your thought\\\n                process before providing the final answer\"\"\",\\\n            },\\\n            {\"role\": \"user\", \"content\": prompt},\\\n        ],\n        response_model=SelfConsistencyResponse,\n        temperature=0.5,\n    )\n\nasync def generate_self_consistent_responses(prompt: str, num_responses: int):\n    coros = [generate_self_consistent_response(prompt) for _ in range(num_responses)]\n    responses = await asyncio.gather(*coros)\n    return responses\n\nif __name__ == \"__main__\":\n    prompt = dedent(\n        \"\"\"\n        Janet's ducks lay 16 eggs per day.\n        She eats three for breakfast every\n        morning and bakes muffins for her\n        friends every day with four. She sells\n        the remainder for $2 per egg. How\n        much does she make every day?\n        \"\"\"\n    )\n    responses = asyncio.run(generate_self_consistent_responses(prompt, 5))\n    answer_counts = Counter([response.correct_answer for response in responses])\n    most_common_answer, _ = answer_counts.most_common(1)[0]\n\n    print(most_common_answer)\n    #> 18\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/ensembling/self_consistency/\\#references \"Permanent link\")\n\n1: [Self-Consistency Improves Chain Of Thought Reasoning In Language Models](https://arxiv.org/pdf/2210.03350)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/ensembling/self_consistency/",
      "ogUrl": "https://python.useinstructor.com/prompting/ensembling/self_consistency/",
      "title": "Generate Multiple Candidate Responses - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/ensembling/self_consistency/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/self_consistency.png",
      "ogTitle": "Generate Multiple Candidate Responses - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/self_consistency.png",
      "og:title": "Generate Multiple Candidate Responses - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/ensembling/self_consistency/",
      "statusCode": 200,
      "description": "Self Consistency aims to help maximise llm performance by sampling multiple potential calls. We then take a majority vote on the final response to derive the answer",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Self Consistency aims to help maximise llm performance by sampling multiple potential calls. We then take a majority vote on the final response to derive the answer",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/self_consistency.png",
      "twitter:title": "Generate Multiple Candidate Responses - Instructor",
      "og:description": "Self Consistency aims to help maximise llm performance by sampling multiple potential calls. We then take a majority vote on the final response to derive the answer",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Self Consistency aims to help maximise llm performance by sampling multiple potential calls. We then take a majority vote on the final response to derive the answer"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/decomposition/decomp/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/decomposition/decomp.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/decomposition/decomp.md \"View source of this page\")\n\n# Break Down Complex Tasks\n\nDecomposed Prompting[1](https://arxiv.org/pdf/2210.02406) leverages a Language Model (LLM) to deconstruct a complex task into a series of manageable sub-tasks. Each sub-task is then processed by specific functions, enabling the LLM to handle intricate problems more effectively and systematically.\n\nIn the code snippet below, we define a series of data models and functions to implement this approach.\n\nThe `derive_action_plan` function generates an action plan using the LLM, which is then executed step-by-step. Each action can be\n\n1. InitialInput: Which represents the chunk of the original prompt we need to process\n2. Split : An operation to split strings using a given separator\n3. StrPos: An operation to help extract a string given an index\n4. Merge: An operation to join a list of strings together using a given character\n\nWe can implement this using `instructor` as seen below.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import Union\n\nclient = instructor.from_openai(OpenAI())\n\nclass Split(BaseModel):\n    split_char: str = Field(\n        description=\"\"\"This is the character to split\n        the string with\"\"\"\n    )\n\n    def split_chars(self, s: str, c: str):\n        return s.split(c)\n\nclass StrPos(BaseModel):\n    index: int = Field(\n        description=\"\"\"This is the index of the character\n        we wish to return\"\"\"\n    )\n\n    def get_char(self, s: list[str], i: int):\n        return [c[i] for c in s]\n\nclass Merge(BaseModel):\n    merge_char: str = Field(\n        description=\"\"\"This is the character to merge the\n        inputs we plan to pass to this function with\"\"\"\n    )\n\n    def merge_string(self, s: list[str]):\n        return self.merge_char.join(s)\n\nclass Action(BaseModel):\n    id: int = Field(\n        description=\"\"\"Unique Incremental id to identify\n        this action with\"\"\"\n    )\n    action: Union[Split, StrPos, Merge]\n\nclass ActionPlan(BaseModel):\n    initial_data: str\n    plan: list[Action]\n\ndef derive_action_plan(task_description: str) -> ActionPlan:\n    return client.chat.completions.create(\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"\"\"Generate an action plan to help you complete\\\n                the task outlined by the user\"\"\",\\\n            },\\\n            {\"role\": \"user\", \"content\": task_description},\\\n        ],\n        response_model=ActionPlan,\n        max_retries=3,\n        model=\"gpt-4o\",\n    )\n\nif __name__ == \"__main__\":\n    task = \"\"\"Concatenate the second letter of every word in Jack\n    Ryan together\"\"\"\n    plan = derive_action_plan(task)\n    print(plan.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"initial_data\": \"Jack Ryan\",\n      \"plan\": [\\\n        {\\\n          \"id\": 1,\\\n          \"action\": {\\\n            \"split_char\": \" \"\\\n          }\\\n        },\\\n        {\\\n          \"id\": 2,\\\n          \"action\": {\\\n            \"index\": 1\\\n          }\\\n        },\\\n        {\\\n          \"id\": 3,\\\n          \"action\": {\\\n            \"merge_char\": \"\"\\\n          }\\\n        }\\\n      ]\n    }\n    \"\"\"\n\n    curr = plan.initial_data\n    cache = {}\n\n    for action in plan.plan:\n        if isinstance(action.action, Split) and isinstance(curr, str):\n            curr = action.action.split_chars(curr, action.action.split_char)\n        elif isinstance(action.action, StrPos) and isinstance(curr, list):\n            curr = action.action.get_char(curr, action.action.index)\n        elif isinstance(action.action, Merge) and isinstance(curr, list):\n            curr = action.action.merge_string(curr)\n        else:\n            raise ValueError(\"Unsupported Operation\")\n\n        print(action, curr)\n        #> id=1 action=Split(split_char=' ') ['Jack', 'Ryan']\n        #> id=2 action=StrPos(index=1) ['a', 'y']\n        #> id=3 action=Merge(merge_char='') ay\n\n    print(curr)\n    #> ay\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/decomposition/decomp/\\#references \"Permanent link\")\n\n1: [Decomposed Prompting: A Modular Approach for Solving Complex Tasks](https://arxiv.org/pdf/2210.02406)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/decomposition/decomp/",
      "ogUrl": "https://python.useinstructor.com/prompting/decomposition/decomp/",
      "title": "Break Down Complex Tasks - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/decomposition/decomp/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/decomp.png",
      "ogTitle": "Break Down Complex Tasks - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/decomp.png",
      "og:title": "Break Down Complex Tasks - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/decomposition/decomp/",
      "statusCode": 200,
      "description": "DECOMP involves using a LLM to break down a complicated task into sub tasks that it has been provided with",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "DECOMP involves using a LLM to break down a complicated task into sub tasks that it has been provided with",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/decomp.png",
      "twitter:title": "Break Down Complex Tasks - Instructor",
      "og:description": "DECOMP involves using a LLM to break down a complicated task into sub tasks that it has been provided with",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "DECOMP involves using a LLM to break down a complicated task into sub tasks that it has been provided with"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/self_criticism/chain_of_verification/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/self_criticism/chain_of_verification.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/self_criticism/chain_of_verification.md \"View source of this page\")\n\n# Independently Verify Responses\n\nChain Of Verification ( CoVe )[1](https://arxiv.org/pdf/2309.11495) is a method that allows us to be able to verify our LLM's generated responses. We can do so using the following steps\n\n1. First we get our LLM to generate a response to a query\n2. Then we generate a set of follow up questions that need to be answered to validate the response\n3. We then independently generate a set of responses to these questions\n4. Lastly, we use a final LLM call to verify the response in light of these new question and answer pairs that we've generated\n\n```md-code__content\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field\nimport asyncio\n\nclient = instructor.from_openai(AsyncOpenAI())\n\nclass QueryResponse(BaseModel):\n    correct_answer: str\n\nclass ValidationQuestions(BaseModel):\n    question: list[str] = Field(\n        description=\"\"\"A list of questions that need to be\n        answered to validate the response\"\"\"\n    )\n\nclass ValidationAnswer(BaseModel):\n    answer: str\n\nclass FinalResponse(BaseModel):\n    correct_answer: str\n\nasync def generate_initial_response(query: str):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=QueryResponse,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"You are an expert question answering system\",\\\n            },\\\n            {\"role\": \"user\", \"content\": query},\\\n        ],\n    )\n\nasync def generate_verification_questions(llm_response: str):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ValidationQuestions,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"\"\"You are an expert AI system that excels at\\\n                generating follow up questions to validate a response.\\\n                These questions should validate key assumptions, facts\\\n                and other important portions of the generated response\"\"\",\\\n            },\\\n            {\"role\": \"user\", \"content\": llm_response},\\\n        ],\n    )\n\nasync def generate_verification_response(questions: list[str]):\n    async def verify_question(question: str) -> tuple[ValidationAnswer, str]:\n        return (\n            await client.chat.completions.create(\n                model=\"gpt-4o\",\n                response_model=ValidationAnswer,\n                messages=[\\\n                    {\\\n                        \"role\": \"system\",\\\n                        \"content\": \"\"\"You are an expert AI system that\\\n                        excels at answering validation questions.\"\"\",\\\n                    },\\\n                    {\"role\": \"user\", \"content\": question},\\\n                ],\n            ),\n            question,\n        )\n\n    coros = [verify_question(question) for question in questions]\n    return await asyncio.gather(*coros)\n\nasync def generate_final_response(\n    answers: list[tuple[ValidationAnswer, str]],\n    initial_response: QueryResponse,\n    original_query: str,\n):\n    formatted_answers = \"\\n\".join(\n        [f\"Q: {question}\\nA: {answer.answer}\" for answer, question in answers]\n    )\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=FinalResponse,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"\"\"You are an expert AI system that excels at\\\n                validating and verifying if an initial answer answers an\\\n                initial query based off some Verification Questions and\\\n                Answers provided. Return the original answer if it is\\\n                valid else generate a new response off the verification\\\n                questions and answers provided.\"\"\",\\\n            },\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                Initial query: {original_query}\\\n                Initial Answer : {initial_response.correct_answer}\\\n                Verification Questions and Answers:\\\n                {formatted_answers}\\\n            \"\"\",\\\n            },\\\n        ],\n    )\n\nif __name__ == \"__main__\":\n    query = \"What was the primary cause of the Mexican-American war and how long did it last?\"\n    initial_response = asyncio.run(generate_initial_response(query))\n    print(initial_response.model_dump_json())\n    \"\"\"\n    {\"correct_answer\":\"The primary cause of the Mexican-American War was\n    the annexation of Texas by the United States and the dispute over\n    whether Texas ended at the Nueces River (as the Mexicans claimed) or\n    the Rio Grande (as the U.S. claimed). The war lasted from April 25,\n    1846, to February 2, 1848, totaling nearly two years.\"}\n    \"\"\"\n\n    verification_questions = asyncio.run(\n        generate_verification_questions(initial_response.correct_answer)\n    )\n    print(verification_questions.model_dump_json())\n    \"\"\"\n    {\"question\":[\"Is it accurate that the primary cause of the\\\n    Mexican-American War was the annexation of Texas by the United\\\n    States?\",\"Was there a dispute over whether Texas ended at the Nueces\\\n    River or the Rio Grande?\",\"Did the Mexican-American War last from\\\n    April 25, 1846, to February 2, 1848?\",\"Is it correct to state that\\\n    the disagreement over the Texas border was between the Nueces River\\\n    and the Rio Grande?\",\"Was the Mexican claim that Texas ended at the\\\n    Nueces River while the U.S. claimed it was at the Rio Grande?\"]}\n    \"\"\"\n\n    responses = asyncio.run(\n        generate_verification_response(verification_questions.question)\n    )\n\n    final_answer = asyncio.run(\n        generate_final_response(responses, initial_response, query)\n    )\n    print(final_answer.model_dump_json())\n    \"\"\"\n    {\"correct_answer\":\"The primary cause of the Mexican-American War was\n    the annexation of Texas by the United States and the dispute over\n    whether Texas ended at the Nueces River (as the Mexicans claimed) or\n    the Rio Grande (as the U.S. claimed). The war lasted from April 25,\n    1846, to February 2, 1848, totaling nearly two years.\"}\n    \"\"\"\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/self_criticism/chain_of_verification/\\#references \"Permanent link\")\n\n1: [Chain-Of-Verification Reduces Hallucination In Large Language Models](https://arxiv.org/pdf/2309.11495)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/self_criticism/chain_of_verification/",
      "ogUrl": "https://python.useinstructor.com/prompting/self_criticism/chain_of_verification/",
      "title": "Independently Verify Responses - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/self_criticism/chain_of_verification/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/chain_of_verification.png",
      "ogTitle": "Independently Verify Responses - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/chain_of_verification.png",
      "og:title": "Independently Verify Responses - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/self_criticism/chain_of_verification/",
      "statusCode": 200,
      "description": "We get a model to output a baseline response. Next, we independently verify the response by using a model to generate questions and to verify these questions. Lastly, we use a final API call to verify the baseline response with the generated data",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "We get a model to output a baseline response. Next, we independently verify the response by using a model to generate questions and to verify these questions. Lastly, we use a final API call to verify the baseline response with the generated data",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/chain_of_verification.png",
      "twitter:title": "Independently Verify Responses - Instructor",
      "og:description": "We get a model to output a baseline response. Next, we independently verify the response by using a model to generate questions and to verify these questions. Lastly, we use a final API call to verify the baseline response with the generated data",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "We get a model to output a baseline response. Next, we independently verify the response by using a model to generate questions and to verify these questions. Lastly, we use a final API call to verify the baseline response with the generated data"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/zero_shot/re2/#implementation)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/zero_shot/re2.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/zero_shot/re2.md \"View source of this page\")\n\n# Ask Model To Repeat Query\n\nHow can we enhance a model's understanding of a query?\n\nRe2 ( **Re** \\- **R** eading) is a technique that asks the model to read the question again.\n\nRe-Reading Prompting\n\n**Prompt Template**: Read the question again: < _query_ \\> < _critical thinking prompt_ >[1](https://arxiv.org/abs/2309.06275)\n\nA common critical thinking prompt is: \"Let's think step by step.\"\n\n## Implementation [¶](https://python.useinstructor.com/prompting/zero_shot/re2/\\#implementation \"Permanent link\")\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\n\nclass Response(BaseModel):\n    answer: int\n\ndef re2(query, thinking_prompt):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"Read the question again: {query} {thinking_prompt}\",\\\n            },\\\n        ],\n    )\n\nif __name__ == \"__main__\":\n    query = \"\"\"Roger has 5 tennis balls.\n        He buys 2 more cans of tennis balls.\n        Each can has 3 tennis balls.\n        How many tennis balls does he have now?\n        \"\"\"\n    thinking_prompt = \"Let's think step by step.\"\n\n    response = re2(query=query, thinking_prompt=thinking_prompt)\n    print(response.answer)\n    #> 11\n\n```\n\n## References [¶](https://python.useinstructor.com/prompting/zero_shot/re2/\\#references \"Permanent link\")\n\n1: [Re-Reading Improves Reasoning in Large Language Models](https://arxiv.org/abs/2309.06275)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/zero_shot/re2/",
      "ogUrl": "https://python.useinstructor.com/prompting/zero_shot/re2/",
      "title": "Ask Model To Repeat Query - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/zero_shot/re2/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/re2.png",
      "ogTitle": "Ask Model To Repeat Query - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/re2.png",
      "og:title": "Ask Model To Repeat Query - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/zero_shot/re2/",
      "statusCode": 200,
      "description": "Re2 (Re-Reading) is a technique that asks the model to read the question again.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Re2 (Re-Reading) is a technique that asks the model to read the question again.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/re2.png",
      "twitter:title": "Ask Model To Repeat Query - Instructor",
      "og:description": "Re2 (Re-Reading) is a technique that asks the model to read the question again.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Re2 (Re-Reading) is a technique that asks the model to read the question again."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/ensembling/dense/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/ensembling/dense.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/ensembling/dense.md \"View source of this page\")\n\n# Use Distinct Example Subsets\n\nWe can maximise the use of our examples by prompting our model multiple times, each time using a different subset of examples. We can then take these multiple outputs and aggregate over them to generate a final response. This is known as Demonstration Ensembling ( DENSE ) [1](https://arxiv.org/pdf/2308.08780).\n\n> For simplicity in this example, we simply iterate over the examples and partition them equally to get equally sized clusters. However, depending on your use-case you might also want to consider sampling these using some form of embedding clusering.\n\nWe can implement this using `instructor` as seen below.\n\n```md-code__content\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\nimport asyncio\nfrom collections import Counter\nfrom typing import Literal\nfrom textwrap import dedent\n\nclass DemonstrationResponse(BaseModel):\n    correct_answer: Literal[\"Positive\", \"Negative\", \"Neutral\"]\n\nclient = instructor.from_openai(AsyncOpenAI())\n\nasync def generate_self_consistent_response(prompt: str, examples: list[str]):\n    concetenated_examples = \"\\n\".join(examples)\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": dedent(\\\n                    f\"\"\"\\\n                You are an intelligent AI System that excels\\\n                at classifying user queries into three\\\n                possible labels:\\\n                - Positive\\\n                - Negative\\\n                - Neutral\\\n\\\n                You are about to be given a user query and\\\n                asked to classify it into one of the three\\\n                categories. Make sure to refer closely to\\\n                the examples provided to you, examining each\\\n                individual example before coming up with the\\\n                final answer.\\\n\\\n                Here are the examples:\\\n                {concetenated_examples}\\\n                \"\"\"\\\n                ),\\\n            },\\\n            {\"role\": \"user\", \"content\": prompt},\\\n        ],\n        response_model=DemonstrationResponse,\n        temperature=0,\n    )\n\nasync def generate_self_consistent_responses(\n    prompt: str, num_responses: int, examples: list[str]\n):\n    assert (\n        len(examples) % num_responses == 0\n    ), \"The number of examples must be evenly divisible by num_responses\"\n\n    # Batch the examples into num_responses batches\n    batch_size = len(examples) // num_responses\n\n    coros = [\\\n        generate_self_consistent_response(prompt, examples[i : i + batch_size])\\\n        for i in range(0, len(examples), batch_size)\\\n    ]\n\n    responses = await asyncio.gather(*coros)\n    return responses\n\nif __name__ == \"__main__\":\n    user_query = \"What is the weather like today?\"\n    examples = [\\\n        \"I love this product! [Positive]\",\\\n        \"This is the worst service ever. [Negative]\",\\\n        \"The movie was okay, not great but not terrible. [Neutral]\",\\\n        \"I'm so happy with my new phone! [Positive]\",\\\n        \"The food was terrible and the service was slow. [Negative]\",\\\n        \"It's an average day, nothing special. [Neutral]\",\\\n        \"Fantastic experience, will come again! [Positive]\",\\\n        \"I wouldn't recommend this to anyone. [Negative]\",\\\n        \"The book was neither good nor bad. [Neutral]\",\\\n        \"Absolutely thrilled with the results! [Positive]\",\\\n    ]\n    responses = asyncio.run(generate_self_consistent_responses(user_query, 5, examples))\n    answer_counts = Counter([response.correct_answer for response in responses])\n    most_common_answer, _ = answer_counts.most_common(1)[0]\n    print(most_common_answer)\n    #> Neutral\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/ensembling/dense/\\#references \"Permanent link\")\n\n1: [Exploring Demonstration Ensembling for In Context Learning](https://arxiv.org/pdf/2308.08780)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/ensembling/dense/",
      "ogUrl": "https://python.useinstructor.com/prompting/ensembling/dense/",
      "title": "Use Distinct Example Subsets - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/ensembling/dense/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/dense.png",
      "ogTitle": "Use Distinct Example Subsets - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/dense.png",
      "og:title": "Use Distinct Example Subsets - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/ensembling/dense/",
      "statusCode": 200,
      "description": "Demonstration Ensembling(DENSE) creates multiple few-shot prompts, each containing a distinct subset of examples from the training set. We then use that to generate a final response",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Demonstration Ensembling(DENSE) creates multiple few-shot prompts, each containing a distinct subset of examples from the training set. We then use that to generate a final response",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/dense.png",
      "twitter:title": "Use Distinct Example Subsets - Instructor",
      "og:description": "Demonstration Ensembling(DENSE) creates multiple few-shot prompts, each containing a distinct subset of examples from the training set. We then use that to generate a final response",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Demonstration Ensembling(DENSE) creates multiple few-shot prompts, each containing a distinct subset of examples from the training set. We then use that to generate a final response"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/ensembling/cosp/#cosp-process)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/ensembling/cosp.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/ensembling/cosp.md \"View source of this page\")\n\n# Prioritize Consistent Examples\n\nConsistency Based Self Adaptive Prompting (COSP)[1](https://arxiv.org/pdf/2305.14106) aims to improve LLM output quality by generating high quality few shot examples to be included in the final prompt. These are examples without labelled ground truth so they use self-consistency and a metric known as normalized entropy to select the best examples.\n\nOnce they've selected the examples, they then append them to the prompt and generate multiple reasoning chains before selecting the final result using [Self-Consistency](https://python.useinstructor.com/prompting/ensembling/self_consistency/).\n\n## COSP process [¶](https://python.useinstructor.com/prompting/ensembling/cosp/\\#cosp-process \"Permanent link\")\n\n![](https://python.useinstructor.com/img/cosp.png)\n\nHow does this look in practice? Let's dive into greater detail.\n\n### Step 1 - Selecting Examples [¶](https://python.useinstructor.com/prompting/ensembling/cosp/\\#step-1-selecting-examples \"Permanent link\")\n\nIn the first step, we try to generate high quality examples from questions that don't have ground truth labels. This is challenging because we want to find a way to automatically determine answer quality when sampling our model multiple times.\n\nIn this case, we have `n` questions which we want to generate `m` possible reasoning chains for each question. This gives a total of `nm` examples. We then want to filter out `k` final few shot examples from these `nm` examples to be included inside our final prompt.\n\n1. Using chain of thought, we first generate `m` responses for each question. These responses contain a final answer and a rationale behind that answer.\n2. We compute a score for each response using a weighted sum of two values - normalized entropy and repetitiveness ( How many times this rationale appears for this amswer )\n3. We rank all of our `nm` responses using this score and choose the `k` examples with the lowest scores as our final few shot examples.\n\n#### Normalized Entropy [¶](https://python.useinstructor.com/prompting/ensembling/cosp/\\#normalized-entropy \"Permanent link\")\n\n> In the paper, the authors write that normalized entropy is a good proxy over a number of different tasks where low entropy is positively correlated with correctness. Entropy is also supposed to range from 0 to 1.\n>\n> Therefore in order to do so, we introduce a `-` term in our implementation so that the calculated values range from 0 to 1.\n\n![](https://python.useinstructor.com/img/cosp_entropy.png)\n\nAssuming that for a specific question x(i)x^{(i)}x(i), we have generated mmm final answers of which uuu are unique. ( Note that this only cares about the answer itself and not the rationale )\n\nH(x(i)∣{y^j(i)}j=1m)=∑α=1up^(y^α(i))log⁡p^(y^α(i))log⁡m, \\\\mathcal{H}\\\\left(x^{(i)} \\\\mid \\\\left\\\\{\\\\hat{y}\\_j^{(i)}\\\\right\\\\}\\_{j=1}^m\\\\right) = \\\\frac{\\\\sum\\_{\\\\alpha=1}^u \\\\hat{p}\\\\left(\\\\hat{y}\\_{\\\\alpha}^{(i)}\\\\right) \\\\log \\\\hat{p}\\\\left(\\\\hat{y}\\_{\\\\alpha}^{(i)}\\\\right)}{\\\\log m}, H(x(i)∣{y^​j(i)​}j=1m​)=logm∑α=1u​p^​(y^​α(i)​)logp^​(y^​α(i)​)​,\n\nWe can measure the entropy of the generated responses using the formula above where\n\n- xix\\_ixi​ is the original question that we prompted the model with\n- yjiy\\_j^{i}yji​ represents the iii-th sampled response from the mmm that we generated\n- p^(y^α(i))\\\\hat{p}\\\\left(\\\\hat{y}\\_{\\\\alpha}^{(i)}\\\\right)p^​(y^​α(i)​) is the frequency of the unique answer in all the mmm generated answers. (Eg. if we generate 8 responses and 4 of them return the value 10, then p^(y^α(i))\\\\hat{p}\\\\left(\\\\hat{y}\\_{\\\\alpha}^{(i)}\\\\right)p^​(y^​α(i)​) is just going to be 0.5)\n\n#### Repetitiveness [¶](https://python.useinstructor.com/prompting/ensembling/cosp/\\#repetitiveness \"Permanent link\")\n\nRr(rj(i))=2Q(Q−1)∑a=1Q∑b=a+1QWab R\\_r(r\\_j^{(i)}) = \\\\frac{2}{Q(Q-1)} \\\\sum\\_{a=1}^{Q} \\\\sum\\_{b=a+1}^{Q} W\\_{ab} Rr​(rj(i)​)=Q(Q−1)2​a=1∑Q​b=a+1∑Q​Wab​\n\nIn the formula above, QQQ refers to the number of phrases in the sentence and WabW\\_{ab}Wab​ refers to the cosine similarity of two phrases aaa and bbb.\n\nRepetitiveness aims to measure how often the language model repeats itself. To do so, the paper sums up the cosine similarity between each sentence inside the generated chain of thought rationale before normalizing it.\n\nThe intuition behind this is that high repetitiveness indicates redundancy, which can lead to poorer performance. Therefore responses with a high number of similar sentences will have a larger score for repetitiveness ( since cosine similarity will be larger for each sentence ).\n\n### Step 2 - Self Consistency [¶](https://python.useinstructor.com/prompting/ensembling/cosp/\\#step-2-self-consistency \"Permanent link\")\n\nWe now take our `k` responses and append them to our prompt. We then sample our model multiple times using this new prompt and take the majority vote as the answer.\n\n## Implementation [¶](https://python.useinstructor.com/prompting/ensembling/cosp/\\#implementation \"Permanent link\")\n\nNow that we understand what COSP is, let's see how we can implement it in instructor. Note that here we'll measure repetitiveness using cosine similarity between sentence embeddings.\n\n```md-code__content\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI, OpenAI\nfrom collections import defaultdict, Counter\nimport asyncio\nfrom textwrap import dedent\nimport math\n\nclient = instructor.from_openai(AsyncOpenAI())\n\nclass Response(BaseModel):\n    chain_of_thought: list[str]\n    answer: int\n\nclass ResponseScore(BaseModel):\n    query: str\n    response: Response\n    score: float\n\n    def format_response(self):\n        return dedent(\n            f\"\"\"\n            Q: {self.query}\n            A: {''.join(self.response.chain_of_thought)}. Therefore the answer is {self.response.answer}.\n            \"\"\"\n        )\n\ndef cosine_similarity(vec1: list[float], vec2: list[float]):\n    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n    magnitude1 = math.sqrt(sum(a * a for a in vec1))\n    magnitude2 = math.sqrt(sum(b * b for b in vec2))\n\n    if magnitude1 * magnitude2 == 0:\n        return 0  # Handle the case of zero vectors\n\n    return dot_product / (magnitude1 * magnitude2)\n\ndef score_repetitiveness(prediction: Response):\n    if len(prediction.chain_of_thought) == 1:\n        return 0\n\n    embedding = OpenAI().embeddings.create(\n        input=prediction.chain_of_thought, model=\"text-embedding-3-small\"\n    )\n    embedding = [item.embedding for item in embedding.data]\n\n    ttl = 0\n    num_comparisons = 0\n    for idx in range(len(embedding)):\n        for idx2 in range(idx + 1, len(embedding)):\n            ttl += cosine_similarity(embedding[idx], embedding[idx2])\n            num_comparisons += 1\n\n    return ttl / num_comparisons if num_comparisons > 0 else 0\n\nasync def generate_cot_response(query: str) -> tuple[Response, str]:\n    return (\n        await client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\"role\": \"user\", \"content\": query}],\n            response_model=Response,\n            temperature=0.4,\n        ),\n        query,\n    )\n\nasync def generate_batch_cot_responses(\n    queries: list[str], m: int\n) -> list[tuple[Response, str]]:\n    coros = [generate_cot_response(query) for query in queries for _ in range(m)]\n    return await asyncio.gather(*coros)\n\ndef score_entropy(predictions: list[Response]):\n    counter = Counter([prediction.answer for prediction in predictions])\n\n    prob = [counter[i] / len(predictions) for i in counter]\n\n    numer = -sum([p * math.log(p) for p in prob])\n    denom = math.log(len(predictions))\n\n    return numer / denom\n\ndef score_responses(\n    predictions: list[tuple[Response, str]], trade_off_param: float\n) -> list[ResponseScore]:\n    query_to_responses: dict[str, list[Response]] = defaultdict(list)\n    for prediction, query in predictions:\n        query_to_responses[query].append(prediction)\n\n    query_to_entropy = {\n        query: score_entropy(predictions)\n        for query, predictions in query_to_responses.items()\n    }\n\n    return [\\\n        ResponseScore(\\\n            query=query,\\\n            response=prediction,\\\n            score=query_to_entropy[query]\\\n            + trade_off_param * score_repetitiveness(prediction),\\\n        )\\\n        for prediction, query in predictions\\\n    ]\n\ndef get_top_k_examples(queries: list[ResponseScore], k: int):\n    \"\"\"\n    This gets the top k responses that have the minimum possible score\n    \"\"\"\n    sorted_responses = sorted(queries, key=lambda x: x.score)\n    return sorted_responses[:k]\n\nasync def generate_answer_with_examples(query: str, examples: list[ResponseScore]):\n    formatted_examples = \"\\n\".join([example.format_response() for example in examples])\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": dedent(\\\n                    f\"\"\"\\\n                You are a world class AI system that excels at answering user queries\\\n\\\n                <query>\\\n                {query}\\\n                </query>\\\n\\\n                <examples>\\\n                {formatted_examples}\\\n                </examples>\\\n                \"\"\"\\\n                ),\\\n            }\\\n        ],\n        response_model=Response,\n    )\n\nasync def generate_final_answers(\n    query: str, examples: list[ResponseScore], number_samples: int\n):\n    coros = [\\\n        generate_answer_with_examples(query, examples) for _ in range(number_samples)\\\n    ]\n\n    return await asyncio.gather(*coros)\n\nif __name__ == \"__main__\":\n    query = (\n        \"The schools debate team had 5 boys and 40 girls on it. \"\n        \"If they were split into groups of 9 how many groups \"\n        \"could they make?\"\n    )\n\n    example_questions = [\\\n        (\\\n            \"Debby's class is going on a field trip to the zoo. \"\\\n            \"If each van can hold 4 people and there are 2 students \"\\\n            \"and 6 adults going, how many vans will they need?\"\\\n        ),\\\n        (\\\n            \"Nancy had 80 files on her computer. She deleted 31 of \"\\\n            \"them and put the rest into folders with 7 files in each \"\\\n            \"one. How many folders did Nancy end up with?\"\\\n        ),\\\n        (\\\n            \"At the arcade, Tom won 32 tickets playing 'whack a mole' \"\\\n            \"and 25 tickets playing 'skee ball'. If he spent 7 of his \"\\\n            \"tickets on a hat, how many tickets does Tom have left?\"\\\n        ),\\\n    ]\n\n    m = 2  # Number of Reasoning Chains per example ( Step 1 )\n    k = 3  # Number of Examples to include in final prompt (Step 2)\n    n = 2  # Number of Reasoning Chains For Self-Consistency ( Step 2 )\n\n    # Step 1 : Generate the examples\n    responses = asyncio.run(generate_batch_cot_responses(example_questions, m))\n    scored_responses = score_responses(responses, 0.2)\n\n    chosen_examples = get_top_k_examples(scored_responses, k)\n\n    # Step 2 : Run Self-Consistency\n    final_responses = asyncio.run(generate_final_answers(query, chosen_examples, n))\n\n    c = Counter([response.answer for response in final_responses])\n    answer = c.most_common(1)[0][0]\n\n    print(answer)\n    #> 5\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/ensembling/cosp/\\#references \"Permanent link\")\n\n1: [Better Zero-Shot Reasoning with Self-Adaptive Prompting](https://arxiv.org/pdf/2305.14106)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/ensembling/cosp/",
      "ogUrl": "https://python.useinstructor.com/prompting/ensembling/cosp/",
      "title": "Prioritize Consistent Examples - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/ensembling/cosp/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/cosp.png",
      "ogTitle": "Prioritize Consistent Examples - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/cosp.png",
      "og:title": "Prioritize Consistent Examples - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/ensembling/cosp/",
      "statusCode": 200,
      "description": "Consistency Based Self Adaptive Prompting (COSP) is a ensembling technique that aims to combine multiple Chain Of Thought reasoning calls",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Consistency Based Self Adaptive Prompting (COSP) is a ensembling technique that aims to combine multiple Chain Of Thought reasoning calls",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/cosp.png",
      "twitter:title": "Prioritize Consistent Examples - Instructor",
      "og:description": "Consistency Based Self Adaptive Prompting (COSP) is a ensembling technique that aims to combine multiple Chain Of Thought reasoning calls",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Consistency Based Self Adaptive Prompting (COSP) is a ensembling technique that aims to combine multiple Chain Of Thought reasoning calls"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/self_criticism/self_refine/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/self_criticism/self_refine.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/self_criticism/self_refine.md \"View source of this page\")\n\n# Improve With Feedback\n\nHow can we provide feedback for an LLM to improve its responses?\n\nSelf-refine is an approach that uses an LLM to generate an output, provide feedback on the output, and improve the output based on the provided feedback. This processes repeats until a stopping condition is achieved. The same LLM is used for all three steps.\n\n```md-code__content\nimport instructor\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\nfrom typing import Optional\n\nclass Response(BaseModel):\n    code: str\n\nclass Feedback(BaseModel):\n    feedback: list[str] = Field(\n        description=\"A list of actions to take to improve the code.\"\n    )\n    done: bool\n\nclass Timestep(BaseModel):\n    response: str\n    feedback: Optional[list[str]] = Field(default_factory=list)\n    refined_response: Optional[str] = Field(default=\"\")\n\nclass History(BaseModel):\n    history: list[Timestep] = Field(default_factory=list)\n\n    def add(self, code, feedback, refined_code):\n        self.history.append(\n            Timestep(response=code, feedback=feedback, refined_response=refined_code)\n        )\n\nclient = instructor.from_openai(OpenAI())\n\ndef generate_feedback(response):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Feedback,\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                        You are an expert Python coder.\\\n                        Provide feedback on this code.\\\n                        How can we make it (1) faster and (2) more readable?\\\n\\\n                        <code>\\\n                        {response.code}\\\n                        </code>\\\n\\\n                        If the code does not need to be improved, then indicate by setting \"done\" to True.\\\n                        \"\"\",\\\n            }\\\n        ],\n    )\n\ndef refine(response, feedback):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                        You are an expert Python coder.\\\n\\\n                        <response>\\\n                        {response.code}\\\n                        </response>\\\n\\\n                        <feedback>\\\n                        {feedback.feedback}\\\n                        </feedback>\\\n\\\n                        Refine your response.\\\n                        \"\"\",\\\n            }\\\n        ],\n    )\n\ndef stop_condition(feedback, history):\n    return feedback.done or len(history.history) >= 3\n\nif __name__ == \"__main__\":\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": \"Write Python code to calculate the fibonacci sequence.\",\\\n            }\\\n        ],\n    )\n\n    history = History()\n\n    while True:\n        feedback = generate_feedback(response)\n        if stop_condition(feedback, history):\n            break\n        refined_response = refine(response, feedback)\n\n        # Save to history\n        history.add(response.code, feedback.feedback, refined_response.code)\n        response = refined_response\n\n    print(history.history[0].response)\n    \"\"\"\n    def fibonacci(n):\n        sequence = [0, 1]\n        while len(sequence) < n:\n            sequence.append(sequence[-1] + sequence[-2])\n        return sequence[:n]\n\n    # Example usage:\n    n = 10\n    print(fibonacci(n))\n    \"\"\"\n    print(history.history[0].feedback)\n    \"\"\"\n    [\\\n        'Use a generator to reduce memory consumption for large `n` values and improve speed.',\\\n        'Enhance readability by adding type hints for input and output.',\\\n        \"Add docstrings to explain the function's purpose and parameters.\",\\\n        \"Avoid slicing the list at the end if it's not necessary; instead, ensure the loop condition is precise.\",\\\n    ]\n    \"\"\"\n    print(history.history[0].refined_response)\n    \"\"\"\n    def fibonacci(n: int) -> list[int]:\n        \"\"\"Generate a Fibonacci sequence of length n.\n\n        Args:\n            n (int): The length of the Fibonacci sequence to generate.\n\n        Returns:\n            list[int]: A list containing the Fibonacci sequence of length n.\n        \"\"\"\n        def fibonacci_generator():\n            a, b = 0, 1\n            for _ in range(n):\n                yield a\n                a, b = b, a + b\n        return list(fibonacci_generator())\n\n    # Example usage:\n    n = 10\n    print(fibonacci(n))\n    \"\"\"\n    print(f\"...process repeated {len(history.history)} times...\")\n    #> ...process repeated 3 times...\n    print(response.code)\n    \"\"\"\n    def fibonacci(n: int) -> list[int]:\n        \"\"\"Generate a Fibonacci sequence of length n.\n\n        Args:\n            n (int): The length of the Fibonacci sequence to generate.\n\n        Returns:\n            list[int]: A list containing the Fibonacci sequence of length n.\n        \"\"\"\n        if n <= 0:\n            return []\n        sequence = [0] * n\n        if n > 1:\n            sequence[1] = 1\n        for i in range(2, n):\n            sequence[i] = sequence[i-1] + sequence[i-2]\n        return sequence\n\n    # Example usage:\n    n = 10\n    print(fibonacci(n))\n    \"\"\"\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/self_criticism/self_refine/\\#references \"Permanent link\")\n\n1: [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/abs/2303.17651)\n\n\\*: [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/self_criticism/self_refine/",
      "ogUrl": "https://python.useinstructor.com/prompting/self_criticism/self_refine/",
      "title": "Improve With Feedback - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/self_criticism/self_refine/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/self_refine.png",
      "ogTitle": "Improve With Feedback - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/self_refine.png",
      "og:title": "Improve With Feedback - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/self_criticism/self_refine/",
      "statusCode": 200,
      "description": "Self-refine is an approach that uses an LLM to generate an output, provide feedback on the output, and improve the output based on the provided feedback.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Self-refine is an approach that uses an LLM to generate an output, provide feedback on the output, and improve the output based on the provided feedback.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/self_refine.png",
      "twitter:title": "Improve With Feedback - Instructor",
      "og:description": "Self-refine is an approach that uses an LLM to generate an output, provide feedback on the output, and improve the output based on the provided feedback.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Self-refine is an approach that uses an LLM to generate an output, provide feedback on the output, and improve the output based on the provided feedback."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/zero_shot/self_ask/#implementation)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/zero_shot/self_ask.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/zero_shot/self_ask.md \"View source of this page\")\n\n# Generate Follow-Up Questions\n\nModels can sometimes correctly answer sub-problems but incorrectly answer the overall query. This is known as the _compositionality gap_ [1](https://arxiv.org/abs/2210.03350).\n\nHow can we encourage a model to use the answers to sub-problems to correctly generate the overall solution?\n\nSelf-Ask is a technique which use a single prompt to:\n\n- decide if follow-up questions are required\n- generate the follow-up questions\n- answer the follow-up questions\n- answer the main query\n\n## Implementation [¶](https://python.useinstructor.com/prompting/zero_shot/self_ask/\\#implementation \"Permanent link\")\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(OpenAI())\n\nclass FollowUp(BaseModel):\n    question: str = Field(description=\"The follow-up question\")\n    answer: str = Field(description=\"The answer to the follow-up question\")\n\nclass Response(BaseModel):\n    follow_ups_required: bool\n    follow_ups: list[FollowUp]\n    final_answer: str\n\ndef self_ask(query):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"\"\"Query: {query}\\\n                        Are follow-up questions needed?\\\n                        If so, generate follow-up questions, their answers, and then the final answer to the query.\\\n                        \"\"\",  # !\\\n            },\\\n        ],\n    )\n\nif __name__ == \"__main__\":\n    query = \"Who was president of the U.S. when superconductivity was discovered?\"\n\n    response = self_ask(query)\n\n    print(response.follow_ups_required)\n    #> True\n    for follow_up in response.follow_ups:\n        print(follow_up)\n        \"\"\"\n        question='When was superconductivity discovered?' answer='Superconductivity was discovered in April 1911.'\n        \"\"\"\n        \"\"\"\n        question='Who was president of the U.S. in April 1911?' answer='William Howard Taft was the President of the United States in April 1911.'\n        \"\"\"\n    print(response.final_answer)\n    \"\"\"\n    William Howard Taft was president of the U.S. when superconductivity was discovered.\n    \"\"\"\n\n```\n\n## References [¶](https://python.useinstructor.com/prompting/zero_shot/self_ask/\\#references \"Permanent link\")\n\n1: [Measuring and Narrowing the Compositionality Gap in Language Models](https://arxiv.org/abs/2210.03350)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/zero_shot/self_ask/",
      "ogUrl": "https://python.useinstructor.com/prompting/zero_shot/self_ask/",
      "title": "Self-Ask - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/zero_shot/self_ask/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/self_ask.png",
      "ogTitle": "Self-Ask - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/self_ask.png",
      "og:title": "Self-Ask - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/zero_shot/self_ask/",
      "statusCode": 200,
      "description": "Self-Ask is a technique which use a single prompt to encourage a model to use the answers to sub-problems to correctly generate the overall solution.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Self-Ask is a technique which use a single prompt to encourage a model to use the answers to sub-problems to correctly generate the overall solution.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/self_ask.png",
      "twitter:title": "Self-Ask - Instructor",
      "og:description": "Self-Ask is a technique which use a single prompt to encourage a model to use the answers to sub-problems to correctly generate the overall solution.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Self-Ask is a technique which use a single prompt to encourage a model to use the answers to sub-problems to correctly generate the overall solution."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/self_criticism/reversecot/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/self_criticism/reversecot.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/self_criticism/reversecot.md \"View source of this page\")\n\n# Reconstruct Prompt from Reasoning Steps\n\nWe can use a method called Reverse Chain Of Thought[1](https://arxiv.org/pdf/2305.11499) to reverse engineer a problem given a solution. This helps us to find specific inconsistencies in the reasoning steps taken by our model and to give targetted feedback which can improve the quality of the solution.\n\nThis is done through a 3 step process\n\n1. **Reconstruct The Question** : We first attempt to reconstruct the original problem given the solution and reasoning steps generated\n2. **Identify Inconsistencies** : Identify the inconsistencies between the original problem and the reconstructed problem\n3. **Generate Feedback** : Give fine-grained fedback to guide the LLM in revising its solution\n\nWe can implement this using `instructor` as seen below.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(OpenAI())\n\nclass ReconstructedPrompt(BaseModel):\n    chain_of_thought: str\n    reconstructed_prompt: str = Field(\n        description=\"\"\"Reconstruction of a potential prompt\n        that could have been used to generate the reasoning\n        and final solution provided by the user\"\"\"\n    )\n\nclass ConditionList(BaseModel):\n    conditions: list[str] = Field(\n        description=\"\"\"Key information and conditions present\n        in the reasoning steps which are relevant to answering\n        the question\"\"\"\n    )\n\nclass ModelFeedback(BaseModel):\n    detected_inconsistencies: list[str] = Field(\n        description=\"\"\"Inconsistencies that were detected between\n        the original condition list and the reconstructed condition\n        list\"\"\"\n    )\n    feedback: str = Field(\n        description=\"\"\"Feedback on how to fix the inconsistencies\n        detected in the original condition list and the reconstructed\n        condition list\"\"\"\n    )\n    is_equal: bool\n\nclass ModelResponse(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"\"\"Logical Steps that were taken to derive\n        the final concluding statement\"\"\"\n    )\n    correct_answer: str\n\ndef generate_response(query: str):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"\"\"\\\n                You are a helpful AI Question Answerer. You are\\\n                about to be passed a query by a User.\\\n\\\n                Make sure to generate a series of logical steps\\\n                and reason about the problem before generating\\\n                a solution.\\\n                \"\"\",\\\n            },\\\n            {\"role\": \"user\", \"content\": query},\\\n        ],\n        response_model=ModelResponse,\n    )\n\ndef reconstruct_prompt(model_response: ModelResponse):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ReconstructedPrompt,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"\"\"\\\n                    Give the concrete prompt (problem) that can\\\n                    generate this answer. The problem should\\\n                    contain all basic and necessary information\\\n                    and correspond to the answer. The problem\\\n                    can only ask for one result\\\n\\\n                    Reasoning: {model_response.chain_of_thought}\\\n                    Response: {model_response.correct_answer}\\\n                    \"\"\",\\\n            }\\\n        ],\n    )\n\ndef deconstruct_prompt_into_condition_list(prompt: str):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ConditionList,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": \"\"\"\\\n                You are an expert AI system that excels at\\\n                analyzing and decomposing questions into their\\\n                constituent parts.\\\n\\\n                Please list the conditions of the problem given\\\n                below. There might be multiple conditions in the\\\n                problem so make sure to navigate through the\\\n                prompt incrementally, indentifying and extracting\\\n                the conditions necessary to answer the question\\\n                in your final response.\\\n                \"\"\",\\\n            },\\\n            {\"role\": \"user\", \"content\": prompt},\\\n        ],\n    )\n\ndef generate_feedback(\n    original_condition_list: list[str], final_condition_list: list[str]\n):\n    formatted_original_conditions = \"\\n- \".join(original_condition_list)\n    formatted_final_conditions = \"\\n- \".join(final_condition_list)\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ModelFeedback,\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"\"\"\\\n                You are an expert AI system that excels at\\\n                analyzing and comparing two lists of conditions.\\\n\\\n                Original Condition List:\\\n                {formatted_original_conditions}\\\n\\\n                Reconstructed Condition List:\\\n                {formatted_final_conditions}\\\n\\\n                Determine if the two condition lists are roughly\\\n                equivalent. If they are not, give targetted\\\n                feedback on what is missing from the reconstructed\\\n                condition list as compared to the original condition\\\n                list and how it can be fixed.\\\n                \"\"\",\\\n            }\\\n        ],\n    )\n\ndef revise_response(response: ModelResponse, feedback: ModelFeedback):\n    formatted_inconsistencies = \"\\n- \".join(feedback.detected_inconsistencies)\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": f\"\"\"\\\n                Here are the mistakes and reasons in your answer\\\n                to the prompt\\\n\\\n                Original Response: {response.correct_answer}\\\n                You have overlooked some real conditions:\\\n                {formatted_inconsistencies}\\\n\\\n                Here are detailed reasons:\\\n                {feedback.feedback}\\\n\\\n                Generate a revised response that takes into account\\\n                the detailed feedback and includes the ignored\\\n                conditions\\\n                \"\"\",\\\n            }\\\n        ],\n        response_model=ModelResponse,\n    )\n\nif __name__ == \"__main__\":\n    query = \"\"\"\n    Mary is an avid gardener. Yesterday, she received 18 new\n    potted plants from her favorite plant nursery. She already\n    has 2 potted plants on each of the 40 window ledges of her\n    large backyard. How many potted plants will Mary remain\n    with?\n    \"\"\"\n    response = generate_response(query)\n    reconstructed_prompt = reconstruct_prompt(response)\n    print(reconstructed_prompt.reconstructed_prompt)\n    \"\"\"\n    Mary received 18 new potted plants. She already has 2 potted plants on each\n    of the 40 window ledges in her backyard. How many potted plants does she have now?\n    \"\"\"\n\n    original_condition_list = deconstruct_prompt_into_condition_list(query)\n    new_condition_list = deconstruct_prompt_into_condition_list(\n        reconstructed_prompt.reconstructed_prompt\n    )\n    print(original_condition_list.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"conditions\": [\\\n        \"Mary received 18 new potted plants.\",\\\n        \"Mary has 2 potted plants on each of the 40 window ledges in her backyard.\",\\\n        \"We are required to find the total number of potted plants Mary will have.\"\\\n      ]\n    }\n    \"\"\"\n    print(new_condition_list.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"conditions\": [\\\n        \"Mary received 18 new potted plants.\",\\\n        \"She already has 2 potted plants on each of the 40 window ledges in her backyard.\"\\\n      ]\n    }\n    \"\"\"\n\n    feedback = generate_feedback(\n        original_condition_list.conditions, new_condition_list.conditions\n    )\n    print(feedback.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"detected_inconsistencies\": [\\\n        \"The reconstructed list is missing the requirement\\\n        to find the total number of potted plants Mary will\\\n        have.\"\\\n      ],\n      \"feedback\": \"Add the requirement of finding the total\n      number of potted plants Mary will have to the\n      reconstructed condition list to match the original\n      condition list.\",\n      \"is_equal\": false\n    }\n    \"\"\"\n\n    if not feedback.is_equal:\n        response = revise_response(response, feedback)\n\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"chain_of_thought\": \"First, we note that Mary starts\n      with 18 potted plants. According to the problem, she\n      bought 2 packs of 40 new potted plants. So, to find\n      the total number of plants she will have, we add the\n      number of plants she initially has to the number she\n      bought. This gives us 18 (initial) + 2 * 40 (new) =\n      18 + 80 = 98 potted plants.\",\n      \"correct_answer\": \"98 potted plants\"\n    }\n    \"\"\"\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/self_criticism/reversecot/\\#references \"Permanent link\")\n\n1: [RCoT: Detecting And Rectifying Factual Inconsistency In Reasoning By Reversing Chain-Ofthought](https://arxiv.org/pdf/2305.11499)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/self_criticism/reversecot/",
      "ogUrl": "https://python.useinstructor.com/prompting/self_criticism/reversecot/",
      "title": "Reconstruct Prompt from Reasoning Steps - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/self_criticism/reversecot/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/reversecot.png",
      "ogTitle": "Reconstruct Prompt from Reasoning Steps - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/reversecot.png",
      "og:title": "Reconstruct Prompt from Reasoning Steps - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/self_criticism/reversecot/",
      "statusCode": 200,
      "description": "Reverse Chain Of Thought is a method to help identify logical inconsistencies in the reasoning steps of a large language model's response",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Reverse Chain Of Thought is a method to help identify logical inconsistencies in the reasoning steps of a large language model's response",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/reversecot.png",
      "twitter:title": "Reconstruct Prompt from Reasoning Steps - Instructor",
      "og:description": "Reverse Chain Of Thought is a method to help identify logical inconsistencies in the reasoning steps of a large language model's response",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Reverse Chain Of Thought is a method to help identify logical inconsistencies in the reasoning steps of a large language model's response"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/self_criticism/self_verification/#forward-reasoning)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/self_criticism/self_verification.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/self_criticism/self_verification.md \"View source of this page\")\n\n# Self-Verify Responses\n\nWe want to verify that an LLM response is correct. How can we automate this?\n\nThe self-verification framework generates multiple response candidates, then uses an LLM to verify these candidates. The process follows two stages:\n\n1. Forward Reasoning\n2. Backward Verification\n\n## Forward Reasoning [¶](https://python.useinstructor.com/prompting/self_criticism/self_verification/\\#forward-reasoning \"Permanent link\")\n\nIn forward reasoning, we leaverage CoT to generate multiple candidate solutions.\n\n## Backward Verification [¶](https://python.useinstructor.com/prompting/self_criticism/self_verification/\\#backward-verification \"Permanent link\")\n\nBackward verification involves three steps.\n\n### Rewrite As Declarative [¶](https://python.useinstructor.com/prompting/self_criticism/self_verification/\\#rewrite-as-declarative \"Permanent link\")\n\nRewrite the original question and its solution as a declarative.\n\nRewritten Declaritive Example\n\n**original question**: Jackie has 10 apples. Adam has 8 apples. How many more apples does Jackie have than Adam? **response candidate**: Jackie has 10 apples. so Jackie has 10-8=2 more apples than Adam, and the answer is 2. **rewritten declarative**: Jackie has 10 apples. Adam has 8 apples. Jackie has 2 more apples than Adam.\n\n### Construct New Question [¶](https://python.useinstructor.com/prompting/self_criticism/self_verification/\\#construct-new-question \"Permanent link\")\n\nConstruct a new question and prompt the LLM to verify it. Two possible methods are:\n\n1. True-False Item Verification (TFV)\n2. Condition Mask Verification (CMV)\n\nTFV asks the LLM if the rewritten declarative is correct. CMV filters out conditions provided in the original question and asks an LLM to predict the filtered condition.\n\nTFV Example Prompt\n\nJackie has 10 apples. Adam has 8 apples. Jackie has 2 more apples than Adam. Is this correct?\n\nCMV Example Prompt\n\nJackie has X apples. Adam has 8 apples. Jackie has 2 more apples than Adam. What is X?\n\n### Compute Verification Score [¶](https://python.useinstructor.com/prompting/self_criticism/self_verification/\\#compute-verification-score \"Permanent link\")\n\nThe LLM is then queried with the new question for each candidate _k_ times. If TFV is used, the verification score is simply the number of times the LLM outputs \"True\". If CMV is used, the verification score is the number of times the masked value and the real value match.\n\nThe candidate with the highest verification score is then chosen as the final answer.\n\n## Implementation [¶](https://python.useinstructor.com/prompting/self_criticism/self_verification/\\#implementation \"Permanent link\")\n\nThe full pipeline with forward reasoning and backward verification can be implemented using `instructor` as seen below:\n\n```md-code__content\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nfrom typing import Literal\n\nclient = instructor.from_openai(OpenAI())\n\nn = 3  # Number of candidates to generate\nk = 5  # Number of times to verify\n\nclass Date(BaseModel):\n    month: int\n    day: int\n\nclass Candidate(BaseModel):\n    reasoning_steps: list[str]\n    month: str\n\nclass Rewritten(BaseModel):\n    declarative: str\n\nclass Verification(BaseModel):\n    correct: Literal[\"True\", \"False\"]\n\ndef query_llm(query, model):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=model,\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"Think step by step: {query}\",\\\n            }\\\n        ],\n    )\n\ndef rewrite(query, candidate):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Rewritten,\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                    Please change the questions and answers into complete declarative sentences\\\n                    {query}\\\n                    The answer is {candidate.month}.\\\n                \"\"\",\\\n            }\\\n        ],\n    )\n\ndef verify(question):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Verification,\n        messages=[{\"role\": \"user\", \"content\": question}],\n    )\n\nif __name__ == \"__main__\":\n    query = \"What month is it now if it has been 3 weeks, 10 days, and 2 hours since May 1, 2024 6pm?\"\n\n    # Step 1: Forward Reasoning\n    candidates = [query_llm(query, Candidate) for _ in range(n)]\n\n    # Step 2: Backwards Verification\n    for candidate in candidates:\n        # 2.a Rewrite\n        rewritten = rewrite(query, candidate)\n        # 2.b Construct new questions\n        question = f\"{rewritten.declarative} Do it is correct (True or False)?\"\n        # 2.c Compute verification score\n        scores = [verify(question).correct for _ in range(k)]\n        verification_score = sum(1 for s in scores if s == \"True\")\n\n        print(f\"Candidate: {candidate.month}, Verification Score: {verification_score}\")\n        #> Candidate: May, Verification Score: 0\n        #> Candidate: June, Verification Score: 2\n        #> Candidate: May, Verification Score: 1\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/self_criticism/self_verification/\\#references \"Permanent link\")\n\n1: [Large Language Models are Better Reasoners with Self-Verification](https://arxiv.org/abs/2212.09561)\n\n\\*: [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/self_criticism/self_verification/",
      "ogUrl": "https://python.useinstructor.com/prompting/self_criticism/self_verification/",
      "title": "Self-Verify LLM Responses - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/self_criticism/self_verification/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/self_verification.png",
      "ogTitle": "Self-Verify LLM Responses - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/self_verification.png",
      "og:title": "Self-Verify LLM Responses - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/self_criticism/self_verification/",
      "statusCode": 200,
      "description": "The self-verification framework generates multiple response candidates, then uses an LLM to verify these candidates.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "The self-verification framework generates multiple response candidates, then uses an LLM to verify these candidates.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/self_verification.png",
      "twitter:title": "Self-Verify LLM Responses - Instructor",
      "og:description": "The self-verification framework generates multiple response candidates, then uses an LLM to verify these candidates.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "The self-verification framework generates multiple response candidates, then uses an LLM to verify these candidates."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/ensembling/max_mutual_information/#whats-max-mutual-information)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/ensembling/max_mutual_information.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/ensembling/max_mutual_information.md \"View source of this page\")\n\n# Use Ensembles To Test Prompts\n\n## What's Max Mutual Information? [¶](https://python.useinstructor.com/prompting/ensembling/max_mutual_information/\\#whats-max-mutual-information \"Permanent link\")\n\nMax Mutual Information Method is a method of prompting that aims to find the best prompt to elicit the desired response from a LLM. We do so by maximising a metric called Mutual Information - which indicates the reduction in a model's uncertainty as a result of the prompt.\n\n### Entropy [¶](https://python.useinstructor.com/prompting/ensembling/max_mutual_information/\\#entropy \"Permanent link\")\n\nWhen a language model recieves a prompt as input, it outputs a series of token probabilities sequentially until it reaches the `<EOS>` token. In the paper, they take the final probability distribution as P(Y∣X)P(Y\\|X)P(Y∣X) where YYY is the final prediction of the model and XXX the prompt.\n\nWhen we have a probability distribution, we can calculate a probability known as entropy. The lower this value is, the better. This is because a lower entropy value means that the model is more confident in its prediction.\n\nWe can calculate entropy with the following formula where P(Ti)P(T\\_i)P(Ti​) represents the probability of the iii-th token in the final output distribution.\n\nH(P(Y∣X))=∑i=0nP(Ti)log(P(Ti)) H(P(Y\\|X)) = \\\\sum\\_{i=0}^n P(T\\_i) log (P(T\\_i)) H(P(Y∣X))=i=0∑n​P(Ti​)log(P(Ti​))\n\n### Mutual Information [¶](https://python.useinstructor.com/prompting/ensembling/max_mutual_information/\\#mutual-information \"Permanent link\")\n\n![](https://python.useinstructor.com/img/mutual_information.png)\n\nWe can apply this to the calculation of Mutual Information as seen above.\n\nWe'll indicate the calculate of entropy of a probability distribution as H(X)H(X)H(X) where XXX here represents a final probability distribution. We also assume you have a train dataset of nnn examples to use.\n\n1. First, we choose a set of tokens that are likely to be part of the final answer. This could be words that appear inside the choices we have provided.\n\n2. Once we've chosen these tokens, we extract out the log probs for each token from our final distribution. We then normalise it so that these new log probs now sum up to 1.\n\n3. We do this for the nnn example inside our train set, this gives us a new distribution P(Yi∣Xi)P(Y\\_i\\|X\\_i)P(Yi​∣Xi​) for each iii-th example.\n\n4. We then take the average of these nnn distributions to get HmarginalH\\_{marginal}Hmarginal​\n\n5. We then calculate the average of the entropy of each distribution to get HconditionalH\\_{conditional}Hconditional​\n\n6. We then derive the Mutual Information by taking Hmarginal−HconditionalH\\_{marginal} - H\\_{conditional}Hmarginal​−Hconditional​, the higher this metric the better.\n\n\nUnsure how to calculate HmarginalH\\_{marginal}Hmarginal​ and H\\_conditionalH\\\\\\_{conditional}H\\_conditional\n\nHmarginal=H(1n∑i=0nP(Yi∣Xi)) H\\_{marginal} = H(\\\\frac{1}{n} \\\\sum\\_{i=0}^n P(Y\\_i \\| X\\_i) ) Hmarginal​=H(n1​i=0∑n​P(Yi​∣Xi​))\n\nHconditional=1n∑i=0nH(P(Yi∣Xi)) H\\_{conditional} = \\\\frac{1}{n} \\\\sum\\_{i=0}^n H(P(Y\\_i\\|X\\_i)) Hconditional​=n1​i=0∑n​H(P(Yi​∣Xi​))\n\nWe can then use this new mutual information metric to compare the effectiveness of different prompts at eliciting a desired response from our train dataset.\n\n## Implementation [¶](https://python.useinstructor.com/prompting/ensembling/max_mutual_information/\\#implementation \"Permanent link\")\n\nSince we don't have access to the raw log probabilites of specific tokens we want in the OpenAI API, we'll instead get the language model to generate a final score from 1 - 10 of its confidence in it's prediction.\n\nWe'll then convert this to a probability distribution with two outcomes and calculate a value for the entropy off of that.\n\nNext we'll compare the Mutual Information value for different prompts before choosing what the best prompt is. For this example, we'll be using values from the Story Cloze set.\n\n```md-code__content\nfrom openai import AsyncOpenAI\nfrom instructor import from_openai\nfrom pydantic import BaseModel\nfrom typing import Callable, Literal\nfrom textwrap import dedent\nimport math\nimport asyncio\n\nclass Response(BaseModel):\n    chain_of_thought: str\n    response: Literal[\"A\", \"B\"]\n    confidence: Literal[\\\n        \"Very High Confidence\",\\\n        \"High Confidence\",\\\n        \"Moderate Confidence\",\\\n        \"Low Confidence\",\\\n        \"Very Low Confidence\",\\\n    ]\n\n    def generate_score(self) -> float:\n        confidence_scores = {\n            \"Very High Confidence\": 1,\n            \"High Confidence\": 0.8,\n            \"Moderate Confidence\": 0.6,\n            \"Low Confidence\": 0.4,\n            \"Very Low Confidence\": 0.2,\n        }\n        return confidence_scores[self.confidence]\n\nclient = from_openai(AsyncOpenAI())\n\ndef prompt_template_1(question: str, options: list[str]):\n    assert len(options) == 2\n    a, b = options\n\n    return dedent(\n        f\"\"\"\n    You are a world class AI System which excels at understanding complex user stories and generating responses. Output your prediction and also quantify your confidence in your prediction with the following scale.\n\n    - Very High Confidence: The model is highly confident in its prediction, displaying deep understanding, flawless execution, and no noticeable errors.\n    - High Confidence: The model is confident in its prediction, with strong relevance and minor errors that do not detract from overall quality.\n    - Moderate Confidence: The model has moderate confidence in its prediction, which is generally relevant with some inaccuracies, and meets minimum requirements.\n    - Low Confidence: The model has low confidence in its prediction, with limited relevance and several inaccuracies.\n    - Very Low Confidence: The model has very low confidence in its prediction, which is largely irrelevant, inaccurate, or incomplete, needing significant improvement\n\n    Context\n    {question}\n\n    Options\n    A. {a}\n    B. {b}\n    \"\"\"\n    )\n\ndef prompt_template_2(question: str, options: list[str]):\n    assert len(options) == 2\n    a, b = options\n\n    return dedent(\n        f\"\"\"\n    <prompt>\n        <Task>\n        You are about to be passed a story. You are to select the correct response from the options provided.\n\n         <confidence-levels>\n             <level>\n                 <name>Very High Confidence</name>\n                 <description>The model is highly confident in its prediction, displaying deep understanding, flawless execution, and no noticeable errors.</description>\n             </level>\n             <level>\n                 <name>High Confidence</name>\n                 <description>The model is confident in its prediction, with strong relevance and minor errors that do not detract from overall quality.</description>\n             </level>\n             <level>\n                 <name>Moderate Confidence</name>\n                 <description>The model has moderate confidence in its prediction, which is generally relevant with some inaccuracies, and meets minimum requirements.</description>\n             </level>\n             <level>\n                 <name>Low Confidence</name>\n                 <description>The model has low confidence in its prediction, with limited relevance and several inaccuracies.</description>\n             </level>\n             <level>\n                 <name>Very Low Confidence</name>\n                 <description>The model has very low confidence in its prediction, which is largely irrelevant, inaccurate, or incomplete, needing significant improvement</description>\n             </level>\n         </confidence-levels>\n        </Task>\n\n        <Question>\n        {question}\n        </Question>\n\n        <Options>\n        <option>A: {a}</option>\n        <option>B: {b}</option>\n        </Options>\n    </prompt>\n    \"\"\"\n    )\n\nasync def generate_response(\n    question: str, options: list[str], prompt_template: Callable[[str, list[str]], str]\n):\n    return await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\\\n            {\\\n                \"role\": \"system\",\\\n                \"content\": prompt_template(question, options),\\\n            }\\\n        ],\n        response_model=Response,\n    )\n\nasync def generate_responses(\n    questions: list[str], prompt_template: Callable[[str, list[str]], str]\n):\n    return await asyncio.gather(\n        *[\\\n            generate_response(\\\n                question=question[\"question\"],\\\n                options=question[\"options\"],\\\n                prompt_template=prompt_template,\\\n            )\\\n            for question in questions\\\n        ]\n    )\n\ndef calculate_entropy(probs: list[float]) -> float:\n    return sum([p * math.log(p) if p != 0 else 0 for p in probs])\n\ndef calculate_mutual_information(predictions: list[Response]) -> float:\n    probs = [\\\n        [prediction.generate_score(), 1 - prediction.generate_score()]\\\n        for prediction in predictions\\\n    ]\n\n    avg_probs = [0, 0]\n\n    for p1, p2 in probs:\n        avg_probs[0] += p1\n        avg_probs[1] += p2\n\n    h_marginal = calculate_entropy([i / len(probs) for i in avg_probs])\n    h_conditional = sum([calculate_entropy(prob) for prob in probs]) / len(probs)\n\n    return h_marginal - h_conditional\n\nif __name__ == \"__main__\":\n    queries = [\\\n        {\\\n            \"question\": \"Karen was assigned a roommate her first year of college. Her roommate asked her to go to a nearby city for a concert. Karen agreed happily. The show was absolutely exhilarating.\",\\\n            \"options\": [\\\n                \"Karen became good friends with her roommate.\",\\\n                \"Karen hated her roommate.\",\\\n            ],\\\n        },\\\n        {\\\n            \"question\": \"Jim got his first credit card in college. He didn’t have a job so he bought everything on his card. After he graduated he amounted a $10,000 debt. Jim realized that he was foolish to spend so much money.    \",\\\n            \"options\": [\\\n                \"Jim decided to devise a plan for repayment.\",\\\n                \"Jim decided to open another credit card.\",\\\n            ],\\\n        },\\\n        {\\\n            \"question\": \"Gina misplaced her phone at her grandparents. It wasn’t anywhere in the living room. She realized she was in the car before. She grabbed her dad’s keys and ran outside.\",\\\n            \"options\": [\\\n                \"She found her phone in the car.\",\\\n                \"She didn’t want her phone anymore.\",\\\n            ],\\\n        },\\\n    ]\n\n    best_mi_score = float(\"-inf\")\n    best_template = None\n\n    for prompt_template in [prompt_template_1, prompt_template_2]:\n        responses = asyncio.run(generate_responses(queries, prompt_template))\n        mi_score = calculate_mutual_information(responses)\n        print(f\"{prompt_template.__name__}: {mi_score}\")\n        #> prompt_template_1: -0.0781292189485728\n        #> prompt_template_2: -0.05907285153542691\n        if mi_score > best_mi_score:\n            best_mi_score = mi_score\n            best_template = prompt_template.__name__\n\n    print(best_template, best_mi_score)\n    #> prompt_template_2 -0.05907285153542691\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/ensembling/max_mutual_information/",
      "ogUrl": "https://python.useinstructor.com/prompting/ensembling/max_mutual_information/",
      "title": "Use Ensembles To Test Prompts - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/ensembling/max_mutual_information/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/max_mutual_information.png",
      "ogTitle": "Use Ensembles To Test Prompts - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/max_mutual_information.png",
      "og:title": "Use Ensembles To Test Prompts - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/ensembling/max_mutual_information/",
      "statusCode": 200,
      "description": "Max Mutual Information creates multiple prompt templates and then selects the optimal template as the one which maximises mutual information between the prompt and the LLM's outputs",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Max Mutual Information creates multiple prompt templates and then selects the optimal template as the one which maximises mutual information between the prompt and the LLM's outputs",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/ensembling/max_mutual_information.png",
      "twitter:title": "Use Ensembles To Test Prompts - Instructor",
      "og:description": "Max Mutual Information creates multiple prompt templates and then selects the optimal template as the one which maximises mutual information between the prompt and the LLM's outputs",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Max Mutual Information creates multiple prompt templates and then selects the optimal template as the one which maximises mutual information between the prompt and the LLM's outputs"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/few_shot/example_ordering/#example-ordering)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/few_shot/example_ordering.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/few_shot/example_ordering.md \"View source of this page\")\n\n# Example Ordering [¶](https://python.useinstructor.com/prompting/few_shot/example_ordering/\\#example-ordering \"Permanent link\")\n\nThe order of few-shot examples in the prompt can affect LLM outputs [1](https://arxiv.org/abs/2104.08786) [2](https://arxiv.org/abs/2106.01751) [3](https://arxiv.org/abs/2101.06804) [4](https://aclanthology.org/2022.naacl-main.191/)[\\*](https://arxiv.org/abs/2406.06608). Consider permutating the order of these examples in your prompt to achieve better results.\n\n## Choosing Your Examples [¶](https://python.useinstructor.com/prompting/few_shot/example_ordering/\\#choosing-your-examples \"Permanent link\")\n\nDepending on your use-case, here are a few different methods that you can consider using to improve the quality of your examples.\n\n### Combinatorics [¶](https://python.useinstructor.com/prompting/few_shot/example_ordering/\\#combinatorics \"Permanent link\")\n\nOne of the easiest methods is for us to manually iterate over each of the examples that we have and try all possible combinations we could create. This will in turn allow us to find the best combination that we can find.\n\n### KATE [¶](https://python.useinstructor.com/prompting/few_shot/example_ordering/\\#kate \"Permanent link\")\n\nKATE (k-Nearest Example Tuning) is a method designed to enhance GPT-3's performance by selecting the most relevant in-context examples. The method involves:\n\nFor each example in the test set, K nearest neighbors (examples) are retrieved based on semantic similarity. Among these K examples, those that appear most frequently across different queries are selected as the best in-context examples.\n\n### Using a Unsupervised Retriever [¶](https://python.useinstructor.com/prompting/few_shot/example_ordering/\\#using-a-unsupervised-retriever \"Permanent link\")\n\n![Retriever Image](https://python.useinstructor.com/img/retriever.png)\n\nWe can use a large LLM to compute a single score for each example with respect to a given prompt. This allows us to create a training set that scores an example's relevance when compared against a prompt. Using this training set, we can train a model that mimics this functionality. This allows us to determine the top `k` most relevant and most irrelevant examples when a user makes a query so that we can include this in our final prompt.\n\n### References [¶](https://python.useinstructor.com/prompting/few_shot/example_ordering/\\#references \"Permanent link\")\n\n1: [Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity](https://arxiv.org/abs/2104.08786)\n\n2: [Reordering Examples Helps during Priming-based Few-Shot Learning](https://arxiv.org/abs/2106.01751)\n\n3: [What Makes Good In-Context Examples for GPT-3?](https://arxiv.org/abs/2101.06804)\n\n4: [Learning To Retrieve Prompts for In-Context Learning](https://aclanthology.org/2022.naacl-main.191/)\n\n\\*: [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/few_shot/example_ordering/",
      "ogUrl": "https://python.useinstructor.com/prompting/few_shot/example_ordering/",
      "title": "Example Ordering - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/few_shot/example_ordering/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/few_shot/example_ordering.png",
      "ogTitle": "Example Ordering - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/few_shot/example_ordering.png",
      "og:title": "Example Ordering - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/few_shot/example_ordering/",
      "statusCode": 200,
      "description": "LLM outputs are heavily impacted by ordering of few shot examples",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "LLM outputs are heavily impacted by ordering of few shot examples",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/few_shot/example_ordering.png",
      "twitter:title": "Example Ordering - Instructor",
      "og:description": "LLM outputs are heavily impacted by ordering of few shot examples",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "LLM outputs are heavily impacted by ordering of few shot examples"
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/decomposition/tree-of-thought.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/decomposition/tree-of-thought.md \"View source of this page\")\n\n# Tree-of-Thought\n\n\\[wip\\]\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/decomposition/tree-of-thought/",
      "ogUrl": "https://python.useinstructor.com/prompting/decomposition/tree-of-thought/",
      "title": "Tree-of-Thought - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/decomposition/tree-of-thought/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/tree-of-thought.png",
      "ogTitle": " - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/tree-of-thought.png",
      "og:title": " - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/decomposition/tree-of-thought/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/decomposition/tree-of-thought.png",
      "twitter:title": " - Instructor",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": []
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/zero_shot/rar/#implementation)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/zero_shot/rar.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/zero_shot/rar.md \"View source of this page\")\n\n# Clarify Ambiguous Information\n\nHow can we identify and clarify ambigious information in the prompt?\n\nLet's say we are given the query: _Was Ed Sheeran born on an odd month?_\n\nThere are many ways a model might interpret an _odd month_:\n\n- Februray is _odd_ because of an irregular number of days.\n- A month is _odd_ if it has an odd number of days.\n- A month is _odd_ if its numberical order in the year is odd (i.e. Janurary is the 1st month).\n\nNote\n\nAmbiguities might not always be so obvious!\n\nTo help the model better infer human intention from ambigious prompts, we can ask the model to rephrase and respond (RaR).\n\n## Implementation [¶](https://python.useinstructor.com/prompting/zero_shot/rar/\\#implementation \"Permanent link\")\n\n```md-code__content\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass Response(BaseModel):\n    rephrased_question: str\n    answer: str\n\ndef rephrase_and_respond(query):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"{query}\\nRephrase and expand the question, and respond.\"\"\",\\\n            }\\\n        ],\n        response_model=Response,\n    )\n\nif __name__ == \"__main__\":\n    query = \"Take the last letters of the words in 'Edgar Bob' and concatinate them.\"\n\n    response = rephrase_and_respond(query)\n\n    print(response.rephrased_question)\n    \"\"\"\n    What are the last letters of each word in the name 'Edgar Bob', and what do you get when you concatenate them?\n    \"\"\"\n    print(response.answer)\n    \"\"\"\n    To find the last letters of each word in the name 'Edgar Bob', we look at 'Edgar' and 'Bob'. The last letter of 'Edgar' is 'r' and the last letter of 'Bob' is 'b'. Concatenating these letters gives us 'rb'.\n    \"\"\"\n\n```\n\nThis can also be implemented as two-step RaR:\n\n1. Ask the model to rephrase the question.\n2. Pass the rephrased question back to the model to generate the final response.\n\n## References [¶](https://python.useinstructor.com/prompting/zero_shot/rar/\\#references \"Permanent link\")\n\n1: [Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves](https://arxiv.org/abs/2311.04205)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/zero_shot/rar/",
      "ogUrl": "https://python.useinstructor.com/prompting/zero_shot/rar/",
      "title": "Clarify Ambiguous Information - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/zero_shot/rar/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/rar.png",
      "ogTitle": "Clarify Ambiguous Information - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/rar.png",
      "og:title": "Clarify Ambiguous Information - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/zero_shot/rar/",
      "statusCode": 200,
      "description": "To help the model better infer human intention from ambigious prompts, we can ask the model to rephrase and respond (RaR).",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "To help the model better infer human intention from ambigious prompts, we can ask the model to rephrase and respond (RaR).",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/zero_shot/rar.png",
      "twitter:title": "Clarify Ambiguous Information - Instructor",
      "og:description": "To help the model better infer human intention from ambigious prompts, we can ask the model to rephrase and respond (RaR).",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "To help the model better infer human intention from ambigious prompts, we can ask the model to rephrase and respond (RaR)."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/self_criticism/self_calibration/#references)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/self_criticism/self_calibration.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/self_criticism/self_calibration.md \"View source of this page\")\n\n# Determine Uncertainty of Reasoning Chain\n\nWe want our language models to be able to output the extent of their confidence in predictions. To do so, we can get language models to evaluate their responses to a given prompt using a technique called Self Calibration [1](https://arxiv.org/pdf/2207.05221)\n\n> The original paper used a fine-tuned regression head over the language model's final output. However, since we don't have access to the model's final hidden states, we can substitute it for a function call instead to achieve a similar result.\n\nWe can ask language models to evaluate their outputs by using the following template\n\nWe can implement this using `instructor` as seen below\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(OpenAI())\n\nclass SelfCalibration(BaseModel):\n    chain_of_thought: str\n    is_valid_answer: bool = Field(description=\"Whether the answer is correct or not\")\n\ndef evaluate_model_output(original_prompt: str, model_response: str):\n    return client.chat.completions.create(\n        messages=[\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": f\"\"\"\\\n                Question: {original_prompt}\\\n\\\n                {model_response}\\\n\\\n                Is this a valid answer to the question?\\\n                Make sure to examine the question\\\n                thoroughly and generate a complete\\\n                reasoning for why the answer is correct\\\n                or not before responding.\\\n                \"\"\",\\\n            }\\\n        ],\n        response_model=SelfCalibration,\n        model=\"gpt-4o\",\n    )\n\nif __name__ == \"__main__\":\n    original_prompt = \"\"\"\n    Question: Who was the third president of the\n    United States?\n    \"\"\"\n    model_response = \"\"\"\n    Here are some brainstormed ideas: James Monroe\n    Thomas Jefferson\n    Jefferson\n    Thomas Jefferson\n    George Washington\n    \"\"\"\n    response = evaluate_model_output(original_prompt, model_response)\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"chain_of_thought\": \"Let's examine the question\n      carefully: 'Who was the third president of the\n      United States?'\\n\\nThe brainstormed ideas are:\n      \\n1. James Monroe\\n2. Thomas Jefferson\\n3.\n      Jefferson\\n4. Thomas Jefferson\\n5. George\n      Washington.\\n\\nTo determine the validity of these\n      answers, I'll cross-check with historical\n      records.\\n\\n1. James Monroe was not the third\n      president; he was the fifth president.\\n2. Thomas\n      Jefferson was indeed the third president of the\n      United States.\\n3. 'Jefferson' is a correct but\n      incomplete answer; it lacks the first name, though\n      it is commonly understood.\\n4. 'Thomas Jefferson'\n      is the full name and correct answer.\\n5. George\n      Washington was the first president, not the\n      third.\\n\\nTherefore, the correct, valid answer to\n      the question 'Who was the third president of the\n      United States?' is 'Thomas Jefferson,' and this\n      answer is correct.\",\n      \"is_valid_answer\": true\n    }\n    \"\"\"\n\n```\n\n### References [¶](https://python.useinstructor.com/prompting/self_criticism/self_calibration/\\#references \"Permanent link\")\n\n1: [Language Models (Mostly) Know What They Know](https://arxiv.org/pdf/2207.05221)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/self_criticism/self_calibration/",
      "ogUrl": "https://python.useinstructor.com/prompting/self_criticism/self_calibration/",
      "title": "Determine Uncertainty of Reasoning Chain - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/self_criticism/self_calibration/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/self_calibration.png",
      "ogTitle": "Determine Uncertainty of Reasoning Chain - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/self_calibration.png",
      "og:title": "Determine Uncertainty of Reasoning Chain - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/self_criticism/self_calibration/",
      "statusCode": 200,
      "description": "Self Calibration aims to get language models to determine what they know and do not know",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Self Calibration aims to get language models to determine what they know and do not know",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/self_criticism/self_calibration.png",
      "twitter:title": "Determine Uncertainty of Reasoning Chain - Instructor",
      "og:description": "Self Calibration aims to get language models to determine what they know and do not know",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Self Calibration aims to get language models to determine what they know and do not know"
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/prompting/?q=#prompting-guide)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/prompting/index.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/prompting/index.md \"View source of this page\")\n\n# Prompting Guide [¶](https://python.useinstructor.com/prompting/?q=\\#prompting-guide \"Permanent link\")\n\nPrompting requires an understanding of techniques to enhance model performance.\n\nThe team at [Learn Prompting](https://learnprompting.org) released The [Prompt Report](https://trigaten.github.io/Prompt_Survey_Site) in collaboration with researchers from OpenAI, Microsoft, and Google. This report surveys over 1,500 prompting papers and condenses the findings into a list of 58 distinct prompting techniques.\n\nHere are examples of the 58 prompting techniques\\* using `instructor`.\n\nPrompting techniques are separated into the following categories: - [Prompting Guide](https://python.useinstructor.com/prompting/?q=#prompting-guide) \\- [Zero-Shot](https://python.useinstructor.com/prompting/?q=#zero-shot) \\- [Few-Shot](https://python.useinstructor.com/prompting/?q=#few-shot) \\- [Thought Generation](https://python.useinstructor.com/prompting/?q=#thought-generation) \\- [Zero Shot](https://python.useinstructor.com/prompting/?q=#zero-shot-1) \\- [Few Shot](https://python.useinstructor.com/prompting/?q=#few-shot-1) \\- [Ensembling](https://python.useinstructor.com/prompting/?q=#ensembling) \\- [Self-Criticism](https://python.useinstructor.com/prompting/?q=#self-criticism) \\- [Decomposition](https://python.useinstructor.com/prompting/?q=#decomposition)\n\nClick links to learn about each method and how to apply them in prompts.\n\n## Zero-Shot [¶](https://python.useinstructor.com/prompting/?q=\\#zero-shot \"Permanent link\")\n\nHow do we increase the performance of our model without any examples?\n\n1. [Use Emotional Language](https://python.useinstructor.com/prompting/zero_shot/emotion_prompting/)\n2. [Assign a Role](https://python.useinstructor.com/prompting/zero_shot/role_prompting/)\n3. [Define a Style](https://python.useinstructor.com/prompting/zero_shot/style_prompting/)\n4. [Auto-Refine The Prompt](https://python.useinstructor.com/prompting/zero_shot/s2a/)\n5. [Simulate a Perspective](https://python.useinstructor.com/prompting/zero_shot/simtom/)\n6. [Clarify Ambiguous Information](https://python.useinstructor.com/prompting/zero_shot/rar/)\n7. [Ask Model To Repeat Query](https://python.useinstructor.com/prompting/zero_shot/re2/)\n8. [Generate Follow-Up Questions](https://python.useinstructor.com/prompting/zero_shot/self_ask/)\n\n## Few-Shot [¶](https://python.useinstructor.com/prompting/?q=\\#few-shot \"Permanent link\")\n\nHow do we choose effective examples to include in our prompt?\n\n1. [Auto-Generate Examples](https://python.useinstructor.com/prompting/few_shot/example_generation/sg_icl/)\n2. [Re-Order Examples](https://python.useinstructor.com/prompting/few_shot/example_ordering/)\n3. [Choose Examples Similar to the Query (KNN)](https://python.useinstructor.com/prompting/few_shot/exemplar_selection/knn/)\n4. [Choose Examples Similar to the Query (Vote-K)](https://python.useinstructor.com/prompting/few_shot/exemplar_selection/vote_k/)\n\n## Thought Generation [¶](https://python.useinstructor.com/prompting/?q=\\#thought-generation \"Permanent link\")\n\nHow do we encourage our model to mimic human-like reasoning?\n\n## Zero Shot [¶](https://python.useinstructor.com/prompting/?q=\\#zero-shot-1 \"Permanent link\")\n\n1. [Auto-Generate Chain-Of-Thought Examples](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_zero_shot/analogical_prompting/)\n2. [First Ask a Higher-Level Question](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_zero_shot/step_back_prompting/)\n3. [Encourage Analysis](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_zero_shot/thread_of_thought/)\n4. [Encourage Structural Reasoning](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_zero_shot/tab_cot/)\n\n## Few Shot [¶](https://python.useinstructor.com/prompting/?q=\\#few-shot-1 \"Permanent link\")\n\n1. [Annotate Only Uncertain Examples](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/active_prompt/)\n2. [Choose Diverse Examples](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/auto_cot/)\n3. [Choose Complex Examples](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/complexity_based/)\n4. [Include Incorrect Demonstrations](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/contrastive/)\n5. [Choose Similar, Auto-Generated, High-Certainty Chain-Of-Thought Reasonings](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/memory_of_thought/)\n6. [Choose the Most Certain Reasoning](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/uncertainty_routed_cot/)\n7. [Generate Template-Based Prompts](https://python.useinstructor.com/prompting/thought_generation/chain_of_thought_few_shot/prompt_mining/)\n\n## Ensembling [¶](https://python.useinstructor.com/prompting/?q=\\#ensembling \"Permanent link\")\n\nHow can we use multiple prompts and aggregate their responses?\n\n01. [Build a Set of Consistent, Diverse Examples](https://python.useinstructor.com/prompting/ensembling/cosp/)\n02. [Batch In-Context Examples](https://python.useinstructor.com/prompting/ensembling/dense/)\n03. [Verify Individual Reasoning Steps](https://python.useinstructor.com/prompting/ensembling/diverse/)\n04. [Maximize Information Between Input and Output](https://python.useinstructor.com/prompting/ensembling/max_mutual_information/)\n05. [Merge Multiple Chains-Of-Thought](https://python.useinstructor.com/prompting/ensembling/meta_cot/)\n06. [Use Specialized Experts](https://python.useinstructor.com/prompting/ensembling/more/)\n07. [Choose The Most Consistent Reasoning](https://python.useinstructor.com/prompting/ensembling/self_consistency/)\n08. [Choose The Most Consistent Reasioning (Universal)](https://python.useinstructor.com/prompting/ensembling/universal_self_consistency/)\n09. [Use Task-Specific Example Selection](https://python.useinstructor.com/prompting/ensembling/usp/)\n10. [Paraphrase The Prompt](https://python.useinstructor.com/prompting/ensembling/prompt_paraphrasing/)\n\n## Self-Criticism [¶](https://python.useinstructor.com/prompting/?q=\\#self-criticism \"Permanent link\")\n\nHow can a model verify or critique its own response?\n\n1. [Generate Verification Questions](https://python.useinstructor.com/prompting/self_criticism/chain_of_verification/)\n2. [Ask If the Answer is Correct](https://python.useinstructor.com/prompting/self_criticism/self_calibration/)\n3. [Generate Feedback and Auto-Improve](https://python.useinstructor.com/prompting/self_criticism/self_refine/)\n4. [Score Multiple Candidate Solutions](https://python.useinstructor.com/prompting/self_criticism/self_verification/)\n5. [Reconstruct The Problem](https://python.useinstructor.com/prompting/self_criticism/reversecot/)\n6. [Generate Possible Steps](https://python.useinstructor.com/prompting/self_criticism/cumulative_reason/)\n\n## Decomposition [¶](https://python.useinstructor.com/prompting/?q=\\#decomposition \"Permanent link\")\n\nHow can we break down complex problems? How do we solve subproblems?\n\n1. [Implement Subproblems As Functions](https://python.useinstructor.com/prompting/decomposition/decomp/)\n2. [Use Natural and Symbolic Language](https://python.useinstructor.com/prompting/decomposition/faithful_cot/)\n3. [Solve Increasingly Complex Subproblems](https://python.useinstructor.com/prompting/decomposition/least_to_most/)\n4. [Generate a Plan](https://python.useinstructor.com/prompting/decomposition/plan_and_solve/)\n5. [Use Code As Reasoning](https://python.useinstructor.com/prompting/decomposition/program_of_thought/)\n6. [Recursively Solve Subproblems](https://python.useinstructor.com/prompting/decomposition/recurs_of_thought/)\n7. [Generate a Skeleton](https://python.useinstructor.com/prompting/decomposition/skeleton_of_thought/)\n8. [Search Through Subproblems](https://python.useinstructor.com/prompting/decomposition/tree-of-thought/)\n\n\\*: [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/prompting/?q=",
      "ogUrl": "https://python.useinstructor.com/prompting/",
      "title": "Comprehensive Guide to Prompting Techniques - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/prompting/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/prompting/index.png",
      "ogTitle": "Comprehensive Guide to Prompting Techniques - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/prompting/index.png",
      "og:title": "Comprehensive Guide to Prompting Techniques - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/prompting/?q=",
      "statusCode": 200,
      "description": "Explore 58 effective prompting techniques categorized for enhanced model performance in AI prompts.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Explore 58 effective prompting techniques categorized for enhanced model performance in AI prompts.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/prompting/index.png",
      "twitter:title": "Comprehensive Guide to Prompting Techniques - Instructor",
      "og:description": "Explore 58 effective prompting techniques categorized for enhanced model performance in AI prompts.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Explore 58 effective prompting techniques categorized for enhanced model performance in AI prompts."
    }
  }
]