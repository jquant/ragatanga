[
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/templating/#prompt-templating)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/templating.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/templating.md \"View source of this page\")\n\n# Prompt Templating [¶](https://python.useinstructor.com/concepts/templating/\\#prompt-templating \"Permanent link\")\n\nWith Instructor's Jinja templating, you can:\n\n- Dynamically adapt prompts to any context\n- Easily manage and version your prompts better\n- Integrate seamlessly with validation processes\n- Handle sensitive information securely\n\nOur solution offers:\n\n- Separation of prompt structure and content\n- Complex logic implementation within prompts\n- Template reusability across scenarios\n- Enhanced prompt versioning and logging\n- Pydantic integration for validation and type safety\n\n## Context is available to the templating engine [¶](https://python.useinstructor.com/concepts/templating/\\#context-is-available-to-the-templating-engine \"Permanent link\")\n\nThe `context` parameter is a dictionary that is passed to the templating engine. It is used to pass in the relevant variables to the templating engine. This single `context` parameter will be passed to jinja to render out the final prompt.\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nresp = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"\"\"Extract the information from the\\\n        following text: `{{ data }}`\"\"\",\\\n        },\\\n    ],\n    response_model=User,\n    context={\"data\": \"John Doe is thirty years old\"},\n)\n\nprint(resp)\n#> name='John Doe' age=30\n\n```\n\n### Context is available to Pydantic validators [¶](https://python.useinstructor.com/concepts/templating/\\#context-is-available-to-pydantic-validators \"Permanent link\")\n\nIn this example, we demonstrate how to leverage the `context` parameter with Pydantic validators to enhance our validation and data processing capabilities. By passing the `context` to the validators, we can implement dynamic validation rules and data transformations based on the input context. This approach allows for flexible and context-aware validation, such as checking for banned words or applying redaction patterns to sensitive information.\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel, ValidationInfo, field_validator\nimport re\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass Response(BaseModel):\n    text: str\n\n    @field_validator('text')\n    @classmethod\n    def redact_regex(cls, v: str, info: ValidationInfo):\n        context = info.context\n        if context:\n            redact_patterns = context.get('redact_patterns', [])\n            for pattern in redact_patterns:\n                v = re.sub(pattern, '****', v)\n        return v\n\nresponse = client.create(\n    model=\"gpt-4o\",\n    response_model=Response,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"\"\"\\\n                Write about a {{ topic }}\\\n\\\n                {% if banned_words %}\\\n                You must not use the following banned words:\\\n\\\n                <banned_words>\\\n                {% for word in banned_words %}\\\n                * {{ word }}\\\n                {% endfor %}\\\n                </banned_words>\\\n                {% endif %}\\\n              \"\"\",\\\n        },\\\n    ],\n    context={\n        \"topic\": \"jason and now his phone number is 123-456-7890\",\n        \"redact_patterns\": [\\\n            r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",  # Phone number pattern\\\n            r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",  # SSN pattern\\\n        ],\n    },\n    max_retries=3,\n)\n\nprint(response.text)\n\"\"\"\nJason is a remarkable individual known for his generosity and lively spirit. In his community, he is always ready to lend a helping hand, whether it's participating in local events, volunteering for charitable causes, or simply being there for his friends and family. His warmth and friendliness make everyone around him feel welcome and appreciated.\n\nJason is an enthusiast of technology and innovation. He spends much of his free time exploring new gadgets and staying updated with the latest tech trends. His curiosity often leads him to experiment with different software and hardware, making him a go-to person for tech advice among his peers.\n\nIn his career, Jason is a dedicated professional, always striving to improve and excel in his field. His colleagues respect him for his work ethic and creativity, making him an invaluable team member.\n\nIn his personal life, Jason enjoys outdoor activities such as hiking and cycling. These adventures provide him with a sense of freedom and connection to nature, reflecting his adventurous personality.\n\nAs much as Jason values his privacy, he is also approachable and open-minded. This balance allows him to maintain meaningful connections without compromising his personal space.\n\nPlease note, sharing personal contact information like phone numbers on public platforms is discouraged to protect privacy. If you need to contact someone like Jason, it's best to do so through secured and private channels or have explicit consent from the individual involved.\n\"\"\"\n\n```\n\n1. Access the variables passed into the `context` variable inside your Pydantic validator\n\n2. Pass in the variables to be used for validation and/or rendering into the `context` parameter\n\n\n### Jinja Syntax [¶](https://python.useinstructor.com/concepts/templating/\\#jinja-syntax \"Permanent link\")\n\nJinja is used to render the prompts, allowing the use of familiar Jinja syntax. This enables rendering of lists, conditionals, and more. It also allows calling functions and methods within Jinja.\n\nThis makes formatting of prompts and rendering logic extremely easy.\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass Citation(BaseModel):\n    source_ids: list[int]\n    text: str\n\nclass Response(BaseModel):\n    answer: list[Citation]\n\nresp = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"\"\"\\\n                You are a {{ role }} tasks with the following question\\\n\\\n                <question>\\\n                {{ question }}\\\n                </question>\\\n\\\n                Use the following context to answer the question, make sure to return [id] for every citation:\\\n\\\n                <context>\\\n                {% for chunk in context %}\\\n                  <context_chunk>\\\n                    <id>{{ chunk.id }}</id>\\\n                    <text>{{ chunk.text }}</text>\\\n                  </context_chunk>\\\n                {% endfor %}\\\n                </context>\\\n\\\n                {% if rules %}\\\n                Make sure to follow these rules:\\\n\\\n                {% for rule in rules %}\\\n                  * {{ rule }}\\\n                {% endfor %}\\\n                {% endif %}\\\n            \"\"\",\\\n        },\\\n    ],\n    response_model=Response,\n    context={\n        \"role\": \"professional educator\",\n        \"question\": \"What is the capital of France?\",\n        \"context\": [\\\n            {\"id\": 1, \"text\": \"Paris is the capital of France.\"},\\\n            {\"id\": 2, \"text\": \"France is a country in Europe.\"},\\\n        ],\n        \"rules\": [\"Use markdown.\"],\n    },\n)\n\nprint(resp)\n#> answer=[Citation(source_ids=[1], text='The capital of France is Paris.')]\n# answer=[Citation(source_ids=[1], text='The capital of France is Paris.')]\n\n```\n\n### Working with Secrets [¶](https://python.useinstructor.com/concepts/templating/\\#working-with-secrets \"Permanent link\")\n\nYour prompts might need to include sensitive user information when they're sent to your model provider. This is probably something you don't want to hard code into your prompt or captured in your logs. An easy way to get around this is to use the `SecretStr` type from `Pydantic` in your model definitions.\n\n```md-code__content\nfrom pydantic import BaseModel, SecretStr\nimport instructor\nimport openai\n\nclass UserContext(BaseModel):\n    name: str\n    address: SecretStr\n\nclass Address(BaseModel):\n    street: SecretStr\n    city: str\n    state: str\n    zipcode: str\n\nclient = instructor.from_openai(openai.OpenAI())\ncontext = UserContext(name=\"scolvin\", address=\"secret address\")\n\naddress = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"{{ user.name }} is `{{ user.address.get_secret_value() }}`, normalize it to an address object\",\\\n        },\\\n    ],\n    context={\"user\": context},\n    response_model=Address,\n)\nprint(context)\n#> name='scolvin' address=SecretStr('**********')\nprint(address)\n#> street=SecretStr('**********') city='scolvin' state='' zipcode=''\n\n```\n\nThis allows you to preserve your sensitive information while still using it in your prompts.\n\n## Security [¶](https://python.useinstructor.com/concepts/templating/\\#security \"Permanent link\")\n\nWe use the `jinja2.sandbox.SandboxedEnvironment` to prevent security issues with the templating engine. This means that you can't use arbitrary python code in your prompts. But this doesn't mean that you should pass untrusted input to the templating engine, as this could still be abused for things like Denial of Service attacks.\n\nYou should [always sanitize](https://jinja.palletsprojects.com/en/stable/sandbox/#security-considerations) any input that you pass to the templating engine.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/templating/",
      "ogUrl": "https://python.useinstructor.com/concepts/templating/",
      "title": "Effective Prompt Templating with Jinja and Pydantic - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/templating/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/templating.png",
      "ogTitle": "Effective Prompt Templating with Jinja and Pydantic - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/templating.png",
      "og:title": "Effective Prompt Templating with Jinja and Pydantic - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/templating/",
      "statusCode": 200,
      "description": "Learn to dynamically create prompts using Jinja templating and validate them with Pydantic for enhanced flexibility and security.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn to dynamically create prompts using Jinja templating and validate them with Pydantic for enhanced flexibility and security.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/templating.png",
      "twitter:title": "Effective Prompt Templating with Jinja and Pydantic - Instructor",
      "og:description": "Learn to dynamically create prompts using Jinja templating and validate them with Pydantic for enhanced flexibility and security.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn to dynamically create prompts using Jinja templating and validate them with Pydantic for enhanced flexibility and security."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/patching/#patching)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/patching.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/patching.md \"View source of this page\")\n\n# Patching [¶](https://python.useinstructor.com/concepts/patching/\\#patching \"Permanent link\")\n\nInstructor enhances client functionality with three new keywords for backwards compatibility. This allows use of the enhanced client as usual, with structured output benefits.\n\n- `response_model`: Defines the response type for `chat.completions.create`.\n- `max_retries`: Determines retry attempts for failed `chat.completions.create` validations.\n- `validation_context`: Provides extra context to the validation process.\n\nThe default mode is `instructor.Mode.TOOLS` which is the recommended mode for OpenAI clients. This mode is the most stable and is the most recommended for OpenAI clients. The other modes are for other clients and are not recommended for OpenAI clients.\n\n## Tool Calling [¶](https://python.useinstructor.com/concepts/patching/\\#tool-calling \"Permanent link\")\n\nThis is the recommended method for OpenAI clients. It is the most stable as functions is being deprecated soon.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.TOOLS)\n\n```\n\n### Gemini Tool Calling [¶](https://python.useinstructor.com/concepts/patching/\\#gemini-tool-calling \"Permanent link\")\n\nGemini supports tool calling for stuctured data extraction. Gemini tool calling requires `jsonref` to be installed.\n\nLimitations\n\nGemini tool calling comes with some known limitations:\n\n```\n- `strict` Pydantic validation can fail for integer/float and enum validations\n- Gemini tool calling is incompatible with Pydantic schema customizations such as examples due to API limitations and may result in errors\n- Gemini can sometimes call the wrong function name, resulting in malformed or invalid json\n- Gemini tool calling could fail with enum and literal field types\n\n```\n\n```md-code__content\nimport instructor\nimport google.generativeai as genai\n\nclient = instructor.from_gemini(\n    genai.GenerativeModel(), mode=instructor.Mode.GEMINI_TOOLS\n)\n\n```\n\n### Gemini Vertex AI Tool Callin [¶](https://python.useinstructor.com/concepts/patching/\\#gemini-vertex-ai-tool-callin \"Permanent link\")\n\nThis method allows us to get structured output from Gemini via tool calling with the Vertex AI SDK.\n\n**Note:** Gemini Tool Calling is in preview and there are some limitations, you can learn more in the [Vertex AI examples notebook](https://python.useinstructor.com/integrations/vertex/).\n\n```md-code__content\nimport instructor\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nimport vertexai\n\nvertexai.init(project=\"vertexai-generative-models\")\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-1.5-pro-preview-0409\"),\n    mode=instructor.Mode.VERTEXAI_TOOLS,\n)\n\n```\n\n## Parallel Tool Calling [¶](https://python.useinstructor.com/concepts/patching/\\#parallel-tool-calling \"Permanent link\")\n\nParallel tool calling is also an option but you must set `response_model` to be `Iterable[Union[...]]` types since we expect an array of results. Check out [Parallel Tool Calling](https://python.useinstructor.com/concepts/parallel/) for more information.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.PARALLEL_TOOLS)\n\n```\n\n## Function Calling [¶](https://python.useinstructor.com/concepts/patching/\\#function-calling \"Permanent link\")\n\nNote that function calling is soon to be deprecated in favor of TOOL mode for OpenAI. But will still be supported for other clients.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.TOOLS)\n\n```\n\n## JSON Mode [¶](https://python.useinstructor.com/concepts/patching/\\#json-mode \"Permanent link\")\n\nJSON mode uses OpenAI's JSON format for responses by setting `response_format={\"type\": \"json_object\"}` in the `chat.completions.create` method.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.JSON)\n\n```\n\nJSON mode is also required for [the Gemini Models via OpenAI's SDK](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/call-gemini-using-openai-library#client-setup).\n\n```md-code__content\npip install google-auth\n\n```\n\n```md-code__content\nimport google.auth\nimport google.auth.transport.requests\nimport instructor\nfrom openai import OpenAI\n\ncreds, project = google.auth.default()\nauth_req = google.auth.transport.requests.Request()\ncreds.refresh(auth_req)\n\n# Pass the Vertex endpoint and authentication to the OpenAI SDK\nPROJECT = 'PROJECT_ID'\nLOCATION = 'LOCATION'\n\nbase_url = f'https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT}/locations/{LOCATION}/endpoints/openapi'\nclient = instructor.from_openai(\n    OpenAI(base_url=base_url, api_key=creds.token), mode=instructor.Mode.JSON\n)\n\n```\n\n### Gemini JSON Mode [¶](https://python.useinstructor.com/concepts/patching/\\#gemini-json-mode \"Permanent link\")\n\nThis mode uses Gemini's response mimetype field to generate a response in JSON format using the schema provided.\n\n```md-code__content\nimport instructor\nimport google.generativeai as genai\n\nclient = instructor.from_gemini(\n    genai.GenerativeModel(), mode=instructor.Mode.GEMINI_JSON\n)\n\n```\n\n## Markdown JSON Mode [¶](https://python.useinstructor.com/concepts/patching/\\#markdown-json-mode \"Permanent link\")\n\nThis just asks for the response in JSON format, but it is not recommended, and may not be supported in the future, this is just left to support vision models and models provided by Databricks and will not give you the full benefits of instructor.\n\nExperimental\n\nThis is not recommended, and may not be supported in the future, this is just left to support vision models and models provided by Databricks.\n\nGeneral syntax:\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.MD_JSON)\n\n```\n\nDatabricks syntax:\n\n```md-code__content\nimport instructor\nimport os\nfrom openai import OpenAI\n\nDATABRICKS_TOKEN = os.environ.get(\"DATABRICKS_TOKEN\", \"\")\nDATABRICKS_HOST = os.environ.get(\"DATABRICKS_HOST\", \"\")\n\n# Assuming Databricks environment variables are set\nclient = instructor.from_openai(\n    OpenAI(\n        api_key=DATABRICKS_TOKEN,\n        base_url=f\"{DATABRICKS_HOST}/serving-endpoints\",\n    ),\n    mode=instructor.Mode.MD_JSON,\n)\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/patching/",
      "ogUrl": "https://python.useinstructor.com/concepts/patching/",
      "title": "Enhancing OpenAI Client Functionality with New Keywords - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/patching/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/patching.png",
      "ogTitle": "Enhancing OpenAI Client Functionality with New Keywords - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/patching.png",
      "og:title": "Enhancing OpenAI Client Functionality with New Keywords - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/patching/",
      "statusCode": 200,
      "description": "Learn how to use new keywords for backward compatibility in the OpenAI client, improving functionality and structured output.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to use new keywords for backward compatibility in the OpenAI client, improving functionality and structured output.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/patching.png",
      "twitter:title": "Enhancing OpenAI Client Functionality with New Keywords - Instructor",
      "og:description": "Learn how to use new keywords for backward compatibility in the OpenAI client, improving functionality and structured output.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to use new keywords for backward compatibility in the OpenAI client, improving functionality and structured output."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/hooks/#hooks)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/hooks.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/hooks.md \"View source of this page\")\n\n# Hooks [¶](https://python.useinstructor.com/concepts/hooks/\\#hooks \"Permanent link\")\n\nHooks provide a powerful mechanism for intercepting and handling events during the completion and parsing process in the Instructor library. They allow you to add custom behavior, logging, or error handling at various stages of the API interaction.\n\n## Overview [¶](https://python.useinstructor.com/concepts/hooks/\\#overview \"Permanent link\")\n\nThe Hooks system in Instructor is based on the `Hooks` class, which manages event registration and emission. It supports several predefined events that correspond to different stages of the completion and parsing process.\n\n## Supported Hook Events [¶](https://python.useinstructor.com/concepts/hooks/\\#supported-hook-events \"Permanent link\")\n\n### `completion:kwargs` [¶](https://python.useinstructor.com/concepts/hooks/\\#completionkwargs \"Permanent link\")\n\nThis hook is emitted when completion arguments are provided. It receives all arguments passed to the completion function. These will contain the `model`, `messages`, `tools`, AFTER any `response_model` or `validation_context` parameters have been converted to their respective values.\n\n```md-code__content\ndef handler(*args, **kwargs) -> None: ...\n\n```\n\n### `completion:response` [¶](https://python.useinstructor.com/concepts/hooks/\\#completionresponse \"Permanent link\")\n\nThis hook is emitted when a completion response is received. It receives the raw response object from the completion API.\n\n```md-code__content\ndef handler(response) -> None: ...\n\n```\n\n### `completion:error` [¶](https://python.useinstructor.com/concepts/hooks/\\#completionerror \"Permanent link\")\n\nThis hook is emitted when an error occurs during completion before any retries are attempted and the response is parsed as a pydantic model.\n\n```md-code__content\ndef handler(error) -> None: ...\n\n```\n\n### `parse:error` [¶](https://python.useinstructor.com/concepts/hooks/\\#parseerror \"Permanent link\")\n\nThis hook is emitted when an error occurs during parsing of the response as a pydantic model. This can happen if the response is not valid or if the pydantic model is not compatible with the response.\n\n```md-code__content\ndef handler(error) -> None: ...\n\n```\n\n### `completion:last_attempt` [¶](https://python.useinstructor.com/concepts/hooks/\\#completionlast_attempt \"Permanent link\")\n\nThis hook is emitted when the last retry attempt is made.\n\n```md-code__content\ndef handler(error) -> None: ...\n\n```\n\n## Implementation Details [¶](https://python.useinstructor.com/concepts/hooks/\\#implementation-details \"Permanent link\")\n\nThe Hooks system is implemented in the `instructor/hooks.py` file. The `Hooks` class handles the registration and emission of hook events. You can refer to this file to see how hooks work under the hood. The retry logic that uses Hooks is implemented in the `instructor/retry.py` file. This shows how Hooks are used when trying again after errors during completions.\n\n### Registering Hooks [¶](https://python.useinstructor.com/concepts/hooks/\\#registering-hooks \"Permanent link\")\n\nYou can register hooks using the `on` method of the Instructor client or a `Hooks` instance. Here's an example:\n\n```md-code__content\nimport instructor\nimport openai\nimport pprint\n\nclient = instructor.from_openai(openai.OpenAI())\n\ndef log_completion_kwargs(*args, **kwargs):\n    pprint.pprint({\"args\": args, \"kwargs\": kwargs})\n\nclient.on(\"completion:kwargs\", log_completion_kwargs)\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n    response_model=str,\n)\nprint(resp)\n#> Hello, world!\n\n```\n\n### Emitting Events [¶](https://python.useinstructor.com/concepts/hooks/\\#emitting-events \"Permanent link\")\n\nEvents are automatically emitted by the Instructor library at appropriate times. You don't need to manually emit events in most cases.\n\n### Removing Hooks [¶](https://python.useinstructor.com/concepts/hooks/\\#removing-hooks \"Permanent link\")\n\nYou can remove a specific hook using the `off` method:\n\n```md-code__content\nclient.off(\"completion:kwargs\", log_completion_kwargs)\n\n```\n\n### Clearing Hooks [¶](https://python.useinstructor.com/concepts/hooks/\\#clearing-hooks \"Permanent link\")\n\nTo remove all hooks for a specific event or all events:\n\n```md-code__content\n# Clear hooks for a specific event\nclient.clear(\"completion:kwargs\")\n\n# Clear all hooks\nclient.clear()\n\n```\n\n## Example: Logging and Debugging [¶](https://python.useinstructor.com/concepts/hooks/\\#example-logging-and-debugging \"Permanent link\")\n\nHere's a comprehensive example demonstrating how to use hooks for logging and debugging:\n\n```md-code__content\nimport instructor\nimport openai\nimport pydantic\n\ndef log_completion_kwargs(kwargs) -> None:\n    print(\"## Completion kwargs:\")\n    print(kwargs)\n    \"\"\"\n    {\n        \"messages\": [\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": \"Extract the user name and age from the following text: 'John is 20 years old'\",\\\n            }\\\n        ],\n        \"model\": \"gpt-4o-mini\",\n        \"tools\": [\\\n            {\\\n                \"type\": \"function\",\\\n                \"function\": {\\\n                    \"name\": \"User\",\\\n                    \"description\": \"Correctly extracted `User` with all the required parameters with correct types\",\\\n                    \"parameters\": {\\\n                        \"properties\": {\\\n                            \"name\": {\"title\": \"Name\", \"type\": \"string\"},\\\n                            \"age\": {\"title\": \"Age\", \"type\": \"integer\"},\\\n                        },\\\n                        \"required\": [\"age\", \"name\"],\\\n                        \"type\": \"object\",\\\n                    },\\\n                },\\\n            }\\\n        ],\n        \"tool_choice\": {\"type\": \"function\", \"function\": {\"name\": \"User\"}},\n    }\n    \"\"\"\n\ndef log_completion_response(response) -> None:\n    print(\"## Completion response:\")\n    #> ## Completion response:\n    \"\"\"\n    {\n        'id': 'chatcmpl-AWl4Mj5Jrv7m7JkOTIiHXSldQIOFm',\n        'choices': [\\\n            {\\\n                'finish_reason': 'stop',\\\n                'index': 0,\\\n                'logprobs': None,\\\n                'message': {\\\n                    'content': None,\\\n                    'refusal': None,\\\n                    'role': 'assistant',\\\n                    'audio': None,\\\n                    'function_call': None,\\\n                    'tool_calls': [\\\n                        {\\\n                            'id': 'call_6oQ9WXxeSiVEV71B9IYtsbIE',\\\n                            'function': {\\\n                                'arguments': '{\"name\":\"John\",\"age\":-1}',\\\n                                'name': 'User',\\\n                            },\\\n                            'type': 'function',\\\n                        }\\\n                    ],\\\n                },\\\n            }\\\n        ],\n        'created': 1732370794,\n        'model': 'gpt-4o-mini-2024-07-18',\n        'object': 'chat.completion',\n        'service_tier': None,\n        'system_fingerprint': 'fp_0705bf87c0',\n        'usage': {\n            'completion_tokens': 10,\n            'prompt_tokens': 87,\n            'total_tokens': 97,\n            'completion_tokens_details': {\n                'audio_tokens': 0,\n                'reasoning_tokens': 0,\n                'accepted_prediction_tokens': 0,\n                'rejected_prediction_tokens': 0,\n            },\n            'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n        },\n    }\n    \"\"\"\n    print(response.model_dump())\n    \"\"\"\n    {\n        'id': 'chatcmpl-AWl4Mxdq0BUGRlVCA61z8YOIVga7F',\n        'choices': [\\\n            {\\\n                'finish_reason': 'stop',\\\n                'index': 0,\\\n                'logprobs': None,\\\n                'message': {\\\n                    'content': None,\\\n                    'refusal': None,\\\n                    'role': 'assistant',\\\n                    'audio': None,\\\n                    'function_call': None,\\\n                    'tool_calls': [\\\n                        {\\\n                            'id': 'call_EJIEr27Mb6sdbplnYw4iBWlm',\\\n                            'function': {\\\n                                'arguments': '{\"name\":\"John\",\"age\":10}',\\\n                                'name': 'User',\\\n                            },\\\n                            'type': 'function',\\\n                        }\\\n                    ],\\\n                },\\\n            }\\\n        ],\n        'created': 1732370794,\n        'model': 'gpt-4o-mini-2024-07-18',\n        'object': 'chat.completion',\n        'service_tier': None,\n        'system_fingerprint': 'fp_0705bf87c0',\n        'usage': {\n            'completion_tokens': 9,\n            'prompt_tokens': 87,\n            'total_tokens': 96,\n            'completion_tokens_details': {\n                'audio_tokens': 0,\n                'reasoning_tokens': 0,\n                'accepted_prediction_tokens': 0,\n                'rejected_prediction_tokens': 0,\n            },\n            'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n        },\n    }\n    \"\"\"\n\ndef log_completion_error(error) -> None:\n    print(\"## Completion error:\")\n    print({\"error\": error})\n\ndef log_parse_error(error) -> None:\n    print(\"## Parse error:\")\n    #> ## Parse error:\n    print(error)\n    \"\"\"\n    1 validation error for User\n    age\n    Value error, Age cannot be negative [type=value_error, input_value=-10, input_type=int]\n        For further information visit https://errors.pydantic.dev/2.8/v/value_error\n    \"\"\"\n\n# Create an Instructor client\nclient = instructor.from_openai(openai.OpenAI())\n\nclient.on(\"completion:kwargs\", log_completion_kwargs)\nclient.on(\"completion:response\", log_completion_response)\n\nclient.on(\"completion:error\", log_completion_error)\nclient.on(\"parse:error\", log_parse_error)\n\n# Define a model with a validator\nclass User(pydantic.BaseModel):\n    name: str\n    age: int\n\n    @pydantic.field_validator(\"age\")\n    def check_age(cls, v: int) -> int:\n        if v < 0:\n            raise ValueError(\"Age cannot be negative\")\n        return v\n\ntry:\n    # Use the client to create a completion\n    user = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\\\n            {\\\n    #> ## Parse error:\\\n                \"role\": \"user\",\\\n                \"content\": \"Extract the user name and age from the following text: 'John is -1 years old'\",\\\n    \"\"\"\\\n    1 validation error for User\\\n    age\\\n      Value error, Age cannot be negative [type=value_error, input_value=-1, input_type=int]\\\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\\\n    \"\"\"\\\n            }\\\n        ],\n        response_model=User,\n        max_retries=1,\n    )\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\nuser = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Extract the user name and age from the following text: 'John is 10 years old'\",\\\n        }\\\n    ],\n    response_model=User,\n    max_retries=1,\n)\nprint(user)\n#> name='John' age=10\n    \"\"\"\n    Error: 1 validation error for User\n    age\n      Value error, Age cannot be negative [type=value_error, input_value=-1, input_type=int]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n\n```\n\nThis example demonstrates:\n\n1. Defining hook handlers for different events.\n2. Registering the hooks with the Instructor client.\n3. Using a Pydantic model with a validator.\n4. Making a completion request that will trigger various hooks.\n\nThe hooks will log information at different stages of the process, helping with debugging and understanding the flow of data.\n\n## Best Practices [¶](https://python.useinstructor.com/concepts/hooks/\\#best-practices \"Permanent link\")\n\n1. **Error Handling**: Always include error handling in your hook handlers to prevent exceptions from breaking the main execution flow. We will automatically warn if an exception is raised in a hook handler.\n\n2. **Performance**: Keep hook handlers lightweight to avoid impacting the performance of the main application.\n\n3. **Modularity**: Use hooks to separate concerns. For example, use hooks for logging, monitoring, or custom business logic without cluttering the main code.\n\n4. **Consistency**: Use the same naming conventions and patterns across all your hooks for better maintainability.\n\n5. **Documentation**: Document the purpose and expected input/output of each hook handler for easier collaboration and maintenance.\n\n\nBy leveraging hooks effectively, you can create more flexible, debuggable, and maintainable applications when working with the Instructor library and language models.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/hooks/",
      "ogUrl": "https://python.useinstructor.com/concepts/hooks/",
      "title": "Understanding Hooks in the Instructor Library - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/hooks/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/hooks.png",
      "ogTitle": "Understanding Hooks in the Instructor Library - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/hooks.png",
      "og:title": "Understanding Hooks in the Instructor Library - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/hooks/",
      "statusCode": 200,
      "description": "Learn how to use hooks for event handling in the Instructor library to enhance logging, error handling, and custom behaviors.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to use hooks for event handling in the Instructor library to enhance logging, error handling, and custom behaviors.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/hooks.png",
      "twitter:title": "Understanding Hooks in the Instructor Library - Instructor",
      "og:description": "Learn how to use hooks for event handling in the Instructor library to enhance logging, error handling, and custom behaviors.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to use hooks for event handling in the Instructor library to enhance logging, error handling, and custom behaviors."
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/enums.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/enums.md \"View source of this page\")\n\n# Enums\n\nTo prevent data misalignment, we can use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\nfrom enum import Enum\n\nclass Role(Enum):\n    PRINCIPAL = \"PRINCIPAL\"\n    TEACHER = \"TEACHER\"\n    STUDENT = \"STUDENT\"\n    OTHER = \"OTHER\"\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(\n        description=\"Correctly assign one of the predefined roles to the user.\"\n    )\n\n```\n\nIf you're having a hard time with `Enum` an alternative is to use `Literal` instead.\n\n```md-code__content\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/enums/",
      "ogUrl": "https://python.useinstructor.com/concepts/enums/",
      "title": "Using Enums and Literals in Pydantic for Role Management - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/enums/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/enums.png",
      "ogTitle": "Using Enums and Literals in Pydantic for Role Management - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/enums.png",
      "og:title": "Using Enums and Literals in Pydantic for Role Management - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/enums/",
      "statusCode": 200,
      "description": "Learn how to implement Enums and Literals in Pydantic to manage standardized user roles with a fallback option.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to implement Enums and Literals in Pydantic to manage standardized user roles with a fallback option.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/enums.png",
      "twitter:title": "Using Enums and Literals in Pydantic for Role Management - Instructor",
      "og:description": "Learn how to implement Enums and Literals in Pydantic to manage standardized user roles with a fallback option.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to implement Enums and Literals in Pydantic to manage standardized user roles with a fallback option."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/distillation/#distilling-python-functions-into-llm)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/distillation.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/distillation.md \"View source of this page\")\n\n# Distilling python functions into LLM [¶](https://python.useinstructor.com/concepts/distillation/\\#distilling-python-functions-into-llm \"Permanent link\")\n\n`Instructions` from the `Instructor` library offers a seamless way to make language models backward compatible with existing Python functions. By employing Pydantic type hints, it not only ensures compatibility but also facilitates fine-tuning `gpt-3.5-turbo` to emulate these functions end-to-end.\n\nIf you want to see the full example checkout [examples/distillation](https://github.com/jxnl/instructor/tree/main/examples/distilations)\n\n## The Challenges in Function-Level Fine-Tuning [¶](https://python.useinstructor.com/concepts/distillation/\\#the-challenges-in-function-level-fine-tuning \"Permanent link\")\n\nReplicating the behavior of a Python function in a language model involves intricate data preparation. For instance, teaching a model to execute three-digit multiplication is not as trivial as implementing `def f(a, b): return a * b`. OpenAI's fine-tuning script coupled with their function calling utility provides a structured output, thereby simplifying the data collection process. Additionally, this eliminates the need for passing the schema to the model, thus conserving tokens.\n\n## The Role of `Instructions` in Simplifying the Fine-Tuning Process [¶](https://python.useinstructor.com/concepts/distillation/\\#the-role-of-instructions-in-simplifying-the-fine-tuning-process \"Permanent link\")\n\nBy using `Instructions`, you can annotate a Python function that returns a Pydantic object, thereby automating the dataset creation for fine-tuning. A handler for logging is all that's needed to build this dataset.\n\n## How to Implement `Instructions` in Your Code [¶](https://python.useinstructor.com/concepts/distillation/\\#how-to-implement-instructions-in-your-code \"Permanent link\")\n\n## Quick Start: How to Use Instructor's Distillation Feature [¶](https://python.useinstructor.com/concepts/distillation/\\#quick-start-how-to-use-instructors-distillation-feature \"Permanent link\")\n\nBefore we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file.\n\n```md-code__content\nimport logging\nimport random\nfrom pydantic import BaseModel\nfrom instructor import Instructions  # pip install instructor\n\n# Logging setup\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs!\n    log_handlers=[logging.FileHandler(\"math_finetunes.jsonl\")],\n)\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n# Define a function with distillation\n# The decorator will automatically generate a dataset for fine-tuning\n# They must return a pydantic model to leverage function calling\n@instructions.distil\ndef fn(a: int, b: int) -> Multiply:\n    resp = a * b\n    return Multiply(a=a, b=b, result=resp)\n\n# Generate some data\nfor _ in range(10):\n    random.seed(42)\n    a = random.randint(100, 999)\n    b = random.randint(100, 999)\n    print(fn(a, b))\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n\n```\n\n## The Intricacies of Fine-tuning Language Models [¶](https://python.useinstructor.com/concepts/distillation/\\#the-intricacies-of-fine-tuning-language-models \"Permanent link\")\n\nFine-tuning isn't just about writing a function like `def f(a, b): return a * b`. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this.\n\n## Why Instructor and Distillation are Game Changers [¶](https://python.useinstructor.com/concepts/distillation/\\#why-instructor-and-distillation-are-game-changers \"Permanent link\")\n\nThe library offers two main benefits:\n\n1. **Efficiency**: Streamlines functions, distilling requirements into model weights and a few lines of code.\n2. **Integration**: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions.\n\n## Role of Instructor in Simplifying Fine-Tuning [¶](https://python.useinstructor.com/concepts/distillation/\\#role-of-instructor-in-simplifying-fine-tuning \"Permanent link\")\n\nThe `from instructor import Instructions` feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior.\n\n## Logging Output and Running a Finetune [¶](https://python.useinstructor.com/concepts/distillation/\\#logging-output-and-running-a-finetune \"Permanent link\")\n\nHere's how the logging output would look:\n\n```md-code__content\n{\n    \"messages\": [\\\n        {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'},\\\n        {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'},\\\n        {\\\n            \"role\": \"assistant\",\\\n            \"function_call\": {\\\n                \"name\": \"Multiply\",\\\n                \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}',\\\n            },\\\n        },\\\n    ],\n    \"functions\": [\\\n        {\"name\": \"Multiply\", \"description\": \"Correctly extracted `Multiply`...\"}\\\n    ],\n}\n\n```\n\nRun a finetune like this:\n\n```md-code__content\ninstructor jobs create-from-file math_finetunes.jsonl\n\n```\n\nOnce a model is trained you can simply change `mode` to `dispatch` and it will use the model to run the function!\n\n```md-code__content\nfrom instructor import Instructions\nfrom pydantic import BaseModel\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n)\n\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")\ndef fn(a: int, b: int) -> Multiply:\n    # now this code will be short circuited and the model will be used instead.\n    resp = a + b\n    return Multiply(a=a, b=b, result=resp)\n\n```\n\nWith this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/distillation/",
      "ogUrl": "https://python.useinstructor.com/concepts/distillation/",
      "title": "Seamless Fine-Tuning of Python Functions Using Instructor's Distillation - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/distillation/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/distillation.png",
      "ogTitle": "Seamless Fine-Tuning of Python Functions Using Instructor's Distillation - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/distillation.png",
      "og:title": "Seamless Fine-Tuning of Python Functions Using Instructor's Distillation - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/distillation/",
      "statusCode": 200,
      "description": "Learn how to fine-tune language models with Python functions using Instructor's `Instructions` for efficient data preparation and logging.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to fine-tune language models with Python functions using Instructor's `Instructions` for efficient data preparation and logging.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/distillation.png",
      "twitter:title": "Seamless Fine-Tuning of Python Functions Using Instructor's Distillation - Instructor",
      "og:description": "Learn how to fine-tune language models with Python functions using Instructor's `Instructions` for efficient data preparation and logging.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to fine-tune language models with Python functions using Instructor's `Instructions` for efficient data preparation and logging."
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/logging.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/logging.md \"View source of this page\")\n\n# Logging\n\nIn order to see the requests made to OpenAI and the responses, you can set logging to DEBUG. This will show the requests and responses made to OpenAI. This can be useful for debugging and understanding the requests and responses made to OpenAI. I would love some contributions that make this a lot cleaner, but for now this is the fastest way to see the prompts.\n\n```md-code__content\nimport instructor\nimport openai\nimport logging\n\nfrom pydantic import BaseModel\n\n# Set logging to DEBUG\nlogging.basicConfig(level=logging.DEBUG)\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\\\n    ],\n)  # type: ignore\n\n\"\"\"\n...\nDEBUG:instructor:Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\nDEBUG:instructor:Instructor Request: mode.value='tool_call', response_model=<class '__main__.UserDetail'>, new_kwargs={'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Extract Jason is 25 years old'}], 'tools': [{'type': 'function', 'function': {'name': 'UserDetail', 'description': 'Correctly extracted `UserDetail` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'UserDetail'}}}\nDEBUG:instructor:max_retries: 1\n...\nDEBUG:instructor:Instructor Pre-Response: ChatCompletion(id='chatcmpl-8zBxMxsOqm5Sj6yeEI38PnU2r6ncC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_E1cftF5U0zEjzIbWt3q0ZLbN', function=Function(arguments='{\"name\":\"Jason\",\"age\":25}', name='UserDetail'), type='function')]))], created=1709594660, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_2b778c6b35', usage=CompletionUsage(completion_tokens=9, prompt_tokens=81, total_tokens=90))\nDEBUG:httpcore.connection:close.started\nDEBUG:httpcore.connection:close.complete\n\"\"\"\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/logging/",
      "ogUrl": "https://python.useinstructor.com/concepts/logging/",
      "title": "Debugging OpenAI Requests with Python Logging - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/logging/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/logging.png",
      "ogTitle": "Debugging OpenAI Requests with Python Logging - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/logging.png",
      "og:title": "Debugging OpenAI Requests with Python Logging - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/logging/",
      "statusCode": 200,
      "description": "Learn how to log OpenAI requests and responses in Python using DEBUG level logging for efficient debugging.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to log OpenAI requests and responses in Python using DEBUG level logging for efficient debugging.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/logging.png",
      "twitter:title": "Debugging OpenAI Requests with Python Logging - Instructor",
      "og:description": "Learn how to log OpenAI requests and responses in Python using DEBUG level logging for efficient debugging.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to log OpenAI requests and responses in Python using DEBUG level logging for efficient debugging."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/prompt_caching/#prompt-caching)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/prompt_caching.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/prompt_caching.md \"View source of this page\")\n\n# Prompt Caching [¶](https://python.useinstructor.com/concepts/prompt_caching/\\#prompt-caching \"Permanent link\")\n\nPrompt Caching is a feature that allows you to cache portions of your prompt, optimizing performance for multiple API calls with shared context. This helps to reduce cost and improve response times.\n\n## Prompt Caching in OpenAI [¶](https://python.useinstructor.com/concepts/prompt_caching/\\#prompt-caching-in-openai \"Permanent link\")\n\nOpenAI implements a prompt caching mechanism to optimize performance for API requests with similar prompts.\n\n> Prompt Caching works automatically on all your API requests (no code changes required) and has no additional fees associated with it.\n\nThis optimization is especially useful for applications making multiple API calls with shared context, minimizing redundant processing and improving overall performance.\n\nPrompt Caching is enabled for the following models:\n\n- gpt-4o\n- gpt-4o-mini\n- o1-preview\n- o1-mini\n\nCaching is based on prefix matching, so if you're using a system prompt that contains a common set of instructions, you're likely to see a cache hit as long as you move all variable parts of the prompt to the end of the message when possible.\n\n## Prompt Caching in Anthropic [¶](https://python.useinstructor.com/concepts/prompt_caching/\\#prompt-caching-in-anthropic \"Permanent link\")\n\nThe `anthropic.beta.prompt_caching.messages.create` method enables you to:\n\n1. Cache specific prompt portions\n2. Reuse cached content in subsequent calls\n3. Reduce processed data per request\n\nBy implementing prompt caching, you can potentially enhance efficiency and reduce costs, especially when dealing with large, shared contexts across multiple API interactions.\n\nSource Text\n\nIn the following example, we'll be using a short excerpt from the novel \"Pride and Prejudice\" by Jane Austen. This text serves as an example of a substantial context that might typically lead to slow response times and high costs when working with language models. You can download it manually [here](https://www.gutenberg.org/cache/epub/1342/pg1342.txt)\n\n```md-code__content\n    _Walt Whitman has somewhere a fine and just distinction between “loving\nby allowance” and “loving with personal love.” This distinction applies\nto books as well as to men and women; and in the case of the not very\nnumerous authors who are the objects of the personal affection, it\nbrings a curious consequence with it. There is much more difference as\nto their best work than in the case of those others who are loved “by\nallowance” by convention, and because it is felt to be the right and\nproper thing to love them. And in the sect--fairly large and yet\nunusually choice--of Austenians or Janites, there would probably be\nfound partisans of the claim to primacy of almost every one of the\nnovels. To some the delightful freshness and humour of_ Northanger\nAbbey, _its completeness, finish, and_ entrain, _obscure the undoubted\ncritical facts that its scale is small, and its scheme, after all, that\nof burlesque or parody, a kind in which the first rank is reached with\ndifficulty._ Persuasion, _relatively faint in tone, and not enthralling\nin interest, has devotees who exalt above all the others its exquisite\ndelicacy and keeping. The catastrophe of_ Mansfield Park _is admittedly\ntheatrical, the hero and heroine are insipid, and the author has almost\nwickedly destroyed all romantic interest by expressly admitting that\nEdmund only took Fanny because Mary shocked him, and that Fanny might\nvery likely have taken Crawford if he had been a little more assiduous;\nyet the matchless rehearsal-scenes and the characters of Mrs. Norris and\nothers have secured, I believe, a considerable party for it._ Sense and\nSensibility _has perhaps the fewest out-and-out admirers; but it does\nnot want them._\n_I suppose, however, that the majority of at least competent votes\nwould, all things considered, be divided between_ Emma _and the present\nbook; and perhaps the vulgar verdict (if indeed a fondness for Miss\nAusten be not of itself a patent of exemption from any possible charge\nof vulgarity) would go for_ Emma. _It is the larger, the more varied, the\nmore popular; the author had by the time of its composition seen rather\nmore of the world, and had improved her general, though not her most\npeculiar and characteristic dialogue; such figures as Miss Bates, as the\nEltons, cannot but unite the suffrages of everybody. On the other hand,\nI, for my part, declare for_ Pride and Prejudice _unhesitatingly. It\nseems to me the most perfect, the most characteristic, the most\neminently quintessential of its author’s works; and for this contention\nin such narrow space as is permitted to me, I propose here to show\ncause._\n_In the first place, the book (it may be barely necessary to remind the\nreader) was in its first shape written very early, somewhere about 1796,\nwhen Miss Austen was barely twenty-one; though it was revised and\nfinished at Chawton some fifteen years later, and was not published till\n1813, only four years before her death. I do not know whether, in this\ncombination of the fresh and vigorous projection of youth, and the\ncritical revision of middle life, there may be traced the distinct\nsuperiority in point of construction, which, as it seems to me, it\npossesses over all the others. The plot, though not elaborate, is almost\nregular enough for Fielding; hardly a character, hardly an incident\ncould be retrenched without loss to the story. The elopement of Lydia\nand Wickham is not, like that of Crawford and Mrs. Rushworth, a_ coup de\nthéâtre; _it connects itself in the strictest way with the course of the\nstory earlier, and brings about the denouement with complete propriety.\nAll the minor passages--the loves of Jane and Bingley, the advent of Mr.\nCollins, the visit to Hunsford, the Derbyshire tour--fit in after the\nsame unostentatious, but masterly fashion. There is no attempt at the\nhide-and-seek, in-and-out business, which in the transactions between\nFrank Churchill and Jane Fairfax contributes no doubt a good deal to the\nintrigue of_ Emma, _but contributes it in a fashion which I do not think\nthe best feature of that otherwise admirable book. Although Miss Austen\nalways liked something of the misunderstanding kind, which afforded her\nopportunities for the display of the peculiar and incomparable talent to\nbe noticed presently, she has been satisfied here with the perfectly\nnatural occasions provided by the false account of Darcy’s conduct given\nby Wickham, and by the awkwardness (arising with equal naturalness) from\nthe gradual transformation of Elizabeth’s own feelings from positive\naversion to actual love. I do not know whether the all-grasping hand of\nthe playwright has ever been laid upon_ Pride and Prejudice; _and I dare\nsay that, if it were, the situations would prove not startling or\ngarish enough for the footlights, the character-scheme too subtle and\ndelicate for pit and gallery. But if the attempt were made, it would\ncertainly not be hampered by any of those loosenesses of construction,\nwhich, sometimes disguised by the conveniences of which the novelist can\navail himself, appear at once on the stage._\n_I think, however, though the thought will doubtless seem heretical to\nmore than one school of critics, that construction is not the highest\nmerit, the choicest gift, of the novelist. It sets off his other gifts\nand graces most advantageously to the critical eye; and the want of it\nwill sometimes mar those graces--appreciably, though not quite\nconsciously--to eyes by no means ultra-critical. But a very badly-built\nnovel which excelled in pathetic or humorous character, or which\ndisplayed consummate command of dialogue--perhaps the rarest of all\nfaculties--would be an infinitely better thing than a faultless plot\nacted and told by puppets with pebbles in their mouths. And despite the\nability which Miss Austen has shown in working out the story, I for one\nshould put_ Pride and Prejudice _far lower if it did not contain what\nseem to me the very masterpieces of Miss Austen’s humour and of her\nfaculty of character-creation--masterpieces who may indeed admit John\nThorpe, the Eltons, Mrs. Norris, and one or two others to their company,\nbut who, in one instance certainly, and perhaps in others, are still\nsuperior to them._\n_The characteristics of Miss Austen’s humour are so subtle and delicate\nthat they are, perhaps, at all times easier to apprehend than to\nexpress, and at any particular time likely to be differently\napprehended by different persons. To me this humour seems to possess a\ngreater affinity, on the whole, to that of Addison than to any other of\nthe numerous species of this great British genus. The differences of\nscheme, of time, of subject, of literary convention, are, of course,\nobvious enough; the difference of sex does not, perhaps, count for much,\nfor there was a distinctly feminine element in “Mr. Spectator,” and in\nJane Austen’s genius there was, though nothing mannish, much that was\nmasculine. But the likeness of quality consists in a great number of\ncommon subdivisions of quality--demureness, extreme minuteness of touch,\navoidance of loud tones and glaring effects. Also there is in both a\ncertain not inhuman or unamiable cruelty. It is the custom with those\nwho judge grossly to contrast the good nature of Addison with the\nsavagery of Swift, the mildness of Miss Austen with the boisterousness\nof Fielding and Smollett, even with the ferocious practical jokes that\nher immediate predecessor, Miss Burney, allowed without very much\nprotest. Yet, both in Mr. Addison and in Miss Austen there is, though a\nrestrained and well-mannered, an insatiable and ruthless delight in\nroasting and cutting up a fool. A man in the early eighteenth century,\nof course, could push this taste further than a lady in the early\nnineteenth; and no doubt Miss Austen’s principles, as well as her heart,\nwould have shrunk from such things as the letter from the unfortunate\nhusband in the_ Spectator, _who describes, with all the gusto and all the\ninnocence in the world, how his wife and his friend induce him to play\nat blind-man’s-buff. But another_ Spectator _letter--that of the damsel\nof fourteen who wishes to marry Mr. Shapely, and assures her selected\nMentor that “he admires your_ Spectators _mightily”--might have been\nwritten by a rather more ladylike and intelligent Lydia Bennet in the\ndays of Lydia’s great-grandmother; while, on the other hand, some (I\nthink unreasonably) have found “cynicism” in touches of Miss Austen’s\nown, such as her satire of Mrs. Musgrove’s self-deceiving regrets over\nher son. But this word “cynical” is one of the most misused in the\nEnglish language, especially when, by a glaring and gratuitous\nfalsification of its original sense, it is applied, not to rough and\nsnarling invective, but to gentle and oblique satire. If cynicism means\nthe perception of “the other side,” the sense of “the accepted hells\nbeneath,” the consciousness that motives are nearly always mixed, and\nthat to seem is not identical with to be--if this be cynicism, then\nevery man and woman who is not a fool, who does not care to live in a\nfool’s paradise, who has knowledge of nature and the world and life, is\na cynic. And in that sense Miss Austen certainly was one. She may even\nhave been one in the further sense that, like her own Mr. Bennet, she\ntook an epicurean delight in dissecting, in displaying, in setting at\nwork her fools and her mean persons. I think she did take this delight,\nand I do not think at all the worse of her for it as a woman, while she\nwas immensely the better for it as an artist.\n\n```\n\n```md-code__content\nfrom instructor import Instructor, Mode, patch\nfrom anthropic import Anthropic\nfrom pydantic import BaseModel\n\nclient = Instructor(\n    client=Anthropic(),\n    create=patch(\n        create=Anthropic().beta.prompt_caching.messages.create,\n        mode=Mode.ANTHROPIC_TOOLS,\n    ),\n    mode=Mode.ANTHROPIC_TOOLS,\n)\n\nclass Character(BaseModel):\n    name: str\n    description: str\n\nwith open(\"./book.txt\") as f:\n    book = f.read()\n\nresp = client.chat.completions.create(\n    model=\"claude-3-haiku-20240307\",\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": [\\\n                {\\\n                    \"type\": \"text\",\\\n                    \"text\": \"<book>\" + book + \"</book>\",\\\n                    \"cache_control\": {\"type\": \"ephemeral\"},\\\n                },\\\n                {\\\n                    \"type\": \"text\",\\\n                    \"text\": \"Extract a character from the text given above\",\\\n                },\\\n            ],\\\n        },\\\n    ],\n    response_model=Character,\n    max_tokens=1000,\n)\n\n```\n\nCaching Considerations\n\n**Minimum cache size**: For Claude Haiku, your cached content needs to be a minimum of 2048 tokens. For Claude Sonnet, the minimum is 1024 tokens.\n\n**Benefits**: The cost of reading from the cache is 10x lower than if we were to process the same message again and enables us to execute our queries significantly faster.\n\nWe've written a more detailed blog on how to use the `create_with_completion` method [here](https://python.useinstructor.com/blog/2024/09/14/why-should-i-use-prompt-caching/) to validate you're getting a cache hit with instructor.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/prompt_caching/",
      "ogUrl": "https://python.useinstructor.com/concepts/prompt_caching/",
      "title": "Understanding Prompt Caching for API Efficiency - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/prompt_caching/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/prompt_caching.png",
      "ogTitle": "Understanding Prompt Caching for API Efficiency - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/prompt_caching.png",
      "og:title": "Understanding Prompt Caching for API Efficiency - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/prompt_caching/",
      "statusCode": 200,
      "description": "Explore how prompt caching optimizes performance for API calls in OpenAI and Anthropic, enhancing efficiency and reducing costs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Explore how prompt caching optimizes performance for API calls in OpenAI and Anthropic, enhancing efficiency and reducing costs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/prompt_caching.png",
      "twitter:title": "Understanding Prompt Caching for API Efficiency - Instructor",
      "og:description": "Explore how prompt caching optimizes performance for API calls in OpenAI and Anthropic, enhancing efficiency and reducing costs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Explore how prompt caching optimizes performance for API calls in OpenAI and Anthropic, enhancing efficiency and reducing costs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/iterable/#multi-task-and-streaming)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/iterable.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/iterable.md \"View source of this page\")\n\n# Multi-task and Streaming [¶](https://python.useinstructor.com/concepts/iterable/\\#multi-task-and-streaming \"Permanent link\")\n\nA common use case of structured extraction is defining a single schema class and then making another schema to create a list to do multiple extraction\n\n```md-code__content\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nclass Users(BaseModel):\n    users: List[User]\n\nprint(Users.model_json_schema())\n\"\"\"\n{\n    '$defs': {\n        'User': {\n            'properties': {\n                'name': {'title': 'Name', 'type': 'string'},\n                'age': {'title': 'Age', 'type': 'integer'},\n            },\n            'required': ['name', 'age'],\n            'title': 'User',\n            'type': 'object',\n        }\n    },\n    'properties': {\n        'users': {'items': {'$ref': '#/$defs/User'}, 'title': 'Users', 'type': 'array'}\n    },\n    'required': ['users'],\n    'title': 'Users',\n    'type': 'object',\n}\n\"\"\"\n\n```\n\nDefining a task and creating a list of classes is a common enough pattern that we make this convenient by making use of `Iterable[T]`. This lets us dynamically create a new class that:\n\n1. Has dynamic docstrings and class name based on the task\n2. Support streaming by collecting tokens until a task is received back out.\n\n## Extracting Tasks using Iterable [¶](https://python.useinstructor.com/concepts/iterable/\\#extracting-tasks-using-iterable \"Permanent link\")\n\nBy using `Iterable` you get a very convenient class with prompts and names automatically defined:\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nusers = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[User],\n    stream=False,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Consider this data: Jason is 10 and John is 30.\\\\\n                         Correctly segment it into entitites\\\\\n                        Make sure the JSON is correct\",\\\n        },\\\n    ],\n)\nfor user in users:\n    print(user)\n    #> name='Jason' age=10\n    #> name='John' age=30\n\n```\n\n## Streaming Tasks [¶](https://python.useinstructor.com/concepts/iterable/\\#streaming-tasks \"Permanent link\")\n\nWe can also generate tasks as the tokens are streamed in by defining an `Iterable[T]` type.\n\nLets look at an example in action with the same class\n\n```md-code__content\nimport instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nusers = client.chat.completions.create(\n    model=\"gpt-4\",\n    temperature=0.1,\n    stream=True,\n    response_model=Iterable[User],\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"You are a perfect entity extraction system\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": (f\"Extract `Jason is 10 and John is 10`\"),\\\n        },\\\n    ],\n    max_tokens=1000,\n)\n\nfor user in users:\n    print(user)\n    #> name='Jason' age=10\n    #> name='John' age=10\n\n```\n\n## Asynchronous Streaming [¶](https://python.useinstructor.com/concepts/iterable/\\#asynchronous-streaming \"Permanent link\")\n\nI also just want to call out in this example that `instructor` also supports asynchronous streaming. This is useful when you want to stream a response model and process the results as they come in, but you'll need to use the `async for` syntax to iterate over the results.\n\n```md-code__content\nimport instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\nasync def print_iterable_results():\n    model = await client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=Iterable[UserExtract],\n        max_retries=2,\n        stream=True,\n        messages=[\\\n            {\"role\": \"user\", \"content\": \"Make two up people\"},\\\n        ],\n    )\n    async for m in model:\n        print(m)\n        #> name='John Doe' age=27\n        #> name='Jane Smith' age=32\n\nimport asyncio\n\nasyncio.run(print_iterable_results())\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/iterable/",
      "ogUrl": "https://python.useinstructor.com/concepts/iterable/",
      "title": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/iterable/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/iterable.png",
      "ogTitle": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/iterable.png",
      "og:title": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/iterable/",
      "statusCode": 200,
      "description": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/iterable.png",
      "twitter:title": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "og:description": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/philosophy/#philosophy)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/philosophy.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/philosophy.md \"View source of this page\")\n\n* * *\n\ntitle: Simplifying AI with Instructor: Flexibility and Transparency in Python Programming description: Discover how Instructor empowers Python developers with simplicity, flexibility, and transparent LLM integration for better AI engineering.\n\n* * *\n\n# Philosophy [¶](https://python.useinstructor.com/concepts/philosophy/\\#philosophy \"Permanent link\")\n\nThe instructor values [simplicity](https://eugeneyan.com/writing/simplicity/) and flexibility in leveraging language models (LLMs). It offers a streamlined approach for structured output, avoiding unnecessary dependencies or complex abstractions. Let [Pydantic](https://docs.pydantic.dev/latest/) do the heavy lifting.\n\n> “Simplicity is a great virtue but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.” — Edsger Dijkstra\n\n### Proof that its simple [¶](https://python.useinstructor.com/concepts/philosophy/\\#proof-that-its-simple \"Permanent link\")\n\n1. Most users will only need to learn `response_model` and `patch` to get started.\n2. No new prompting language to learn, no new abstractions to learn.\n\n### Proof that its transparent [¶](https://python.useinstructor.com/concepts/philosophy/\\#proof-that-its-transparent \"Permanent link\")\n\n1. We write very little prompts, and we don't try to hide the prompts from you.\n2. We'll do better in the future to give you config over the 2 prompts we do write, Reasking and JSON\\_MODE prompts.\n\n### Proof that its flexible [¶](https://python.useinstructor.com/concepts/philosophy/\\#proof-that-its-flexible \"Permanent link\")\n\n1. If you build a system with OpenAI directly, it is easy to incrementally adopt instructor.\n2. Add `response_model` and if you want to revert, just remove it.\n\n## The zen of `instructor` [¶](https://python.useinstructor.com/concepts/philosophy/\\#the-zen-of-instructor \"Permanent link\")\n\nMaintain the flexibility and power of Python, without unnecessary constraints.\n\nBegin with a function and a return type hint – simplicity is key. With my experience maintaining a large enterprise framework at my previous job over many years I've learned that the goal of making a useful framework is minimizing regret, both for the author and hopefully for the user.\n\n1. Define a Schema `class StructuredData(BaseModel):`\n2. Define validators and methods on your schema.\n3. Encapsulate all your LLM logic into a function `def extract(a) -> StructuredData:`\n4. Define typed computations against your data with `def compute(data: StructuredData):` or call methods on your schema `data.compute()`\n\nIt should be that simple.\n\n## My Goals [¶](https://python.useinstructor.com/concepts/philosophy/\\#my-goals \"Permanent link\")\n\nThe goal for the library, [documentation](https://jxnl.github.io/instructor/), and [blog](https://jxnl.github.io/instructor/blog/), is to help you be a better python programmer and as a result a better AI engineer.\n\n- The library is a result of my desire for simplicity.\n- The library should help maintain simplicity in your codebase.\n- I won't try to write prompts for you,\n- I don't try to create indirections or abstractions that make it hard to debug in the future\n\nPlease note that the library is designed to be adaptable and open-ended, allowing you to customize and extend its functionality based on your specific requirements. If you have any further questions or ideas hit me up on [twitter](https://twitter.com/jxnlco)\n\nCheers!\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/philosophy/",
      "ogUrl": "https://python.useinstructor.com/concepts/philosophy/",
      "title": "Philosophy - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/philosophy/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/philosophy.png",
      "ogTitle": "Philosophy - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/philosophy.png",
      "og:title": "Philosophy - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/philosophy/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/philosophy.png",
      "twitter:title": "Philosophy - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/parallel/#parallel-tools)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/parallel.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/parallel.md \"View source of this page\")\n\n# Parallel Tools [¶](https://python.useinstructor.com/concepts/parallel/\\#parallel-tools \"Permanent link\")\n\nOne of the latest capabilities that OpenAI has recently introduced is parallel function calling. To learn more you can read up on [this](https://platform.openai.com/docs/guides/function-calling/parallel-function-calling)\n\nExperimental Feature\n\nThis feature is currently in preview and is subject to change. only supported by the `gpt-4-turbo-preview` model.\n\n## Understanding Parallel Function Calling [¶](https://python.useinstructor.com/concepts/parallel/\\#understanding-parallel-function-calling \"Permanent link\")\n\nBy using parallel function callings that allow you to call multiple functions in a single request, you can significantly reduce the latency of your application without having to use tricks with now one builds a schema.\n\n```md-code__content\nfrom __future__ import annotations\n\nimport openai\nimport instructor\n\nfrom typing import Iterable, Literal\nfrom pydantic import BaseModel\n\nclass Weather(BaseModel):\n    location: str\n    units: Literal[\"imperial\", \"metric\"]\n\nclass GoogleSearch(BaseModel):\n    query: str\n\nclient = instructor.from_openai(\n    openai.OpenAI(), mode=instructor.Mode.PARALLEL_TOOLS\n)\n\nfunction_calls = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\\\n        {\"role\": \"system\", \"content\": \"You must always use tools\"},\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"What is the weather in toronto and dallas and who won the super bowl?\",\\\n        },\\\n    ],\n    response_model=Iterable[Weather | GoogleSearch],\n)\n\nfor fc in function_calls:\n    print(fc)\n    #> location='Toronto' units='metric'\n    #> location='Dallas' units='imperial'\n    #> query='who won the super bowl'\n\n```\n\nNoticed that the `response_model` Must be in the form `Iterable[Type1 | Type2 | ...]` or `Iterable[Type1]` where `Type1` and `Type2` are the types of the objects that will be returned in the response.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/parallel/",
      "ogUrl": "https://python.useinstructor.com/concepts/parallel/",
      "title": "Understanding Parallel Function Calling in OpenAI - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/parallel/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/parallel.png",
      "ogTitle": "Understanding Parallel Function Calling in OpenAI - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/parallel.png",
      "og:title": "Understanding Parallel Function Calling in OpenAI - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/parallel/",
      "statusCode": 200,
      "description": "Learn about OpenAI's experimental parallel function calling to reduce latency and improve application performance.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn about OpenAI's experimental parallel function calling to reduce latency and improve application performance.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/parallel.png",
      "twitter:title": "Understanding Parallel Function Calling in OpenAI - Instructor",
      "og:description": "Learn about OpenAI's experimental parallel function calling to reduce latency and improve application performance.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn about OpenAI's experimental parallel function calling to reduce latency and improve application performance."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/raw_response/#creating-a-model-with-completions)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/raw_response.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/raw_response.md \"View source of this page\")\n\n# Creating a model with completions [¶](https://python.useinstructor.com/concepts/raw_response/\\#creating-a-model-with-completions \"Permanent link\")\n\nIn instructor>1.0.0 we have a custom client, if you wish to use the raw response you can do the following\n\n```md-code__content\nimport instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\nuser, completion = client.chat.completions.create_with_completion(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserExtract,\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\\\n    ],\n)\n\nprint(user)\n#> name='Jason' age=25\n\nprint(completion)\n\"\"\"\nChatCompletion(\n    id='chatcmpl-AWl4kOf2XIrMZ2cBWC41gXCkFCpQs',\n    choices=[\\\n        Choice(\\\n            finish_reason='stop',\\\n            index=0,\\\n            logprobs=None,\\\n            message=ChatCompletionMessage(\\\n                content=None,\\\n                refusal=None,\\\n                role='assistant',\\\n                audio=None,\\\n                function_call=None,\\\n                tool_calls=[\\\n                    ChatCompletionMessageToolCall(\\\n                        id='call_bGBFg2QrTqw30Y8zXCs9RYGY',\\\n                        function=Function(\\\n                            arguments='{\"name\":\"Jason\",\"age\":25}', name='UserExtract'\\\n                        ),\\\n                        type='function',\\\n                    )\\\n                ],\\\n            ),\\\n        )\\\n    ],\n    created=1732370818,\n    model='gpt-3.5-turbo-0125',\n    object='chat.completion',\n    service_tier=None,\n    system_fingerprint=None,\n    usage=CompletionUsage(\n        completion_tokens=9,\n        prompt_tokens=82,\n        total_tokens=91,\n        completion_tokens_details=CompletionTokensDetails(\n            audio_tokens=0, reasoning_tokens=0\n        ),\n        prompt_tokens_details=None,\n        prompt_token_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0),\n    ),\n)\n\"\"\"\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/raw_response/",
      "ogUrl": "https://python.useinstructor.com/concepts/raw_response/",
      "title": "Creating a Model with OpenAI Completions - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/raw_response/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/raw_response.png",
      "ogTitle": "Creating a Model with OpenAI Completions - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/raw_response.png",
      "og:title": "Creating a Model with OpenAI Completions - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/raw_response/",
      "statusCode": 200,
      "description": "Learn how to create a custom model using OpenAI's API to extract user data efficiently with Python.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to create a custom model using OpenAI's API to extract user data efficiently with Python.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/raw_response.png",
      "twitter:title": "Creating a Model with OpenAI Completions - Instructor",
      "og:description": "Learn how to create a custom model using OpenAI's API to extract user data efficiently with Python.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to create a custom model using OpenAI's API to extract user data efficiently with Python."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/validation/#validation-in-instructor)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/validation.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/validation.md \"View source of this page\")\n\n# Validation in Instructor [¶](https://python.useinstructor.com/concepts/validation/\\#validation-in-instructor \"Permanent link\")\n\nThis guide covers validation concepts and best practices when using Instructor for structured outputs.\n\n## Overview [¶](https://python.useinstructor.com/concepts/validation/\\#overview \"Permanent link\")\n\nValidation in Instructor ensures that the output from language models matches your expected schema. This is crucial for: - Data consistency - Error handling - Type safety - Business logic enforcement\n\n## Basic Validation [¶](https://python.useinstructor.com/concepts/validation/\\#basic-validation \"Permanent link\")\n\nInstructor uses Pydantic for validation, which provides: 1. Type checking 2. Data coercion 3. Custom validators 4. Field constraints\n\n```md-code__content\nfrom pydantic import BaseModel, Field, validator\nfrom typing import List\n\nclass User(BaseModel):\n    name: str = Field(..., min_length=2)\n    age: int = Field(..., ge=0, le=150)\n    emails: List[str]\n\n    @validator('emails')\n    def validate_emails(cls, v):\n        if not all('@' in email for email in v):\n            raise ValueError('Invalid email format')\n        return v\n\n```\n\n## Validation Strategies [¶](https://python.useinstructor.com/concepts/validation/\\#validation-strategies \"Permanent link\")\n\n### 1\\. Field Validation [¶](https://python.useinstructor.com/concepts/validation/\\#1-field-validation \"Permanent link\")\n\nUse Field() for basic constraints:\n\n```md-code__content\nclass Product(BaseModel):\n    name: str = Field(..., min_length=1, max_length=100)\n    price: float = Field(..., gt=0)\n    quantity: int = Field(..., ge=0)\n\n```\n\n### 2\\. Custom Validators [¶](https://python.useinstructor.com/concepts/validation/\\#2-custom-validators \"Permanent link\")\n\nUse [@validator](https://github.com/validator \"GitHub User: validator\") for complex validation:\n\n```md-code__content\nclass Order(BaseModel):\n    items: List[str]\n    total: float\n\n    @validator('total')\n    def validate_total(cls, v, values):\n        if v < 0:\n            raise ValueError('Total cannot be negative')\n        return v\n\n```\n\n### 3\\. Pre-validation Hooks [¶](https://python.useinstructor.com/concepts/validation/\\#3-pre-validation-hooks \"Permanent link\")\n\nUse pre-validation hooks for data transformation:\n\n```md-code__content\nclass UserProfile(BaseModel):\n    username: str\n\n    @validator('username', pre=True)\n    def lowercase_username(cls, v):\n        return v.lower()\n\n```\n\n## Error Handling [¶](https://python.useinstructor.com/concepts/validation/\\#error-handling \"Permanent link\")\n\nInstructor provides robust error handling for validation failures:\n\n```md-code__content\nfrom instructor import patch\nimport openai\n\nclient = patch(openai.OpenAI())\n\ntry:\n    user = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=User,\n        messages=[{\"role\": \"user\", \"content\": \"Extract: John Doe, age: -5\"}],\n    )\nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n\n```\n\n## Best Practices [¶](https://python.useinstructor.com/concepts/validation/\\#best-practices \"Permanent link\")\n\n1. **Start Simple**: Begin with basic type validation before adding complex rules\n2. **Use Type Hints**: Always specify types for better code clarity\n3. **Document Constraints**: Add clear descriptions to Field() definitions\n4. **Handle Errors**: Implement proper error handling for validation failures\n5. **Test Edge Cases**: Verify validation works with unexpected inputs\n\n## Common Patterns [¶](https://python.useinstructor.com/concepts/validation/\\#common-patterns \"Permanent link\")\n\n### Optional Fields [¶](https://python.useinstructor.com/concepts/validation/\\#optional-fields \"Permanent link\")\n\n```md-code__content\nclass Profile(BaseModel):\n    name: str\n    bio: Optional[str] = None\n\n```\n\n### Nested Validation [¶](https://python.useinstructor.com/concepts/validation/\\#nested-validation \"Permanent link\")\n\n```md-code__content\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\nclass User(BaseModel):\n    name: str\n    addresses: List[Address]\n\n```\n\n### Complex Validation [¶](https://python.useinstructor.com/concepts/validation/\\#complex-validation \"Permanent link\")\n\n```md-code__content\nclass Transaction(BaseModel):\n    amount: float\n    currency: str\n    timestamp: datetime\n\n    @validator('currency')\n    def validate_currency(cls, v):\n        valid_currencies = ['USD', 'EUR', 'GBP']\n        if v not in valid_currencies:\n            raise ValueError(f'Currency must be one of {valid_currencies}')\n        return v\n\n```\n\n## Related Resources [¶](https://python.useinstructor.com/concepts/validation/\\#related-resources \"Permanent link\")\n\n- [Pydantic Documentation](https://docs.pydantic.dev/)\n- [OpenAI Function Calling](https://platform.openai.com/docs/guides/gpt/function-calling)\n- [Instructor Examples](https://python.useinstructor.com/examples/)\n\n## Updates and Compatibility [¶](https://python.useinstructor.com/concepts/validation/\\#updates-and-compatibility \"Permanent link\")\n\n- Works with all supported LLM providers\n- Compatible with latest Pydantic versions\n- Regular updates for new validation features\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/validation/",
      "ogUrl": "https://python.useinstructor.com/concepts/validation/",
      "title": "Validation - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/validation/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/validation.png",
      "ogTitle": "Validation - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/validation.png",
      "og:title": "Validation - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/validation/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/validation.png",
      "twitter:title": "Validation - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/unions/#working-with-union-types-in-instructor)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/unions.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/unions.md \"View source of this page\")\n\n# Working with Union Types in Instructor [¶](https://python.useinstructor.com/concepts/unions/\\#working-with-union-types-in-instructor \"Permanent link\")\n\nThis guide explains how to work with union types in Instructor, allowing you to handle multiple possible response types from language models.\n\n## Basic Union Types [¶](https://python.useinstructor.com/concepts/unions/\\#basic-union-types \"Permanent link\")\n\nUnion types let you specify that a field can be one of several types:\n\n```md-code__content\nfrom typing import Union\nfrom pydantic import BaseModel\n\nclass Response(BaseModel):\n    value: Union[str, int]  # Can be either string or integer\n\n```\n\n## Discriminated Unions [¶](https://python.useinstructor.com/concepts/unions/\\#discriminated-unions \"Permanent link\")\n\nUse discriminated unions to handle different response types:\n\n```md-code__content\nfrom typing import Literal, Union\nfrom pydantic import BaseModel\n\nclass UserQuery(BaseModel):\n    type: Literal[\"user\"]\n    username: str\n\nclass SystemQuery(BaseModel):\n    type: Literal[\"system\"]\n    command: str\n\nQuery = Union[UserQuery, SystemQuery]\n\n# Usage with Instructor\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Query,\n    messages=[{\"role\": \"user\", \"content\": \"Parse: user lookup jsmith\"}],\n)\n\n```\n\n## Optional Fields [¶](https://python.useinstructor.com/concepts/unions/\\#optional-fields \"Permanent link\")\n\nCombine Union with Optional for nullable fields:\n\n```md-code__content\nfrom typing import Optional\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    email: Optional[str] = None  # Same as Union[str, None]\n\n```\n\n## Best Practices [¶](https://python.useinstructor.com/concepts/unions/\\#best-practices \"Permanent link\")\n\n1. **Type Hints**: Use proper type hints for clarity\n2. **Discriminators**: Add discriminator fields for complex unions\n3. **Validation**: Add validators for union fields\n4. **Documentation**: Document expected types clearly\n\n## Common Patterns [¶](https://python.useinstructor.com/concepts/unions/\\#common-patterns \"Permanent link\")\n\n### Multiple Response Types [¶](https://python.useinstructor.com/concepts/unions/\\#multiple-response-types \"Permanent link\")\n\n```md-code__content\nfrom typing import Union, Literal\nfrom pydantic import BaseModel\n\nclass SuccessResponse(BaseModel):\n    status: Literal[\"success\"]\n    data: dict\n\nclass ErrorResponse(BaseModel):\n    status: Literal[\"error\"]\n    message: str\n\nResponse = Union[SuccessResponse, ErrorResponse]\n\n```\n\n### Nested Unions [¶](https://python.useinstructor.com/concepts/unions/\\#nested-unions \"Permanent link\")\n\n```md-code__content\nfrom typing import Union, List\nfrom pydantic import BaseModel\n\nclass TextContent(BaseModel):\n    type: Literal[\"text\"]\n    text: str\n\nclass ImageContent(BaseModel):\n    type: Literal[\"image\"]\n    url: str\n\nclass Message(BaseModel):\n    content: List[Union[TextContent, ImageContent]]\n\n```\n\n## Integration with Instructor [¶](https://python.useinstructor.com/concepts/unions/\\#integration-with-instructor \"Permanent link\")\n\n### Validation with Unions [¶](https://python.useinstructor.com/concepts/unions/\\#validation-with-unions \"Permanent link\")\n\n```md-code__content\nfrom instructor import patch\nfrom openai import OpenAI\n\nclient = patch(OpenAI())\n\ndef validate_response(response: Response) -> bool:\n    if isinstance(response, ErrorResponse):\n        return len(response.message) > 0\n    return True\n\nresult = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Response,\n    validation_hook=validate_response,\n    messages=[{\"role\": \"user\", \"content\": \"Process this request\"}],\n)\n\n```\n\n### Streaming with Unions [¶](https://python.useinstructor.com/concepts/unions/\\#streaming-with-unions \"Permanent link\")\n\n```md-code__content\ndef stream_content():\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Message,\n        stream=True,\n        messages=[{\"role\": \"user\", \"content\": \"Generate mixed content\"}],\n    )\n    for partial in response:\n        if partial.content:\n            for item in partial.content:\n                if isinstance(item, TextContent):\n                    print(f\"Text: {item.text}\")\n                elif isinstance(item, ImageContent):\n                    print(f\"Image: {item.url}\")\n\n```\n\n## Error Handling [¶](https://python.useinstructor.com/concepts/unions/\\#error-handling \"Permanent link\")\n\nHandle union type validation errors:\n\n```md-code__content\nfrom pydantic import ValidationError\n\ntry:\n    response = Response(status=\"invalid\", data={\"key\": \"value\"})  # Invalid status\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\n\n```\n\n## Type Checking [¶](https://python.useinstructor.com/concepts/unions/\\#type-checking \"Permanent link\")\n\nUse isinstance() for runtime type checking:\n\n```md-code__content\ndef process_response(response: Response):\n    if isinstance(response, SuccessResponse):\n        # Handle success case\n        process_data(response.data)\n    elif isinstance(response, ErrorResponse):\n        # Handle error case\n        log_error(response.message)\n\n```\n\nFor more information about union types, check out the [Pydantic documentation on unions](https://docs.pydantic.dev/latest/concepts/types/#unions).\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/unions/",
      "ogUrl": "https://python.useinstructor.com/concepts/unions/",
      "title": "Unions - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/unions/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/unions.png",
      "ogTitle": "Unions - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/unions.png",
      "og:title": "Unions - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/unions/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/unions.png",
      "twitter:title": "Unions - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/union/#unions-for-multiple-types)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/union.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/union.md \"View source of this page\")\n\n# Union\n\nPydantic models also support `Union` types, which are used to represent a value that can be one of several types.\n\nWhile many libraries support multiple function calls, and tool calls support multiple returns, the goal is to provide only one way to do things.\n\n## Unions for Multiple Types [¶](https://python.useinstructor.com/concepts/union/\\#unions-for-multiple-types \"Permanent link\")\n\nYou can use `Union` types to write _agents_ that can dynamically choose actions - by choosing an output class. For example, in a search and lookup function, the LLM can determine whether to execute another search, lookup or other action.\n\n```md-code__content\nfrom pydantic import BaseModel\nfrom typing import Union\n\nclass Search(BaseModel):\n    query: str\n\n    def execute(self):\n        return ...\n\nclass Lookup(BaseModel):\n    key: str\n\n    def execute(self):\n        return ...\n\nclass Action(BaseModel):\n    action: Union[Search, Lookup]\n\n    def execute(self):\n        return self.action.execute()\n\n```\n\nSee 'examples/union/run.py' for a working example.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/union/",
      "ogUrl": "https://python.useinstructor.com/concepts/union/",
      "title": "Using Union Types in Pydantic Models - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/union/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/union.png",
      "ogTitle": "Using Union Types in Pydantic Models - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/union.png",
      "og:title": "Using Union Types in Pydantic Models - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/union/",
      "statusCode": 200,
      "description": "Learn how to implement Union types in Pydantic models to handle multiple action types in Python.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to implement Union types in Pydantic models to handle multiple action types in Python.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/union.png",
      "twitter:title": "Using Union Types in Pydantic Models - Instructor",
      "og:description": "Learn how to implement Union types in Pydantic models to handle multiple action types in Python.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to implement Union types in Pydantic models to handle multiple action types in Python."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/fields/#default-values)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/fields.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/fields.md \"View source of this page\")\n\nThe `pydantic.Field` function is used to customize and add metadata to fields of models. To learn more, check out the Pydantic [documentation](https://docs.pydantic.dev/latest/concepts/fields/) as this is a near replica of that documentation that is relevant to prompting.\n\n## Default values [¶](https://python.useinstructor.com/concepts/fields/\\#default-values \"Permanent link\")\n\nThe `default` parameter is used to define a default value for a field.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\n\nclass User(BaseModel):\n    name: str = Field(default='John Doe')\n\nuser = User()\nprint(user)\n#> name='John Doe'\n\n```\n\nYou can also use `default_factory` to define a callable that will be called to generate a default value.\n\n```md-code__content\nfrom uuid import uuid4\n\nfrom pydantic import BaseModel, Field\n\nclass User(BaseModel):\n    id: str = Field(default_factory=lambda: uuid4().hex)\n\n```\n\nInfo\n\nThe `default` and `default_factory` parameters are mutually exclusive.\n\nNote\n\nIf you use `typing.Optional`, it doesn't mean that the field has a default value of `None` you must use `default` or `default_factory` to define a default value. Then it will be considered `not required` when sent to the language model.\n\n## Using `Annotated` [¶](https://python.useinstructor.com/concepts/fields/\\#using-annotated \"Permanent link\")\n\nThe `Field` function can also be used together with `Annotated`.\n\n```md-code__content\nfrom uuid import uuid4\nfrom typing_extensions import Annotated\nfrom pydantic import BaseModel, Field\n\nclass User(BaseModel):\n    id: Annotated[str, Field(default_factory=lambda: uuid4().hex)]\n\n```\n\n## Exclude [¶](https://python.useinstructor.com/concepts/fields/\\#exclude \"Permanent link\")\n\nThe `exclude` parameter can be used to control which fields should be excluded from the model when exporting the model. This is helpful when you want to exclude fields that are not relevant to the model generation like `scratch_pad` or `chain_of_thought`\n\nSee the following example:\n\n```md-code__content\nfrom pydantic import BaseModel, Field\nfrom datetime import date\n\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Reasoning behind the date range.\", exclude=True\n    )\n    start_date: date\n    end_date: date\n\ndate_range = DateRange(\n    chain_of_thought=\"\"\"\n        I want to find the date range for the last 30 days.\n        Today is 2021-01-30 therefore the start date\n        should be 2021-01-01 and the end date is 2021-01-30\"\"\",\n    start_date=date(2021, 1, 1),\n    end_date=date(2021, 1, 30),\n)\nprint(date_range.model_dump_json())\n#> {\"start_date\":\"2021-01-01\",\"end_date\":\"2021-01-30\"}\n\n```\n\n## Omitting fields from schema sent to the language model [¶](https://python.useinstructor.com/concepts/fields/\\#omitting-fields-from-schema-sent-to-the-language-model \"Permanent link\")\n\nIn some cases, you may wish to have the language model ignore certain fields in your model. You can do this by using Pydantic's `SkipJsonSchema` annotation. This omits a field from the JSON schema emitted by Pydantic (which `instructor` uses for constructing its prompts and tool definitions). For example:\n\n```md-code__content\nfrom pydantic import BaseModel\nfrom pydantic.json_schema import SkipJsonSchema\nfrom typing import Union\n\nclass Response(BaseModel):\n    question: str\n    answer: str\n    private_field: SkipJsonSchema[Union[str, None]] = None\n\nassert \"private_field\" not in Response.model_json_schema()[\"properties\"]\n\n```\n\nNote that because the language model will never return a value for `private_field`, you'll need a default value (this can be a generator via a declared Pydantic `Field`).\n\n## Customizing JSON Schema [¶](https://python.useinstructor.com/concepts/fields/\\#customizing-json-schema \"Permanent link\")\n\nThere are some fields that are exclusively used to customise the generated JSON Schema:\n\n- `title`: The title of the field.\n- `description`: The description of the field.\n- `examples`: The examples of the field.\n- `json_schema_extra`: Extra JSON Schema properties to be added to the field.\n\nThese all work as great opportunities to add more information to the JSON schema as part of your prompt engineering.\n\nHere's an example:\n\n```md-code__content\nfrom pydantic import BaseModel, Field, SecretStr\n\nclass User(BaseModel):\n    age: int = Field(description='Age of the user')\n    name: str = Field(title='Username')\n    password: SecretStr = Field(\n        json_schema_extra={\n            'title': 'Password',\n            'description': 'Password of the user',\n            'examples': ['123456'],\n        }\n    )\n\nprint(User.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'age': {'description': 'Age of the user', 'title': 'Age', 'type': 'integer'},\n        'name': {'title': 'Username', 'type': 'string'},\n        'password': {\n            'description': 'Password of the user',\n            'examples': ['123456'],\n            'format': 'password',\n            'title': 'Password',\n            'type': 'string',\n            'writeOnly': True,\n        },\n    },\n    'required': ['age', 'name', 'password'],\n    'title': 'User',\n    'type': 'object',\n}\n\"\"\"\n\n```\n\n# General notes on JSON schema generation [¶](https://python.useinstructor.com/concepts/fields/\\#general-notes-on-json-schema-generation \"Permanent link\")\n\n- The JSON schema for Optional fields indicates that the value null is allowed.\n- The Decimal type is exposed in JSON schema (and serialized) as a string.\n- The JSON schema does not preserve namedtuples as namedtuples.\n- When they differ, you can specify whether you want the JSON schema to represent the inputs to validation or the outputs from serialization.\n- Sub-models used are added to the `$defs` JSON attribute and referenced, as per the spec.\n- Sub-models with modifications (via the Field class) like a custom title, description, or default value, are recursively included instead of referenced.\n- The description for models is taken from either the docstring of the class or the argument description to the Field class.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/fields/",
      "ogUrl": "https://python.useinstructor.com/concepts/fields/",
      "title": "Customizing Pydantic Models with Field Metadata - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/fields/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/fields.png",
      "ogTitle": "Customizing Pydantic Models with Field Metadata - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/fields.png",
      "og:title": "Customizing Pydantic Models with Field Metadata - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/fields/",
      "statusCode": 200,
      "description": "Learn how to enhance Pydantic models with metadata using Field, including default values, JSON schema customization, and more.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to enhance Pydantic models with metadata using Field, including default values, JSON schema customization, and more.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/fields.png",
      "twitter:title": "Customizing Pydantic Models with Field Metadata - Instructor",
      "og:description": "Learn how to enhance Pydantic models with metadata using Field, including default values, JSON schema customization, and more.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to enhance Pydantic models with metadata using Field, including default values, JSON schema customization, and more."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/typeddicts/#typeddicts)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/typeddicts.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/typeddicts.md \"View source of this page\")\n\n# TypedDicts [¶](https://python.useinstructor.com/concepts/typeddicts/\\#typeddicts \"Permanent link\")\n\nWe also support typed dicts.\n\n```md-code__content\nfrom typing_extensions import TypedDict\nfrom openai import OpenAI\nimport instructor\n\nclass User(TypedDict):\n    name: str\n    age: int\n\nclient = instructor.from_openai(OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=User,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Timothy is a man from New York who is turning 32 this year\",\\\n        }\\\n    ],\n)\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/typeddicts/",
      "ogUrl": "https://python.useinstructor.com/concepts/typeddicts/",
      "title": "Using TypedDicts with OpenAI API - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/typeddicts/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/typeddicts.png",
      "ogTitle": "Using TypedDicts with OpenAI API - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/typeddicts.png",
      "og:title": "Using TypedDicts with OpenAI API - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/typeddicts/",
      "statusCode": 200,
      "description": "Learn how to utilize TypedDicts in Python with the OpenAI API for structured data responses.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to utilize TypedDicts in Python with the OpenAI API for structured data responses.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/typeddicts.png",
      "twitter:title": "Using TypedDicts with OpenAI API - Instructor",
      "og:description": "Learn how to utilize TypedDicts in Python with the OpenAI API for structured data responses.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to utilize TypedDicts in Python with the OpenAI API for structured data responses."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/multimodal/#multimodal)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/multimodal.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/multimodal.md \"View source of this page\")\n\n# Multimodal [¶](https://python.useinstructor.com/concepts/multimodal/\\#multimodal \"Permanent link\")\n\nInstructor supports multimodal interactions by providing helper classes that are automatically converted to the correct format for different providers, allowing you to work with both text and images in your prompts and responses. This functionality is implemented in the `multimodal.py` module and provides a seamless way to handle images alongside text for various AI models.\n\n## `Image` [¶](https://python.useinstructor.com/concepts/multimodal/\\#image \"Permanent link\")\n\nThe core of multimodal support in Instructor is the `Image` class. This class represents an image that can be loaded from a URL or file path. It provides methods to create `Image` instances and convert them to formats compatible with different AI providers.\n\nIt's important to note that Anthropic and OpenAI have different formats for handling images in their API requests. The `Image` class in Instructor abstracts away these differences, allowing you to work with a unified interface.\n\n### Usage [¶](https://python.useinstructor.com/concepts/multimodal/\\#usage \"Permanent link\")\n\nYou can create an `Image` instance from a URL or file path using the `from_url` or `from_path` methods. The `Image` class will automatically convert the image to a base64-encoded string and include it in the API request.\n\n```md-code__content\nimport instructor\nimport openai\nfrom pydantic import BaseModel\n\nclass ImageAnalyzer(BaseModel):\n    description: str\n\nimage1 = instructor.Image.from_url(url)\nimage2 = instructor.Image.from_path(\"muffin.jpg\")\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=ImageAnalyzer,\n    messages=[\\\n        {\"role\": \"user\", \"content\": [\"What is in this two images?\", image1, image2]}\\\n    ],\n)\n\nprint(response.model_dump_json())\n\"\"\"\n{\"description\":\"A tray of freshly baked blueberry muffins. The muffins have a golden-brown top, are placed in paper liners, and some have blueberries peeking out. In the background, more muffins are visible, along with a single blueberry on the tray.\"}\n\"\"\"\n\n```\n\nThe `Image` class takes care of the necessary conversions and formatting, ensuring that your code remains clean and provider-agnostic. This flexibility is particularly valuable when you're experimenting with different models or when you need to switch providers based on specific project requirements.\n\nBy leveraging Instructor's multimodal capabilities, you can focus on building your application logic without worrying about the intricacies of each provider's image handling format. This not only saves development time but also makes your code more maintainable and adaptable to future changes in AI provider APIs.\n\nAlternatively, by passing `autodetect_images=True` to `client.chat.completions.create`, you can pass file paths, URLs, or base64 encoded content directly as strings.\n\n```md-code__content\nimport instructor\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=ImageAnalyzer,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": [\\\n                \"What is in this two images?\",\\\n                \"https://example.com/image.jpg\",\\\n                \"path/to/image.jpg\",\\\n            ],\\\n        }\\\n    ],\n    autodetect_images=True,\n)\n\n```\n\n### Anthropic Prompt Caching [¶](https://python.useinstructor.com/concepts/multimodal/\\#anthropic-prompt-caching \"Permanent link\")\n\nInstructor supports Anthropic prompt caching with images. To activate prompt caching, you can pass image content as a dictionary of the form\n\n```md-code__content\n{\"type\": \"image\", \"source\": <path_or_url_or_base64_encoding>, \"cache_control\": True}\n\n```\n\nand set `autodetect_images=True`, or flag it within a constructor such as `instructor.Image.from_path(\"path/to/image.jpg\", cache_control=True)`. For example:\n\n```md-code__content\nimport instructor\nfrom anthropic import Anthropic\n\nclient = instructor.from_anthropic(Anthropic(), enable_prompt_caching=True)\n\ncache_control = {\"type\": \"ephemeral\"}\nresponse = client.chat.completions.create(\n    model=\"claude-3-haiku-20240307\",\n    response_model=ImageAnalyzer,  # This can be set to `None` to return an Anthropic prompt caching message\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": [\\\n                \"What is in this two images?\",\\\n                {\\\n                    \"type\": \"image\",\\\n                    \"source\": \"https://example.com/image.jpg\",\\\n                    \"cache_control\": cache_control,\\\n                },\\\n                {\\\n                    \"type\": \"image\",\\\n                    \"source\": \"path/to/image.jpg\",\\\n                    \"cache_control\": cache_control,\\\n                },\\\n            ],\\\n        }\\\n    ],\n    autodetect_images=True,\n)\n\n```\n\n## `Audio` [¶](https://python.useinstructor.com/concepts/multimodal/\\#audio \"Permanent link\")\n\nThe `Audio` class represents an audio file that can be loaded from a URL or file path. It provides methods to create `Audio` instances but currently only OpenAI supports it. You can create an instance using the `from_path` and `from_url` methods. The `Audio` class will automatically convert it to a base64-encoded image and include it in the API request.\n\n### Usage [¶](https://python.useinstructor.com/concepts/multimodal/\\#usage_1 \"Permanent link\")\n\n```md-code__content\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nimport instructor\nfrom instructor.multimodal import Audio\n\nclient = instructor.from_openai(OpenAI())\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nresp = client.chat.completions.create(\n    model=\"gpt-4o-audio-preview\",\n    response_model=User,\n    modalities=[\"text\"],\n    audio={\"voice\": \"alloy\", \"format\": \"wav\"},\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": [\\\n                \"Extract the following information from the audio:\",\\\n                Audio.from_path(\"./output.wav\"),\\\n            ],\\\n        },  # type: ignore\\\n    ],\n)\n\nprint(resp)\n#> name='Jason' age=20\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/multimodal/",
      "ogUrl": "https://python.useinstructor.com/concepts/multimodal/",
      "title": "Seamless Multimodal Interactions with Instructor - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/multimodal/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/multimodal.png",
      "ogTitle": "Seamless Multimodal Interactions with Instructor - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/multimodal.png",
      "og:title": "Seamless Multimodal Interactions with Instructor - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/multimodal/",
      "statusCode": 200,
      "description": "Learn how the Image and Audio class in Instructor enables seamless handling of images, audio and text across different AI models.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how the Image and Audio class in Instructor enables seamless handling of images, audio and text across different AI models.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/multimodal.png",
      "twitter:title": "Seamless Multimodal Interactions with Instructor - Instructor",
      "og:description": "Learn how the Image and Audio class in Instructor enables seamless handling of images, audio and text across different AI models.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how the Image and Audio class in Instructor enables seamless handling of images, audio and text across different AI models."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/models/#response-model)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/models.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/models.md \"View source of this page\")\n\n# Response Model [¶](https://python.useinstructor.com/concepts/models/\\#response-model \"Permanent link\")\n\nDefining LLM output schemas in Pydantic is done via `pydantic.BaseModel`. To learn more about models in Pydantic, check out their [documentation](https://docs.pydantic.dev/latest/concepts/models/).\n\nAfter defining a Pydantic model, we can use it as the `response_model` in your client `create` calls to OpenAI or any other supported model. The job of the `response_model` parameter is to:\n\n- Define the schema and prompts for the language model\n- Validate the response from the API\n- Return a Pydantic model instance.\n\n## Prompting [¶](https://python.useinstructor.com/concepts/models/\\#prompting \"Permanent link\")\n\nWhen defining a response model, we can use docstrings and field annotations to define the prompt that will be used to generate the response.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\n\nclass User(BaseModel):\n    \"\"\"\n    This is the prompt that will be used to generate the response.\n    Any instructions here will be passed to the language model.\n    \"\"\"\n\n    name: str = Field(description=\"The name of the user.\")\n    age: int = Field(description=\"The age of the user.\")\n\n```\n\nHere all docstrings, types, and field annotations will be used to generate the prompt. The prompt will be generated by the `create` method of the client and will be used to generate the response.\n\n## Optional Values [¶](https://python.useinstructor.com/concepts/models/\\#optional-values \"Permanent link\")\n\nIf we use `Optional` and `default`, they will be considered not required when sent to the language model.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the user.\")\n    age: int = Field(description=\"The age of the user.\")\n    email: Optional[str] = Field(description=\"The email of the user.\", default=None)\n\n```\n\nNote that fields can also be omitted entirely from being sent to the language model by using Pydantic's `SkipJsonSchema` annotation. See [Fields](https://python.useinstructor.com/concepts/fields/#omitting-fields-from-schema-sent-to-the-language-model) for additional details.\n\n## Dynamic model creation [¶](https://python.useinstructor.com/concepts/models/\\#dynamic-model-creation \"Permanent link\")\n\nThere are some occasions where it is desirable to create a model using runtime information to specify the fields. For this, Pydantic provides the create\\_model function to allow models to be created on the fly:\n\n```md-code__content\nfrom pydantic import BaseModel, create_model\n\nclass FooModel(BaseModel):\n    foo: str\n    bar: int = 123\n\nBarModel = create_model(\n    'BarModel',\n    apple=(str, 'russet'),\n    banana=(str, 'yellow'),\n    __base__=FooModel,\n)\nprint(BarModel)\n#> <class '__main__.BarModel'>\nprint(BarModel.model_fields.keys())\n#> dict_keys(['foo', 'bar', 'apple', 'banana'])\n\n```\n\nWhen would I use this?\n\nConsider a situation where the model is dynamically defined, based on some configuration or database. For example, we could have a database table that stores the properties of a model for some model name or id. We could then query the database for the properties of the model and use that to create the model.\n\n```md-code__content\nSELECT property_name, property_type, description\nFROM prompt\nWHERE model_name = {model_name}\n\n```\n\nWe can then use this information to create the model.\n\n```md-code__content\nfrom pydantic import BaseModel, create_model\nfrom typing import List\n\ntypes = {\n    'string': str,\n    'integer': int,\n    'boolean': bool,\n    'number': float,\n    'List[str]': List[str],\n}\n\n# Mocked cursor.fetchall()\ncursor = [\\\n    ('name', 'string', 'The name of the user.'),\\\n    ('age', 'integer', 'The age of the user.'),\\\n    ('email', 'string', 'The email of the user.'),\\\n]\n\nBarModel = create_model(\n    'User',\n    **{\n        property_name: (types[property_type], description)\n        for property_name, property_type, description in cursor\n    },\n    __base__=BaseModel,\n)\n\nprint(BarModel.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'name': {'default': 'The name of the user.', 'title': 'Name', 'type': 'string'},\n        'age': {'default': 'The age of the user.', 'title': 'Age', 'type': 'integer'},\n        'email': {\n            'default': 'The email of the user.',\n            'title': 'Email',\n            'type': 'string',\n        },\n    },\n    'title': 'User',\n    'type': 'object',\n}\n\"\"\"\n\n```\n\nThis would be useful when different users have different descriptions for the same model. We can use the same model but have different prompts for each user.\n\n## Adding Behavior [¶](https://python.useinstructor.com/concepts/models/\\#adding-behavior \"Permanent link\")\n\nWe can add methods to our Pydantic models, just as any plain Python class. We might want to do this to add some custom logic to our models.\n\n```md-code__content\nfrom pydantic import BaseModel\nfrom typing import Literal\n\nfrom openai import OpenAI\n\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\n\nclass SearchQuery(BaseModel):\n    query: str\n    query_type: Literal[\"web\", \"image\", \"video\"]\n\n    def execute(self):\n        print(f\"Searching for {self.query} of type {self.query_type}\")\n        #> Searching for cat of type image\n        return \"Results for cat\"\n\nquery = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Search for a picture of a cat\"}],\n    response_model=SearchQuery,\n)\n\nresults = query.execute()\nprint(results)\n#> Results for cat\n\n```\n\nNow we can call `execute` on our model instance after extracting it from a language model. If you want to see more examples of this checkout our post on [RAG is more than embeddings](https://python.useinstructor.com/blog/2023/09/17/rag-is-more-than-just-embedding-search/)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/models/",
      "ogUrl": "https://python.useinstructor.com/concepts/models/",
      "title": "Using Pydantic for Dynamic LLM Response Models - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/models/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/models.png",
      "ogTitle": "Using Pydantic for Dynamic LLM Response Models - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/models.png",
      "og:title": "Using Pydantic for Dynamic LLM Response Models - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/models/",
      "statusCode": 200,
      "description": "Learn how to define and manage LLM output schemas with Pydantic, including dynamic model creation and adding custom behavior.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to define and manage LLM output schemas with Pydantic, including dynamic model creation and adding custom behavior.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/models.png",
      "twitter:title": "Using Pydantic for Dynamic LLM Response Models - Instructor",
      "og:description": "Learn how to define and manage LLM output schemas with Pydantic, including dynamic model creation and adding custom behavior.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to define and manage LLM output schemas with Pydantic, including dynamic model creation and adding custom behavior."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/reask_validation/#validation-and-reasking)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/reask_validation.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/reask_validation.md \"View source of this page\")\n\n# Validation and Reasking [¶](https://python.useinstructor.com/concepts/reask_validation/\\#validation-and-reasking \"Permanent link\")\n\nInstead of framing \"self-critique\" or \"self-reflection\" in AI as new concepts, we can view them as validation errors with clear error messages that the system can use to self-correct.\n\n## Pydantic [¶](https://python.useinstructor.com/concepts/reask_validation/\\#pydantic \"Permanent link\")\n\nPydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the [Pydantic docs](https://docs.pydantic.dev/latest/concepts/validators/) on validators.\n\nGood llm validation is just good validation\n\nIf you want to see some more examples on validators checkout our blog post [Good LLM validation is just good validation](https://jxnl.github.io/instructor/blog/2023/10/23/good-llm-validation-is-just-good-validation/)\n\n### Code-based Validation Example [¶](https://python.useinstructor.com/concepts/reask_validation/\\#code-based-validation-example \"Permanent link\")\n\nFirst define a Pydantic model with a validator using the `Annotation` class from `typing_extensions`.\n\nEnforce a naming rule using Pydantic's built-in validation:\n\n```md-code__content\nfrom pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated\nfrom pydantic import AfterValidator\n\ndef name_must_contain_space(v: str) -> str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\ntry:\n    person = UserDetail(age=29, name=\"Jason\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n\n```\n\n#### Output for Code-Based Validation [¶](https://python.useinstructor.com/concepts/reask_validation/\\#output-for-code-based-validation \"Permanent link\")\n\n```md-code__content\n1 validation error for UserDetail\nname\n   Value error, name must contain a space (type=value_error)\n\n```\n\nAs we can see, Pydantic raises a validation error when the name attribute does not contain a space. This is a simple example, but it demonstrates how Pydantic can be used to validate attributes of a model.\n\n### LLM-Based Validation Example [¶](https://python.useinstructor.com/concepts/reask_validation/\\#llm-based-validation-example \"Permanent link\")\n\nLLM-based validation can also be plugged into the same Pydantic model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom instructor import llm_validator\nfrom pydantic import BaseModel, ValidationError, BeforeValidator\nfrom typing_extensions import Annotated\n\n# Apply the patch to the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\\\n        str,\\\n        BeforeValidator(llm_validator(\"don't say objectionable things\", client=client)),\\\n    ]\n\ntry:\n    qa = QuestionAnswer(\n        question=\"What is the meaning of life?\",\n        answer=\"The meaning of life is to be evil and steal\",\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for QuestionAnswer\n    answer\n      Assertion failed, The statement promotes objectionable behavior by encouraging evil and stealing. [type=assertion_error, input_value='The meaning of life is to be evil and steal', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/assertion_error\n    \"\"\"\n\n```\n\n#### Output for LLM-Based Validation [¶](https://python.useinstructor.com/concepts/reask_validation/\\#output-for-llm-based-validation \"Permanent link\")\n\nIt is important to note here that the error message is generated by the LLM, not the code, so it'll be helpful for re-asking the model.\n\n```md-code__content\n1 validation error for QuestionAnswer\nanswer\n   Assertion failed, The statement is objectionable. (type=assertion_error)\n\n```\n\n## Using Reasking Logic to Correct Outputs [¶](https://python.useinstructor.com/concepts/reask_validation/\\#using-reasking-logic-to-correct-outputs \"Permanent link\")\n\nValidators are a great tool for ensuring some property of the outputs. When you use the `patch()` method with the `openai` client, you can use the `max_retries` parameter to set the number of times you can reask the model to correct the output.\n\nIt is a great layer of defense against bad outputs of two forms:\n\n1. Pydantic Validation Errors (code or llm based)\n2. JSON Decoding Errors (when the model returns a bad response)\n\n### Step 1: Define the Response Model with Validators [¶](https://python.useinstructor.com/concepts/reask_validation/\\#step-1-define-the-response-model-with-validators \"Permanent link\")\n\nNotice that the field validator wants the name in uppercase, but the user input is lowercase. The validator will raise a `ValueError` if the name is not in uppercase.\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel, field_validator\n\n# Apply the patch to the OpenAI client\nclient = instructor.from_openai(openai.OpenAI())\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n\n```\n\n### Step 2. Using the Client with Retries [¶](https://python.useinstructor.com/concepts/reask_validation/\\#step-2-using-the-client-with-retries \"Permanent link\")\n\nHere, the `UserDetails` model is passed as the `response_model`, and `max_retries` is set to 2.\n\n```md-code__content\nimport instructor\nimport openai\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\nmodel = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetails,\n    max_retries=2,\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\\\n    ],\n)\n\nprint(model.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Jason\",\n  \"age\": 25\n}\n\"\"\"\n\n```\n\n### What happens behind the scenes? [¶](https://python.useinstructor.com/concepts/reask_validation/\\#what-happens-behind-the-scenes \"Permanent link\")\n\nBehind the scenes, the `instructor.from_openai()` method adds a `max_retries` parameter to the `openai.ChatCompletion.create()` method. The `max_retries` parameter will trigger up to 2 reattempts if the `name` attribute fails the uppercase validation in `UserDetails`.\n\n```md-code__content\nfrom pydantic import ValidationError\n\ntry:\n    ...\nexcept ValidationError as e:\n    kwargs[\"messages\"].append(response.choices[0].message)\n    kwargs[\"messages\"].append(\n        {\n            \"role\": \"user\",\n            \"content\": f\"Please correct the function call; errors encountered:\\n{e}\",\n        }\n    )\n\n```\n\n## Advanced Validation Techniques [¶](https://python.useinstructor.com/concepts/reask_validation/\\#advanced-validation-techniques \"Permanent link\")\n\nThe docs are currently incomplete, but we have a few advanced validation techniques that we're working on documenting better such as model level validation, and using a validation context. Check out our example on [verifying citations](https://python.useinstructor.com/examples/exact_citations/) which covers:\n\n1. Validate the entire object with all attributes rather than one attribute at a time\n2. Using some 'context' to validate the object: In this case, we use the `context` to check if the citation existed in the original text.\n\n## Optimizing Token usage [¶](https://python.useinstructor.com/concepts/reask_validation/\\#optimizing-token-usage \"Permanent link\")\n\nPydantic automatically includes a URL within the error message itself when an error is thrown so that users can learn more about the specific error that was thrown. Some users might want to remove this URL since it adds extra tokens that otherwise might not add much value to the validation process.\n\nWe've created a small helper function that you can use below which removes this url in the error message\n\n```md-code__content\nfrom instructor.utils import disable_pydantic_error_url\nfrom pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated\nfrom pydantic import AfterValidator\n\ndisable_pydantic_error_url()\n\ndef name_must_contain_space(v: str) -> str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\ntry:\n    person = UserDetail(age=29, name=\"Jason\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n\n```\n\n## Takeaways [¶](https://python.useinstructor.com/concepts/reask_validation/\\#takeaways \"Permanent link\")\n\nBy integrating these advanced validation techniques, we not only improve the quality and reliability of LLM-generated content, but also pave the way for more autonomous and effective systems.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/reask_validation/",
      "ogUrl": "https://python.useinstructor.com/concepts/reask_validation/",
      "title": "Enhancing AI Validations with Pydantic's Framework - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/reask_validation/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/reask_validation.png",
      "ogTitle": "Enhancing AI Validations with Pydantic's Framework - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/reask_validation.png",
      "og:title": "Enhancing AI Validations with Pydantic's Framework - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/reask_validation/",
      "statusCode": 200,
      "description": "Learn how to improve AI outputs using Pydantic for validation and reasking techniques.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to improve AI outputs using Pydantic for validation and reasking techniques.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/reask_validation.png",
      "twitter:title": "Enhancing AI Validations with Pydantic's Framework - Instructor",
      "og:description": "Learn how to improve AI outputs using Pydantic for validation and reasking techniques.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to improve AI outputs using Pydantic for validation and reasking techniques."
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/usage.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/usage.md \"View source of this page\")\n\n# Usage Tokens\n\nThe easiest way to get usage for non streaming requests is to access the raw response.\n\n```md-code__content\nimport instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\nuser, completion = client.chat.completions.create_with_completion(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserExtract,\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\\\n    ],\n)\n\nprint(completion.usage)\n\"\"\"\nCompletionUsage(\n    completion_tokens=9,\n    prompt_tokens=82,\n    total_tokens=91,\n    completion_tokens_details=CompletionTokensDetails(\n        audio_tokens=0, reasoning_tokens=0\n    ),\n    prompt_tokens_details=None,\n    prompt_token_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0),\n)\n\"\"\"\n\n```\n\nYou can catch an IncompleteOutputException whenever the context length is exceeded and react accordingly, such as by trimming your prompt by the number of exceeding tokens.\n\n```md-code__content\nfrom instructor.exceptions import IncompleteOutputException\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\ntry:\n    client.chat.completions.create_with_completion(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserExtract,\n        messages=[\\\n            {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\\\n        ],\n    )\nexcept IncompleteOutputException as e:\n    token_count = e.last_completion.usage.total_tokens  # type: ignore\n    # your logic here\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/usage/",
      "ogUrl": "https://python.useinstructor.com/concepts/usage/",
      "title": "Handling Non-Streaming Requests in OpenAI with Usage Tracking - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/usage/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/usage.png",
      "ogTitle": "Handling Non-Streaming Requests in OpenAI with Usage Tracking - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/usage.png",
      "og:title": "Handling Non-Streaming Requests in OpenAI with Usage Tracking - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/usage/",
      "statusCode": 200,
      "description": "Learn how to manage non-streaming requests in OpenAI, track token usage, and handle exceptions with Python.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to manage non-streaming requests in OpenAI, track token usage, and handle exceptions with Python.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/usage.png",
      "twitter:title": "Handling Non-Streaming Requests in OpenAI with Usage Tracking - Instructor",
      "og:description": "Learn how to manage non-streaming requests in OpenAI, track token usage, and handle exceptions with Python.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to manage non-streaming requests in OpenAI, track token usage, and handle exceptions with Python."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/fastapi/#integrating-pydantic-models-with-fastapi)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/fastapi.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/fastapi.md \"View source of this page\")\n\n# Integrating Pydantic Models with FastAPI [¶](https://python.useinstructor.com/concepts/fastapi/\\#integrating-pydantic-models-with-fastapi \"Permanent link\")\n\n[FastAPI](https://fastapi.tiangolo.com/) is an enjoyable tool for building web applications in Python. It is well known for its integration with `Pydantic` models, which makes defining and validating data structures straightforward and efficient. In this guide, we explore how simple functions that return `Pydantic` models can seamlessly integrate with `FastAPI`.\n\n## Why Choose FastAPI and Pydantic? [¶](https://python.useinstructor.com/concepts/fastapi/\\#why-choose-fastapi-and-pydantic \"Permanent link\")\n\n- FastAPI is a modern, high-performance web framework for building APIs with Python.\n- Supports OpenAPI and JSON Schema for automatic documentation and validation.\n- Supports AsyncIO for asynchronous programming leveraging the AsyncOpenAI() client\n\n## Code Example: Starting a FastAPI App with a POST Request [¶](https://python.useinstructor.com/concepts/fastapi/\\#code-example-starting-a-fastapi-app-with-a-post-request \"Permanent link\")\n\nThe following code snippet demonstrates how to start a `FastAPI` app with a POST endpoint. This endpoint accepts and returns data defined by a `Pydantic` model.\n\n```md-code__content\nimport instructor\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\n\n# Enables response_model\nclient = instructor.from_openai(AsyncOpenAI())\napp = FastAPI()\n\nclass UserData(BaseModel):\n    # This can be the model for the input data\n    query: str\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@app.post(\"/endpoint\", response_model=UserDetail)\nasync def endpoint_function(data: UserData) -> UserDetail:\n    user_detail = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\\\n            {\"role\": \"user\", \"content\": f\"Extract: `{data.query}`\"},\\\n        ],\n    )\n    return user_detail\n\n```\n\n## Streaming Responses with FastAPI [¶](https://python.useinstructor.com/concepts/fastapi/\\#streaming-responses-with-fastapi \"Permanent link\")\n\n`FastAPI` supports streaming responses, which is useful for returning large amounts of data. This feature is particularly useful when working with large language models (LLMs) that generate a large amount of data.\n\n```md-code__content\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass UserData(BaseModel):\n    query: str\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n# Route to handle SSE events and return users\n@app.post(\"/extract\", response_class=StreamingResponse)\nasync def extract(data: UserData):\n    users = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        stream=True,\n        messages=[\\\n            {\"role\": \"user\", \"content\": data.query},\\\n        ],\n    )\n\n    async def generate():\n        async for user in users:\n            resp_json = user.model_dump_json()\n            yield f\"data: {resp_json}\"\n        yield \"data: [DONE]\"\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n\n```\n\n## Automatic Documentation with FastAPI [¶](https://python.useinstructor.com/concepts/fastapi/\\#automatic-documentation-with-fastapi \"Permanent link\")\n\nFastAPI leverages the OpenAPI specification to automatically generate a dynamic and interactive documentation page, commonly referred to as the `/docs` page. This feature is incredibly useful for developers, as it offers a live environment to test API endpoints directly through the browser.\n\nTo explore the capabilities of your API, follow these steps:\n\n1. Run the API using the Uvicorn command: `uvicorn main:app --reload`.\n2. Open your web browser and navigate to `http://127.0.0.1:8000/docs`.\n3. You will find an interactive UI where you can send different requests to your API and see the responses in real-time.\n\n![Screenshot of FastAPI /docs page](https://python.useinstructor.com/concepts/response.png)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/fastapi/",
      "ogUrl": "https://python.useinstructor.com/concepts/fastapi/",
      "title": "Integrating Pydantic with FastAPI for Efficient APIs - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/fastapi/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/fastapi.png",
      "ogTitle": "Integrating Pydantic with FastAPI for Efficient APIs - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/fastapi.png",
      "og:title": "Integrating Pydantic with FastAPI for Efficient APIs - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/fastapi/",
      "statusCode": 200,
      "description": "Learn how to leverage Pydantic models with FastAPI for seamless API development and automatic documentation.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to leverage Pydantic models with FastAPI for seamless API development and automatic documentation.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/fastapi.png",
      "twitter:title": "Integrating Pydantic with FastAPI for Efficient APIs - Instructor",
      "og:description": "Learn how to leverage Pydantic models with FastAPI for seamless API development and automatic documentation.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to leverage Pydantic models with FastAPI for seamless API development and automatic documentation."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/caching/#1-functoolscache-for-simple-in-memory-caching)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/caching.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/caching.md \"View source of this page\")\n\n# Caching\n\n* * *\n\ntitle: Caching Techniques in Python: In-Memory, Disk, and Redis description: Explore caching methods in Python with functools, diskcache, and Redis for improved performance in your applications.\n\n* * *\n\nIf you want to learn more about concepts in caching and how to use them in your own projects, check out our [blog](https://python.useinstructor.com/blog/2023/11/26/python-caching/) on the topic.\n\n## 1\\. `functools.cache` for Simple In-Memory Caching [¶](https://python.useinstructor.com/concepts/caching/\\#1-functoolscache-for-simple-in-memory-caching \"Permanent link\")\n\n**When to Use**: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session. or in an application where we don't need to persist the cache between sessions.\n\n```md-code__content\nimport time\nimport functools\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@functools.cache\ndef extract(data) -> UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\\\n            {\"role\": \"user\", \"content\": data},\\\n        ],\n    )\n\nstart = time.perf_counter()  #\nmodel = extract(\"Extract jason is 25 years old\")\nprint(f\"Time taken: {time.perf_counter() - start}\")\n#> Time taken: 0.38183529197704047\n\nstart = time.perf_counter()\nmodel = extract(\"Extract jason is 25 years old\")  #\nprint(f\"Time taken: {time.perf_counter() - start}\")\n#> Time taken: 8.75093974173069e-07\n\n```\n\nChanging the Model does not Invalidate the Cache\n\nNote that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result.\n\nNow we can call `extract` multiple times with the same argument, and the result will be cached in memory for faster access.\n\n**Benefits**: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries.\n\nWhat is a decorator?\n\nA decorator is a function that takes another function and extends the behavior of the latter function without explicitly modifying it. In Python, decorators are functions that take a function as an argument and return a closure.\n\n```md-code__content\ndef decorator(func):\n    def wrapper(*args, **kwargs):\n        print(\"Do something before\")  #\n        #> Do something before\n        result = func(*args, **kwargs)\n        print(\"Do something after\")  #\n        #> Do something after\n        return result\n\n    return wrapper\n\n@decorator\ndef say_hello():\n    #> Hello!\n    print(\"Hello!\")\n    #> Hello!\n\nsay_hello()\n#> \"Do something before\"\n#> \"Hello!\"\n#> \"Do something after\"\n\n```\n\n## 2\\. `diskcache` for Persistent, Large Data Caching [¶](https://python.useinstructor.com/concepts/caching/\\#2-diskcache-for-persistent-large-data-caching \"Permanent link\")\n\nCopy Caching Code\n\nWe'll be using the same `instructor_cache` decorator for both `diskcache` and `redis` caching. You can copy the code below and use it for both examples.\n\n```md-code__content\nimport functools\nimport inspect\nimport diskcache\n\ncache = diskcache.Cache('./my_cache_directory')  #\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  #\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n```\n\nRemember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the `Model.model_json_schema()` as part of the key.\n\n**When to Use**: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data!\n\n```md-code__content\nimport functools\nimport inspect\nimport instructor\nimport diskcache\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\ncache = diskcache.Cache('./my_cache_directory')\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation  #\n    if not issubclass(return_type, BaseModel):  #\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (\n            f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  #\n        )\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@instructor_cache\ndef extract(data) -> UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\\\n            {\"role\": \"user\", \"content\": data},\\\n        ],\n    )\n\n```\n\n**Benefits**: Reduces computation time for heavy data processing, provides disk-based caching for persistence.\n\n## 2\\. Redis Caching Decorator for Distributed Systems [¶](https://python.useinstructor.com/concepts/caching/\\#2-redis-caching-decorator-for-distributed-systems \"Permanent link\")\n\nCopy Caching Code\n\nWe'll be using the same `instructor_cache` decorator for both `diskcache` and `redis` caching. You can copy the code below and use it for both examples.\n\n```md-code__content\nimport functools\nimport inspect\nimport redis\n\ncache = redis.Redis(\"localhost\")\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n```\n\nRemember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the `Model.model_json_schema()` as part of the key.\n\n**When to Use**: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures.\n\n```md-code__content\nimport redis\nimport functools\nimport inspect\nimport instructor\n\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\ncache = redis.Redis(\"localhost\")\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  #\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  #\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@instructor_cache\ndef extract(data) -> UserDetail:\n    # Assuming client.chat.completions.create returns a UserDetail instance\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\\\n            {\"role\": \"user\", \"content\": data},\\\n        ],\n    )\n\n```\n\n**Benefits**: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types.\n\nLooking carefully\n\nIf you look carefully at the code above you'll notice that we're using the same `instructor_cache` decorator as before. The implementation is the same, but we're using a different caching backend!\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/caching/",
      "ogUrl": "https://python.useinstructor.com/concepts/caching/",
      "title": "Caching - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/caching/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/caching.png",
      "ogTitle": "Caching - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/caching.png",
      "og:title": "Caching - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/caching/",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/caching.png",
      "twitter:title": "Caching - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/#concepts)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/index.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/index.md \"View source of this page\")\n\n# Concepts [¶](https://python.useinstructor.com/concepts/\\#concepts \"Permanent link\")\n\nWelcome to the Concepts section of Instructor documentation. This section provides an in-depth exploration of key ideas and techniques that form the foundation of working with structured outputs in AI applications using Instructor.\n\n## Introduction [¶](https://python.useinstructor.com/concepts/\\#introduction \"Permanent link\")\n\nInstructor is designed to simplify the process of extracting structured data from large language models (LLMs). By leveraging the power of Pydantic and OpenAI's function calling API, Instructor enables developers to create robust, type-safe applications that can efficiently process and validate AI-generated outputs.\n\nIn this section, we'll cover a range of concepts that are crucial for understanding and effectively using Instructor. Whether you're new to the library or looking to deepen your knowledge, these guides will provide valuable insights into the core principles and advanced features of Instructor.\n\n## Key Concepts [¶](https://python.useinstructor.com/concepts/\\#key-concepts \"Permanent link\")\n\nHere's an overview of the concepts we'll explore:\n\n1. [Aliases](https://python.useinstructor.com/concepts/alias/): Learn how to use aliases to customize field names in your Pydantic models.\n\n2. [Caching](https://python.useinstructor.com/concepts/caching/): Discover techniques for improving performance through effective data management and caching strategies.\n\n3. [Templating](https://python.useinstructor.com/concepts/templating/): Explore Jinja templating for dynamic and efficient prompt management.\n\n4. [Type Adapter](https://python.useinstructor.com/concepts/typeadapter/): Understand Pydantic's Type Adapter for enhanced data validation and parsing.\n\n5. [TypedDicts](https://python.useinstructor.com/concepts/typeddicts/): Learn about using TypedDicts for structured data handling with OpenAI's API.\n\n6. [Types](https://python.useinstructor.com/concepts/types/): Dive into the various data types supported by Instructor, from simple to complex.\n\n7. [Union](https://python.useinstructor.com/concepts/union/): Explore the use of Union types for flexible and dynamic operations in your models.\n\n8. [Usage](https://python.useinstructor.com/concepts/usage/): Get insights on handling non-streaming requests and managing token usage with the OpenAI API.\n\n\nEach of these concepts plays a crucial role in building efficient, type-safe, and robust applications with Instructor. By mastering these ideas, you'll be well-equipped to tackle complex data extraction and validation tasks in your AI-powered projects.\n\nWe encourage you to explore these concepts in depth and see how they can be applied to your specific use cases. Remember, the power of Instructor lies in its ability to combine these concepts seamlessly, allowing you to create sophisticated applications with ease.\n\nHappy learning, and enjoy your journey through the world of structured outputs with Instructor!\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/",
      "ogUrl": "https://python.useinstructor.com/concepts/",
      "title": "Key Concepts for Structured Outputs in AI - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/index.png",
      "ogTitle": "Key Concepts for Structured Outputs in AI - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/index.png",
      "og:title": "Key Concepts for Structured Outputs in AI - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/",
      "statusCode": 200,
      "description": "Explore essential concepts in Instructor for efficient extraction and validation of structured data from AI models.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Explore essential concepts in Instructor for efficient extraction and validation of structured data from AI models.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/index.png",
      "twitter:title": "Key Concepts for Structured Outputs in AI - Instructor",
      "og:description": "Explore essential concepts in Instructor for efficient extraction and validation of structured data from AI models.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Explore essential concepts in Instructor for efficient extraction and validation of structured data from AI models."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/maybe/#handling-missing-data)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/maybe.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/maybe.md \"View source of this page\")\n\n# Handling Missing Data [¶](https://python.useinstructor.com/concepts/maybe/\\#handling-missing-data \"Permanent link\")\n\nThe `Maybe` pattern is a concept in functional programming used for error handling. Instead of raising exceptions or returning `None`, you can use a `Maybe` type to encapsulate both the result and potential errors.\n\nThis pattern is particularly useful when making LLM calls, as providing language models with an escape hatch can effectively reduce hallucinations.\n\n## Defining the Model [¶](https://python.useinstructor.com/concepts/maybe/\\#defining-the-model \"Permanent link\")\n\nUsing Pydantic, we'll first define the `UserDetail` and `MaybeUser` classes.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n\n    def __bool__(self):\n        return self.result is not None\n\n```\n\nNotice that `MaybeUser` has a `result` field that is an optional `UserDetail` instance where the extracted data will be stored. The `error` field is a boolean that indicates whether an error occurred, and the `message` field is an optional string that contains the error message.\n\n## Defining the function [¶](https://python.useinstructor.com/concepts/maybe/\\#defining-the-function \"Permanent link\")\n\nOnce we have the model defined, we can create a function that uses the `Maybe` pattern to extract the data.\n\n```md-code__content\nimport instructor\nimport openai\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\n# This enables the `response_model` keyword\nclient = instructor.from_openai(openai.OpenAI())\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n\n    def __bool__(self):\n        return self.result is not None\n\ndef extract(content: str) -> MaybeUser:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MaybeUser,\n        messages=[\\\n            {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},\\\n        ],\n    )\n\nuser1 = extract(\"Jason is a 25-year-old scientist\")\nprint(user1.model_dump_json(indent=2))\n\"\"\"\n{\n  \"result\": {\n    \"age\": 25,\n    \"name\": \"Jason\",\n    \"role\": \"scientist\"\n  },\n  \"error\": false,\n  \"message\": null\n}\n\"\"\"\n\nuser2 = extract(\"Unknown user\")\nprint(user2.model_dump_json(indent=2))\n\"\"\"\n{\n  \"result\": null,\n  \"error\": false,\n  \"message\": null\n}\n\"\"\"\n\n```\n\nAs you can see, when the data is extracted successfully, the `result` field contains the `UserDetail` instance. When an error occurs, the `error` field is set to `True`, and the `message` field contains the error message.\n\nIf you want to learn more about pattern matching, check out Pydantic's docs on [Structural Pattern Matching](https://docs.pydantic.dev/latest/concepts/models/#structural-pattern-matching)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/maybe/",
      "ogUrl": "https://python.useinstructor.com/concepts/maybe/",
      "title": "Implementing the Maybe Pattern for Error Handling in Functional Programming - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/maybe/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/maybe.png",
      "ogTitle": "Implementing the Maybe Pattern for Error Handling in Functional Programming - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/maybe.png",
      "og:title": "Implementing the Maybe Pattern for Error Handling in Functional Programming - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/maybe/",
      "statusCode": 200,
      "description": "Learn how to use the Maybe pattern with Pydantic in Python for robust error handling when extracting user details.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to use the Maybe pattern with Pydantic in Python for robust error handling when extracting user details.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/maybe.png",
      "twitter:title": "Implementing the Maybe Pattern for Error Handling in Functional Programming - Instructor",
      "og:description": "Learn how to use the Maybe pattern with Pydantic in Python for robust error handling when extracting user details.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to use the Maybe pattern with Pydantic in Python for robust error handling when extracting user details."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/types/#support-for-simple-types)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/types.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/types.md \"View source of this page\")\n\n# Support for Simple Types [¶](https://python.useinstructor.com/concepts/types/\\#support-for-simple-types \"Permanent link\")\n\nAside from the recommended `pydantic.BaseModel`, and [Iterable](https://python.useinstructor.com/concepts/lists/), and [Partial](https://python.useinstructor.com/concepts/partial/),\n\nInstructor supports simple types like `str`, `int`, `float`, `bool`, `Union`, `Literal`, out of the box. You can use these types directly in your response models.\n\nTo add more descriptions you can also use `typing.Annotated` to include more information about the type.\n\n## What happens behind the scenes? [¶](https://python.useinstructor.com/concepts/types/\\#what-happens-behind-the-scenes \"Permanent link\")\n\nWe will actually wrap the response model with a `pydantic.BaseModel` of the following form:\n\n```\nfrom typing import Annotated\nfrom pydantic import create_model, Field, BaseModel\n\ntypehint = Annotated[bool, Field(description=\"Sample Description\")]\n\nmodel = create_model(\"Response\", content=(typehint, ...), __base__=BaseModel)\n\nprint(model.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'content': {\n            'description': 'Sample Description',\n            'title': 'Content',\n            'type': 'boolean',\n        }\n    },\n    'required': ['content'],\n    'title': 'Response',\n    'type': 'object',\n}\n\"\"\"\n\n```\n\n## Primitive Types (str, int, float, bool) [¶](https://python.useinstructor.com/concepts/types/\\#primitive-types-str-int-float-bool \"Permanent link\")\n\n```\nimport instructor\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n\n# Response model with simple types like str, int, float, bool\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=bool,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Is it true that Paris is the capital of France?\",\\\n        },\\\n    ],\n)\nassert resp is True, \"Paris is the capital of France\"\nprint(resp)\n#> True\n\n```\n\n## Annotated [¶](https://python.useinstructor.com/concepts/types/\\#annotated \"Permanent link\")\n\nAnnotations can be used to add more information about the type. This can be useful for adding descriptions to the type, along with more complex information like field names, and more.\n\n```\nimport instructor\nimport openai\nfrom typing import Annotated\nfrom pydantic import Field\n\nclient = instructor.from_openai(openai.OpenAI())\n\nUpperCaseStr = Annotated[str, Field(description=\"string must be upper case\")]\n\n# Response model with simple types like str, int, float, bool\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UpperCaseStr,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"What is the capital of france?\",\\\n        },\\\n    ],\n)\nassert resp == \"PARIS\", \"Paris is the capital of France\"\nprint(resp)\n#> PARIS\n\n```\n\n## Literal [¶](https://python.useinstructor.com/concepts/types/\\#literal \"Permanent link\")\n\nWhen doing simple classification Literals go quite well, they support literal of string, int, bool.\n\n```\nimport instructor\nimport openai\nfrom typing import Literal\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Literal[\"BILLING\", \"SHIPPING\"],\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Classify the following messages: 'I am having trouble with my billing'\",\\\n        },\\\n    ],\n)\nassert resp == \"BILLING\"\nprint(resp)\n#> BILLING\n\n```\n\n## Enum [¶](https://python.useinstructor.com/concepts/types/\\#enum \"Permanent link\")\n\nEnums are harder to get right without some addition promping but are useful if these are values that are shared across the application.\n\n```\nimport instructor\nimport openai\nfrom enum import Enum\n\nclass Label(str, Enum):\n    BILLING = \"BILLING\"\n    SHIPPING = \"SHIPPING\"\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Label,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Classify the following messages: 'I am having trouble with my billing'\",\\\n        },\\\n    ],\n)\nassert resp == Label.BILLING\nprint(resp)\n#> BILLING\n\n```\n\n## List [¶](https://python.useinstructor.com/concepts/types/\\#list \"Permanent link\")\n\n```\nimport instructor\nimport openai\nfrom typing import List\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=List[int],\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Give me the first 5 prime numbers\",\\\n        },\\\n    ],\n)\n\nassert resp == [2, 3, 5, 7, 11]\nprint(resp)\n#> [2, 3, 5, 7, 11]\n\n```\n\n## Union [¶](https://python.useinstructor.com/concepts/types/\\#union \"Permanent link\")\n\nUnion is a great way to handle multiple types of responses, similar to multiple function calls but not limited to the function calling api, like in JSON\\_SCHEMA modes.\n\n```\nimport instructor\nimport openai\nfrom pydantic import BaseModel\nfrom typing import Union\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass Add(BaseModel):\n    a: int\n    b: int\n\nclass Weather(BaseModel):\n    location: str\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Union[Add, Weather],\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"What is 5 + 5?\",\\\n        },\\\n    ],\n)\n\nassert resp == Add(a=5, b=5)\nprint(resp)\n#> a=5 b=5\n\n```\n\n## Complex Types [¶](https://python.useinstructor.com/concepts/types/\\#complex-types \"Permanent link\")\n\n### Pandas DataFrame [¶](https://python.useinstructor.com/concepts/types/\\#pandas-dataframe \"Permanent link\")\n\nThis is a more complex example, where we use a custom type to convert markdown to a pandas DataFrame.\n\n```\nfrom io import StringIO\nfrom typing import Annotated, Any\nfrom pydantic import BeforeValidator, PlainSerializer, InstanceOf, WithJsonSchema\nimport pandas as pd\nimport instructor\nimport openai\n\ndef md_to_df(data: Any) -> Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\nMarkdownDataFrame = Annotated[\\\n    # Validates final type\\\n    InstanceOf[pd.DataFrame],\\\n    # Converts markdown to DataFrame\\\n    BeforeValidator(md_to_df),\\\n    # Converts DataFrame to markdown on model_dump_json\\\n    PlainSerializer(lambda df: df.to_markdown()),\\\n    # Adds a description to the type\\\n    WithJsonSchema(\\\n        {\\\n            \"type\": \"string\",\\\n            \"description\": \"\"\"\\\n            The markdown representation of the table,\\\n            each one should be tidy, do not try to join\\\n            tables that should be seperate\"\"\",\\\n        }\\\n    ),\\\n]\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=MarkdownDataFrame,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Jason is 20, Sarah is 30, and John is 40\",\\\n        },\\\n    ],\n)\n\nassert isinstance(resp, pd.DataFrame)\nprint(resp)\n\"\"\"\n        Age\n Name\nJason     20\nSarah     30\nJohn      40\n\"\"\"\n\n```\n\n### Lists of Unions [¶](https://python.useinstructor.com/concepts/types/\\#lists-of-unions \"Permanent link\")\n\nJust like Unions we can use List of Unions to represent multiple types of responses. This will feel similar to the parallel function calls but not limited to the function calling api, like in JSON\\_SCHEMA modes.\n\n```\nimport instructor\nimport openai\nfrom pydantic import BaseModel\nfrom typing import Union, List\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass Weather(BaseModel, frozen=True):\n    location: str\n\nclass Add(BaseModel, frozen=True):\n    a: int\n    b: int\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=List[Union[Add, Weather]],\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Add 5 and 5, and also whats the weather in Toronto?\",\\\n        },\\\n    ],\n)\n\nassert resp == [Add(a=5, b=5), Weather(location=\"Toronto\")]\nprint(resp)\n#> [Add(a=5, b=5), Weather(location='Toronto')]\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/types/",
      "ogUrl": "https://python.useinstructor.com/concepts/types/",
      "title": "Support for Simple Types in Python with Pydantic - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/types/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/types.png",
      "ogTitle": "Support for Simple Types in Python with Pydantic - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/types.png",
      "og:title": "Support for Simple Types in Python with Pydantic - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/types/",
      "statusCode": 200,
      "description": "Learn how to use simple types like str, int, float, and more in Python Pydantic models for effective API responses.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to use simple types like str, int, float, and more in Python Pydantic models for effective API responses.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/types.png",
      "twitter:title": "Support for Simple Types in Python with Pydantic - Instructor",
      "og:description": "Learn how to use simple types like str, int, float, and more in Python Pydantic models for effective API responses.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to use simple types like str, int, float, and more in Python Pydantic models for effective API responses."
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/alias.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/alias.md \"View source of this page\")\n\n# Alias\n\nThis page is a work in progress\n\nThis page is a work in progress. Check out [Pydantic's documentation](https://docs.pydantic.dev/latest/concepts/alias/)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/alias/",
      "ogUrl": "https://python.useinstructor.com/concepts/alias/",
      "title": "Pydantic Aliases Overview - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/alias/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/alias.png",
      "ogTitle": "Pydantic Aliases Overview - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/alias.png",
      "og:title": "Pydantic Aliases Overview - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/alias/",
      "statusCode": 200,
      "description": "Explore the concept of aliases in Pydantic. Discover the latest documentation and features for better data validation.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Explore the concept of aliases in Pydantic. Discover the latest documentation and features for better data validation.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/alias.png",
      "twitter:title": "Pydantic Aliases Overview - Instructor",
      "og:description": "Explore the concept of aliases in Pydantic. Discover the latest documentation and features for better data validation.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Explore the concept of aliases in Pydantic. Discover the latest documentation and features for better data validation."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/retrying/#retrying)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/retrying.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/retrying.md \"View source of this page\")\n\n# Retrying [¶](https://python.useinstructor.com/concepts/retrying/\\#retrying \"Permanent link\")\n\nOne of the benefits of having Pydantic is the ease with which we can define validators. We cover this topic in many articles, like [Reasking Validation](https://python.useinstructor.com/concepts/reask_validation/) and in our blog post [Good LLM validation is just good validation](https://python.useinstructor.com/blog/2023/10/23/good-llm-validation-is-just-good-validation/).\n\nThis post will mostly describe how to use simple and more complex retry and logic.\n\n## Example of a Validator [¶](https://python.useinstructor.com/concepts/retrying/\\#example-of-a-validator \"Permanent link\")\n\nBefore we begin, we'll use a simple example of a validator. One that checks that the name is in all caps. While we could obviously prompt that we want the name in all caps, this serves as an example of how we can build in additional logic without changing our prompts.\n\n```md-code__content\nfrom typing import Annotated\nfrom pydantic import AfterValidator, BaseModel\n\ndef uppercase_validator(v):\n    if v.islower():\n        raise ValueError(\"Name must be ALL CAPS\")\n    return v\n\nclass UserDetail(BaseModel):\n    name: Annotated[str, AfterValidator(uppercase_validator)]\n    age: int\n\ntry:\n    UserDetail(name=\"jason\", age=12)\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must be ALL CAPS [type=value_error, input_value='jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n\n```\n\n## Simple: Max Retries [¶](https://python.useinstructor.com/concepts/retrying/\\#simple-max-retries \"Permanent link\")\n\nThe simplest way to set up retries is to assign an integer value to `max_retries`.\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\\\n    ],\n    max_retries=3,\n)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n\n```\n\n## Catching Retry Exceptions [¶](https://python.useinstructor.com/concepts/retrying/\\#catching-retry-exceptions \"Permanent link\")\n\nIf you want to catch the retry exceptions, you can do so and access the `last_completion`, `n_attempts` and `messages` attributes.\n\n```md-code__content\nfrom pydantic import BaseModel, field_validator\nimport openai\nimport instructor\nfrom instructor.exceptions import InstructorRetryException\nfrom tenacity import Retrying, retry_if_not_exception_type, stop_after_attempt\n\n# Patch the OpenAI client to enable response_model\nclient = instructor.from_openai(openai.OpenAI())\n\n# Define a Pydantic model for the user details\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"age\")\n    def validate_age(cls, v: int):\n        raise ValueError(f\"You will never succeed with {str(v)}\")\n\nretries = Retrying(\n    retry=retry_if_not_exception_type(ZeroDivisionError), stop=stop_after_attempt(3)\n)\n# Use the client to create a user detail\ntry:\n    user = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[{\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}],\n        max_retries=retries,\n    )\nexcept InstructorRetryException as e:\n    print(e.messages[-1][\"content\"])  # type: ignore\n    \"\"\"\n    Validation Error found:\n    1 validation error for UserDetail\n    age\n      Value error, You will never succeed with 25 [type=value_error, input_value=25, input_type=int]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    Recall the function correctly, fix the errors\n    \"\"\"\n\n    print(e.n_attempts)\n    #> 3\n\n    print(e.last_completion)\n    \"\"\"\n    ChatCompletion(\n        id='chatcmpl-AWl4B5JrGm7QSxBPQhx7lQH89WHxg',\n        choices=[\\\n            Choice(\\\n                finish_reason='stop',\\\n                index=0,\\\n                logprobs=None,\\\n                message=ChatCompletionMessage(\\\n                    content=None,\\\n                    refusal=None,\\\n                    role='assistant',\\\n                    audio=None,\\\n                    function_call=None,\\\n                    tool_calls=[\\\n                        ChatCompletionMessageToolCall(\\\n                            id='call_LGFqmLGaMvlkriHANf2nqLus',\\\n                            function=Function(\\\n                                arguments='{\"name\":\"Jason\",\"age\":25}', name='UserDetail'\\\n                            ),\\\n                            type='function',\\\n                        )\\\n                    ],\\\n                ),\\\n            )\\\n        ],\n        created=1732370783,\n        model='gpt-3.5-turbo-0125',\n        object='chat.completion',\n        service_tier=None,\n        system_fingerprint=None,\n        usage=CompletionUsage(\n            completion_tokens=27,\n            prompt_tokens=522,\n            total_tokens=549,\n            completion_tokens_details=CompletionTokensDetails(\n                audio_tokens=0, reasoning_tokens=0\n            ),\n            prompt_tokens_details=None,\n            prompt_token_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0),\n        ),\n    )\n    \"\"\"\n\n```\n\n## Advanced: Retry Logic [¶](https://python.useinstructor.com/concepts/retrying/\\#advanced-retry-logic \"Permanent link\")\n\nIf you want more control over how we define retries such as back-offs and additional retry logic we can use a library called Tenacity. To learn more, check out the documentation on the [Tenacity](https://tenacity.readthedocs.io/en/latest/) website.\n\nRather than using the decorator `@retry`, we can use the `Retrying` and `AsyncRetrying` classes to define our own retry logic.\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel\nfrom tenacity import Retrying, stop_after_attempt, wait_fixed\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\\\n    ],\n    max_retries=Retrying(\n        stop=stop_after_attempt(2),\n        wait=wait_fixed(1),\n    ),\n)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n\n```\n\n### asynchronous retries [¶](https://python.useinstructor.com/concepts/retrying/\\#asynchronous-retries \"Permanent link\")\n\nIf you're using asynchronous code, you can use `AsyncRetrying` instead.\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel\nfrom tenacity import AsyncRetrying, stop_after_attempt, wait_fixed\n\nclient = instructor.from_openai(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\ntask = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\\\n    ],\n    max_retries=AsyncRetrying(\n        stop=stop_after_attempt(2),\n        wait=wait_fixed(1),\n    ),\n)\n\nimport asyncio\n\nresponse = asyncio.run(task)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n\n```\n\n## Other Features of Tenacity [¶](https://python.useinstructor.com/concepts/retrying/\\#other-features-of-tenacity \"Permanent link\")\n\nTenacity features a huge number of different retrying capabilities. A few of them are listed below.\n\n- `Retrying(stop=stop_after_attempt(2))`: Stop after 2 attempts\n- `Retrying(stop=stop_after_delay(10))`: Stop after 10 seconds\n- `Retrying(wait=wait_fixed(1))`: Wait 1 second between each attempt\n- `Retrying(wait=wait_random(0, 1))`: Wait a random amount of time between 0 and 1 seconds\n- `Retrying(wait=wait_exponential(multiplier=1, min=4, max=10))`: Wait an exponential amount of time between 4 and 10 seconds\n- `Retrying(wait=(stop_after_attempt(2) | stop_after_delay(10)))`: Stop after 2 attempts or 10 seconds\n- `Retrying(wait=(wait_fixed(1) + wait_random(0.2)))`: Wait at least 1 second and add up to 0.2 seconds\n\nRemember that for async clients you need to use `AsyncRetrying` instead of `Retrying`!\n\n## Retry Callbacks [¶](https://python.useinstructor.com/concepts/retrying/\\#retry-callbacks \"Permanent link\")\n\nYou can also define callbacks to be called before and after each attempt. This is useful for logging or debugging.\n\n```md-code__content\nfrom pydantic import BaseModel, field_validator\nimport instructor\nimport tenacity\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    def name_is_uppercase(cls, v: str):\n        assert v.isupper(), \"Name must be uppercase\"\n        return v\n\nresp = client.messages.create(\n    model=\"gpt-3.5-turbo\",\n    max_tokens=1024,\n    max_retries=tenacity.Retrying(\n        stop=tenacity.stop_after_attempt(3),\n        before=lambda _: print(\"before:\", _),\n\"\"\"\nbefore:\n<RetryCallState 5565220688: attempt #1; slept for 0.0; last result: none yet>\n\"\"\"\n        after=lambda _: print(\"after:\", _),\n    ),  # type: ignore\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Extract John is 18 years old.\",\\\n        }\\\n    ],\n    response_model=User,\n)\n\nassert isinstance(resp, User)\nassert resp.name == \"JOHN\"  # due to validation\nassert resp.age == 18\nprint(resp)\n#> name='JOHN' age=18\n\n\"\"\"\nbefore: <RetryCallState 4421908816: attempt #1; slept for 0.0; last result: none yet>\nafter: <RetryCallState 4421908816: attempt #1; slept for 0.0; last result: failed (ValidationError 1 validation error for User\nname\n  Assertion failed, Name must be uppercase [type=assertion_error, input_value='John', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error)>\n\nbefore: <RetryCallState 4421908816: attempt #2; slept for 0.0; last result: none yet>\nname='JOHN' age=18\n\"\"\"\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/retrying/",
      "ogUrl": "https://python.useinstructor.com/concepts/retrying/",
      "title": "Implementing Effective Retry Logic with Pydantic and Tenacity - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/retrying/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/retrying.png",
      "ogTitle": "Implementing Effective Retry Logic with Pydantic and Tenacity - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/retrying.png",
      "og:title": "Implementing Effective Retry Logic with Pydantic and Tenacity - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/retrying/",
      "statusCode": 200,
      "description": "Learn how to establish simple and advanced retry strategies in Python using Pydantic and Tenacity for robust application behavior.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to establish simple and advanced retry strategies in Python using Pydantic and Tenacity for robust application behavior.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/retrying.png",
      "twitter:title": "Implementing Effective Retry Logic with Pydantic and Tenacity - Instructor",
      "og:description": "Learn how to establish simple and advanced retry strategies in Python using Pydantic and Tenacity for robust application behavior.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to establish simple and advanced retry strategies in Python using Pydantic and Tenacity for robust application behavior."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/partial/#streaming-partial-responses)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/partial.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/partial.md \"View source of this page\")\n\n# Streaming Partial Responses [¶](https://python.useinstructor.com/concepts/partial/\\#streaming-partial-responses \"Permanent link\")\n\nLiteral\n\nIf the data structure you're using has literal values, you need to make sure to import the `PartialLiteralMixin` mixin.\n\n```md-code__content\nfrom instructor.dsl.partial import PartialLiteralMixin\n\nclass User(BaseModel, PartialLiteralMixin):\n    name: str\n    age: int\n    category: Literal[\"admin\", \"user\", \"guest\"]\n\n// The rest of your code below\n\n```\n\nThis is because `jiter` throws an error otherwise if it encounters a incomplete Literal value while it's being streamed in\n\nField level streaming provides incremental snapshots of the current state of the response model that are immediately useable. This approach is particularly relevant in contexts like rendering UI components.\n\nInstructor supports this pattern by making use of `create_partial`. This lets us dynamically create a new class that treats all of the original model's fields as `Optional`.\n\n## Understanding Partial Responses [¶](https://python.useinstructor.com/concepts/partial/\\#understanding-partial-responses \"Permanent link\")\n\nConsider what happens whene we define a response model:\n\n```md-code__content\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n```\n\nIf we streamed json out from OpenAI, we would only be able to parse when the object is completed returned!\n\n```md-code__content\n{\"name\": \"Jo\n{\"name\": \"John\", \"ag\n{\"name\": \"John\", \"age\":\n{\"name\": \"John\", \"age\": 25} # Completed\n\n```\n\nWhen specifying a `create_partial` and setting `stream=True`, the response from `instructor` becomes a `Generator[T]`. As the generator yields results, you can iterate over these incremental updates. The last value yielded by the generator represents the completed extraction!\n\n```md-code__content\n{\"name\": \"Jo                 => User(name=\"Jo\", age=None)\n{\"name\": \"John\", \"ag         => User(name=\"John\", age=None)\n{\"name\": \"John\", \"age:       => User(name=\"John\", age=None)\n{\"name\": \"John\", \"age\": 25}  => User(name=\"John\", age=25)\n\n```\n\nLimited Validator Support\n\nDue to the streaming nature of the response model, we do not support validators since they would not be able to be applied to the streaming response.\n\nLet's look at an example of streaming an extraction of conference information, that would be used to stream in an react component.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import List\nfrom rich.console import Console\n\nclient = instructor.from_openai(OpenAI())\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss the upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024, at the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher, will be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities. Each participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meetingis scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\nextraction_stream = client.chat.completions.create_partial(\n    model=\"gpt-4\",\n    response_model=MeetingInfo,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\\\n        },\\\n    ],\n    stream=True,\n)\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n\nprint(extraction.model_dump_json(indent=2))\n\"\"\"\n{\n  \"users\": [\\\n    {\\\n      \"name\": \"John Doe\",\\\n      \"email\": \"johndoe@email.com\",\\\n      \"twitter\": \"@TechGuru44\"\\\n    },\\\n    {\\\n      \"name\": \"Jane Smith\",\\\n      \"email\": \"janesmith@email.com\",\\\n      \"twitter\": \"@DigitalDiva88\"\\\n    },\\\n    {\\\n      \"name\": \"Alex Johnson\",\\\n      \"email\": \"alexj@email.com\",\\\n      \"twitter\": \"@CodeMaster2023\"\\\n    }\\\n  ],\n  \"date\": \"2024-03-15\",\n  \"location\": \"Grand Tech Arena located at 4521 Innovation Drive\",\n  \"budget\": 50000,\n  \"deadline\": \"2024-02-20\"\n}\n\"\"\"\n\n```\n\nThis will output the following:\n\n![Partial Streaming Gif](https://python.useinstructor.com/img/partial.gif)\n\n## Asynchronous Streaming [¶](https://python.useinstructor.com/concepts/partial/\\#asynchronous-streaming \"Permanent link\")\n\nI also just want to call out in this example that `instructor` also supports asynchronous streaming. This is useful when you want to stream a response model and process the results as they come in, but you'll need to use the `async for` syntax to iterate over the results.\n\n```md-code__content\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(AsyncOpenAI())\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nasync def print_partial_results():\n    user = client.chat.completions.create_partial(\n        model=\"gpt-4-turbo-preview\",\n        response_model=User,\n        max_retries=2,\n        stream=True,\n        messages=[\\\n            {\"role\": \"user\", \"content\": \"Jason is 12 years old\"},\\\n        ],\n    )\n    async for m in user:\n        print(m)\n        #> name=None age=None\n        #> name=None age=None\n        #> name=None age=None\n        #> name=None age=12\n        #> name=None age=12\n        #> name=None age=12\n        #> name='' age=12\n        #> name='Jason' age=12\n        #> name='Jason' age=12\n\nimport asyncio\n\nasyncio.run(print_partial_results())\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/partial/",
      "ogUrl": "https://python.useinstructor.com/concepts/partial/",
      "title": "Streaming Partial Responses with Instructor and OpenAI - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/partial/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/partial.png",
      "ogTitle": "Streaming Partial Responses with Instructor and OpenAI - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/partial.png",
      "og:title": "Streaming Partial Responses with Instructor and OpenAI - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/partial/",
      "statusCode": 200,
      "description": "Learn to utilize field-level streaming with Instructor and OpenAI for incremental responses in Python.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn to utilize field-level streaming with Instructor and OpenAI for incremental responses in Python.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/partial.png",
      "twitter:title": "Streaming Partial Responses with Instructor and OpenAI - Instructor",
      "og:description": "Learn to utilize field-level streaming with Instructor and OpenAI for incremental responses in Python.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn to utilize field-level streaming with Instructor and OpenAI for incremental responses in Python."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/lists/#multi-task-and-streaming)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/lists.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/lists.md \"View source of this page\")\n\n# Multi-task and Streaming [¶](https://python.useinstructor.com/concepts/lists/\\#multi-task-and-streaming \"Permanent link\")\n\nA common use case of structured extraction is defining a single schema class and then making another schema to create a list to do multiple extraction\n\n```md-code__content\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nclass Users(BaseModel):\n    users: List[User]\n\nprint(Users.model_json_schema())\n\"\"\"\n{\n    '$defs': {\n        'User': {\n            'properties': {\n                'name': {'title': 'Name', 'type': 'string'},\n                'age': {'title': 'Age', 'type': 'integer'},\n            },\n            'required': ['name', 'age'],\n            'title': 'User',\n            'type': 'object',\n        }\n    },\n    'properties': {\n        'users': {'items': {'$ref': '#/$defs/User'}, 'title': 'Users', 'type': 'array'}\n    },\n    'required': ['users'],\n    'title': 'Users',\n    'type': 'object',\n}\n\"\"\"\n\n```\n\nDefining a task and creating a list of classes is a common enough pattern that we make this convenient by making use of `Iterable[T]`. This lets us dynamically create a new class that:\n\n1. Has dynamic docstrings and class name based on the task\n2. Support streaming by collecting tokens until a task is received back out.\n\n## Extracting Tasks using Iterable [¶](https://python.useinstructor.com/concepts/lists/\\#extracting-tasks-using-iterable \"Permanent link\")\n\nBy using `Iterable` you get a very convenient class with prompts and names automatically defined:\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nusers = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[User],\n    stream=False,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Consider this data: Jason is 10 and John is 30.\\\\\n                         Correctly segment it into entitites\\\\\n                        Make sure the JSON is correct\",\\\n        },\\\n    ],\n)\nfor user in users:\n    print(user)\n    #> name='Jason' age=10\n    #> name='John' age=30\n\n```\n\n## Streaming Tasks [¶](https://python.useinstructor.com/concepts/lists/\\#streaming-tasks \"Permanent link\")\n\nWe can also generate tasks as the tokens are streamed in by defining an `Iterable[T]` type.\n\nLets look at an example in action with the same class\n\n```md-code__content\nimport instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nusers = client.chat.completions.create(\n    model=\"gpt-4\",\n    temperature=0.1,\n    stream=True,\n    response_model=Iterable[User],\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"You are a perfect entity extraction system\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": (f\"Extract `Jason is 10 and John is 10`\"),\\\n        },\\\n    ],\n    max_tokens=1000,\n)\n\nfor user in users:\n    print(user)\n    #> name='Jason' age=10\n    #> name='John' age=10\n\n```\n\n## Asynchronous Streaming [¶](https://python.useinstructor.com/concepts/lists/\\#asynchronous-streaming \"Permanent link\")\n\nI also just want to call out in this example that `instructor` also supports asynchronous streaming. This is useful when you want to stream a response model and process the results as they come in, but you'll need to use the `async for` syntax to iterate over the results.\n\n```md-code__content\nimport instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\nasync def print_iterable_results():\n    model = await client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=Iterable[UserExtract],\n        max_retries=2,\n        stream=True,\n        messages=[\\\n            {\"role\": \"user\", \"content\": \"Make two up people\"},\\\n        ],\n    )\n    async for m in model:\n        print(m)\n        #> name='John Doe' age=34\n        #> name='Jane Smith' age=27\n\nimport asyncio\n\nasyncio.run(print_iterable_results())\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/lists/",
      "ogUrl": "https://python.useinstructor.com/concepts/lists/",
      "title": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/lists/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/lists.png",
      "ogTitle": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/lists.png",
      "og:title": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/lists/",
      "statusCode": 200,
      "description": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/lists.png",
      "twitter:title": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "og:description": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/prompting/#general-tips-for-prompt-engineering)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/prompting.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/prompting.md \"View source of this page\")\n\n# General Tips for Prompt Engineering [¶](https://python.useinstructor.com/concepts/prompting/\\#general-tips-for-prompt-engineering \"Permanent link\")\n\nThe overarching theme of using Instructor and Pydantic for function calling is to make the models as self-descriptive, modular, and flexible as possible, while maintaining data integrity and ease of use.\n\n- **Modularity**: Design self-contained components for reuse.\n- **Self-Description**: Use Pydantic's `Field` for clear field descriptions.\n- **Optionality**: Use Python's `Optional` type for nullable fields and set sensible defaults.\n- **Standardization**: Employ enumerations for fields with a fixed set of values; include a fallback option.\n- **Dynamic Data**: Use key-value pairs for arbitrary properties and limit list lengths.\n- **Entity Relationships**: Define explicit identifiers and relationship fields.\n- **Contextual Logic**: Optionally add a \"chain of thought\" field in reusable components for extra context.\n\n## Modular Chain of Thought [¶](https://python.useinstructor.com/concepts/prompting/\\#chain-of-thought \"Permanent link\")\n\nThis approach to \"chain of thought\" improves data quality but can have modular components rather than global CoT.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\n\nclass Role(BaseModel):\n    chain_of_thought: str = Field(\n        ..., description=\"Think step by step to determine the correct title\"\n    )\n    title: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n\n```\n\n## Utilize Optional Attributes [¶](https://python.useinstructor.com/concepts/prompting/\\#utilize-optional-attributes \"Permanent link\")\n\nUse Python's Optional type and set a default value to prevent undesired defaults like empty strings.\n\n```md-code__content\nfrom typing import Optional\nfrom pydantic import BaseModel, Field\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\n```\n\n## Handling Errors Within Function Calls [¶](https://python.useinstructor.com/concepts/prompting/\\#handling-errors-within-function-calls \"Permanent link\")\n\nYou can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None\n\n```\n\nWith the `MaybeUser` class, you can either receive a `UserDetail` object in result or get an error message in message.\n\n### Simplification with the Maybe Pattern [¶](https://python.useinstructor.com/concepts/prompting/\\#simplification-with-the-maybe-pattern \"Permanent link\")\n\nYou can further simplify this using instructor to create the `Maybe` pattern dynamically from any `BaseModel`.\n\n```md-code__content\nimport instructor\nfrom pydantic import BaseModel\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n\nMaybeUser = instructor.Maybe(UserDetail)\n\n```\n\nThis allows you to quickly create a Maybe type for any class, streamlining the process.\n\n## Tips for Enumerations [¶](https://python.useinstructor.com/concepts/prompting/\\#tips-for-enumerations \"Permanent link\")\n\nTo prevent data misalignment, use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.\n\n```md-code__content\nfrom enum import Enum, auto\nfrom pydantic import BaseModel, Field\n\nclass Role(Enum):\n    PRINCIPAL = auto()\n    TEACHER = auto()\n    STUDENT = auto()\n    OTHER = auto()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(\n        description=\"Correctly assign one of the predefined roles to the user.\"\n    )\n\n```\n\n## Literals [¶](https://python.useinstructor.com/concepts/prompting/\\#literals \"Permanent link\")\n\nIf you're having a hard time with `Enum` an alternative is to use `Literal`\n\n```md-code__content\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n\n```\n\nIf you'd like to improve performance more you can reiterate the requirements in the field descriptions or in the docstrings.\n\n## Reiterate Long Instructions [¶](https://python.useinstructor.com/concepts/prompting/\\#reiterate-long-instructions \"Permanent link\")\n\nFor complex attributes, it helps to reiterate the instructions in the field's description.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\n\nclass Role(BaseModel):\n    \"\"\"\n    Extract the role based on the following rules ...\n    \"\"\"\n\n    instructions: str = Field(\n        ...,\n        description=\"Restate the instructions and rules to correctly determine the title.\",\n    )\n    title: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n\n```\n\n## Handle Arbitrary Properties [¶](https://python.useinstructor.com/concepts/prompting/\\#handle-arbitrary-properties \"Permanent link\")\n\nWhen you need to extract undefined attributes, use a list of key-value pairs.\n\n```md-code__content\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\nclass Property(BaseModel):\n    key: str\n    value: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(\n        ..., description=\"Extract any other properties that might be relevant.\"\n    )\n\n```\n\n## Limiting the Length of Lists [¶](https://python.useinstructor.com/concepts/prompting/\\#limiting-the-length-of-lists \"Permanent link\")\n\nWhen dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.\n\n```md-code__content\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\nclass Property(BaseModel):\n    index: str = Field(..., description=\"Monotonically increasing ID\")\n    key: str\n    value: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be less than 6\",\n    )\n\n```\n\n**Using Tuples for Simple Types**\n\nFor simple types, tuples can be a more compact alternative to custom classes, especially when the properties don't require additional descriptions.\n\n```md-code__content\nfrom typing import List, Tuple\nfrom pydantic import BaseModel, Field\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Tuple[int, str]] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be less than 6\",\n    )\n\n```\n\n## Advanced Arbitrary Properties [¶](https://python.useinstructor.com/concepts/prompting/\\#advanced-arbitrary-properties \"Permanent link\")\n\nFor multiple users, aim to use consistent key names when extracting properties.\n\n```md-code__content\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n\nclass UserDetails(BaseModel):\n    \"\"\"\n    Extract information for multiple users.\n    Use consistent key names for properties across users.\n    \"\"\"\n\n    users: List[UserDetail]\n\n```\n\nThis refined guide should offer a cleaner and more organized approach to structure engineering in Python.\n\n## Defining Relationships Between Entities [¶](https://python.useinstructor.com/concepts/prompting/\\#defining-relationships-between-entities \"Permanent link\")\n\nIn cases where relationships exist between entities, it's vital to define them explicitly in the model. The following example demonstrates how to define relationships between users by incorporating an id and a friends field:\n\n```md-code__content\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    friends: List[int] = Field(\n        ...,\n        description=\"Correct and complete list of friend IDs, representing relationships between users.\",\n    )\n\nclass UserRelationships(BaseModel):\n    users: List[UserDetail] = Field(\n        ...,\n        description=\"Collection of users, correctly capturing the relationships among them.\",\n    )\n\n```\n\n## Reusing Components with Different Contexts [¶](https://python.useinstructor.com/concepts/prompting/\\#reusing-components-with-different-contexts \"Permanent link\")\n\nYou can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both work\\_time and leisure\\_time.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\n\nclass TimeRange(BaseModel):\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    work_time: TimeRange = Field(\n        ..., description=\"Time range during which the user is working.\"\n    )\n    leisure_time: TimeRange = Field(\n        ..., description=\"Time range reserved for leisure activities.\"\n    )\n\n```\n\nSometimes, a component like TimeRange may require some context or additional logic to be used effectively. Employing a \"chain of thought\" field within the component can help in understanding or optimizing the time range allocations.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\n\nclass TimeRange(BaseModel):\n    chain_of_thought: str = Field(\n        ..., description=\"Step by step reasoning to get the correct time range\"\n    )\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/prompting/",
      "ogUrl": "https://python.useinstructor.com/concepts/prompting/",
      "title": "Effective Prompt Engineering with Pydantic and Instructor - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/prompting/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/prompting.png",
      "ogTitle": "Effective Prompt Engineering with Pydantic and Instructor - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/prompting.png",
      "og:title": "Effective Prompt Engineering with Pydantic and Instructor - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/prompting/",
      "statusCode": 200,
      "description": "Discover best practices for prompt engineering using Pydantic and Instructor to enhance modularity, flexibility, and data integrity.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Discover best practices for prompt engineering using Pydantic and Instructor to enhance modularity, flexibility, and data integrity.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/prompting.png",
      "twitter:title": "Effective Prompt Engineering with Pydantic and Instructor - Instructor",
      "og:description": "Discover best practices for prompt engineering using Pydantic and Instructor to enhance modularity, flexibility, and data integrity.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Discover best practices for prompt engineering using Pydantic and Instructor to enhance modularity, flexibility, and data integrity."
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/typeadapter.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/typeadapter.md \"View source of this page\")\n\n# Type Adapter\n\nThis page is a work in progress\n\nThis page is a work in progress. Check out [Pydantic's documentation](https://docs.pydantic.dev/latest/concepts/type_adapter/)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/typeadapter/",
      "ogUrl": "https://python.useinstructor.com/concepts/typeadapter/",
      "title": "Pydantic Type Adapter Overview - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/typeadapter/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/typeadapter.png",
      "ogTitle": "Pydantic Type Adapter Overview - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/typeadapter.png",
      "og:title": "Pydantic Type Adapter Overview - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/typeadapter/",
      "statusCode": 200,
      "description": "Explore the ongoing updates of Pydantic's Type Adapter concepts and access the official documentation.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Explore the ongoing updates of Pydantic's Type Adapter concepts and access the official documentation.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/typeadapter.png",
      "twitter:title": "Pydantic Type Adapter Overview - Instructor",
      "og:description": "Explore the ongoing updates of Pydantic's Type Adapter concepts and access the official documentation.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Explore the ongoing updates of Pydantic's Type Adapter concepts and access the official documentation."
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/enums.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/enums.md \"View source of this page\")\n\n# Enums\n\nTo prevent data misalignment, we can use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\nfrom enum import Enum\n\nclass Role(Enum):\n    PRINCIPAL = \"PRINCIPAL\"\n    TEACHER = \"TEACHER\"\n    STUDENT = \"STUDENT\"\n    OTHER = \"OTHER\"\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(\n        description=\"Correctly assign one of the predefined roles to the user.\"\n    )\n\n```\n\nIf you're having a hard time with `Enum` an alternative is to use `Literal` instead.\n\n```md-code__content\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/enums/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/enums/",
      "title": "Using Enums and Literals in Pydantic for Role Management - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/enums/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/enums.png",
      "ogTitle": "Using Enums and Literals in Pydantic for Role Management - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/enums.png",
      "og:title": "Using Enums and Literals in Pydantic for Role Management - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/enums/?q=",
      "statusCode": 200,
      "description": "Learn how to implement Enums and Literals in Pydantic to manage standardized user roles with a fallback option.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to implement Enums and Literals in Pydantic to manage standardized user roles with a fallback option.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/enums.png",
      "twitter:title": "Using Enums and Literals in Pydantic for Role Management - Instructor",
      "og:description": "Learn how to implement Enums and Literals in Pydantic to manage standardized user roles with a fallback option.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to implement Enums and Literals in Pydantic to manage standardized user roles with a fallback option."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/hooks/?q=#hooks)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/hooks.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/hooks.md \"View source of this page\")\n\n# Hooks [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#hooks \"Permanent link\")\n\nHooks provide a powerful mechanism for intercepting and handling events during the completion and parsing process in the Instructor library. They allow you to add custom behavior, logging, or error handling at various stages of the API interaction.\n\n## Overview [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#overview \"Permanent link\")\n\nThe Hooks system in Instructor is based on the `Hooks` class, which manages event registration and emission. It supports several predefined events that correspond to different stages of the completion and parsing process.\n\n## Supported Hook Events [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#supported-hook-events \"Permanent link\")\n\n### `completion:kwargs` [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#completionkwargs \"Permanent link\")\n\nThis hook is emitted when completion arguments are provided. It receives all arguments passed to the completion function. These will contain the `model`, `messages`, `tools`, AFTER any `response_model` or `validation_context` parameters have been converted to their respective values.\n\n```md-code__content\ndef handler(*args, **kwargs) -> None: ...\n\n```\n\n### `completion:response` [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#completionresponse \"Permanent link\")\n\nThis hook is emitted when a completion response is received. It receives the raw response object from the completion API.\n\n```md-code__content\ndef handler(response) -> None: ...\n\n```\n\n### `completion:error` [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#completionerror \"Permanent link\")\n\nThis hook is emitted when an error occurs during completion before any retries are attempted and the response is parsed as a pydantic model.\n\n```md-code__content\ndef handler(error) -> None: ...\n\n```\n\n### `parse:error` [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#parseerror \"Permanent link\")\n\nThis hook is emitted when an error occurs during parsing of the response as a pydantic model. This can happen if the response is not valid or if the pydantic model is not compatible with the response.\n\n```md-code__content\ndef handler(error) -> None: ...\n\n```\n\n### `completion:last_attempt` [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#completionlast_attempt \"Permanent link\")\n\nThis hook is emitted when the last retry attempt is made.\n\n```md-code__content\ndef handler(error) -> None: ...\n\n```\n\n## Implementation Details [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#implementation-details \"Permanent link\")\n\nThe Hooks system is implemented in the `instructor/hooks.py` file. The `Hooks` class handles the registration and emission of hook events. You can refer to this file to see how hooks work under the hood. The retry logic that uses Hooks is implemented in the `instructor/retry.py` file. This shows how Hooks are used when trying again after errors during completions.\n\n### Registering Hooks [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#registering-hooks \"Permanent link\")\n\nYou can register hooks using the `on` method of the Instructor client or a `Hooks` instance. Here's an example:\n\n```md-code__content\nimport instructor\nimport openai\nimport pprint\n\nclient = instructor.from_openai(openai.OpenAI())\n\ndef log_completion_kwargs(*args, **kwargs):\n    pprint.pprint({\"args\": args, \"kwargs\": kwargs})\n\nclient.on(\"completion:kwargs\", log_completion_kwargs)\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n    response_model=str,\n)\nprint(resp)\n#> Hello, world!\n\n```\n\n### Emitting Events [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#emitting-events \"Permanent link\")\n\nEvents are automatically emitted by the Instructor library at appropriate times. You don't need to manually emit events in most cases.\n\n### Removing Hooks [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#removing-hooks \"Permanent link\")\n\nYou can remove a specific hook using the `off` method:\n\n```md-code__content\nclient.off(\"completion:kwargs\", log_completion_kwargs)\n\n```\n\n### Clearing Hooks [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#clearing-hooks \"Permanent link\")\n\nTo remove all hooks for a specific event or all events:\n\n```md-code__content\n# Clear hooks for a specific event\nclient.clear(\"completion:kwargs\")\n\n# Clear all hooks\nclient.clear()\n\n```\n\n## Example: Logging and Debugging [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#example-logging-and-debugging \"Permanent link\")\n\nHere's a comprehensive example demonstrating how to use hooks for logging and debugging:\n\n```md-code__content\nimport instructor\nimport openai\nimport pydantic\n\ndef log_completion_kwargs(kwargs) -> None:\n    print(\"## Completion kwargs:\")\n    print(kwargs)\n    \"\"\"\n    {\n        \"messages\": [\\\n            {\\\n                \"role\": \"user\",\\\n                \"content\": \"Extract the user name and age from the following text: 'John is 20 years old'\",\\\n            }\\\n        ],\n        \"model\": \"gpt-4o-mini\",\n        \"tools\": [\\\n            {\\\n                \"type\": \"function\",\\\n                \"function\": {\\\n                    \"name\": \"User\",\\\n                    \"description\": \"Correctly extracted `User` with all the required parameters with correct types\",\\\n                    \"parameters\": {\\\n                        \"properties\": {\\\n                            \"name\": {\"title\": \"Name\", \"type\": \"string\"},\\\n                            \"age\": {\"title\": \"Age\", \"type\": \"integer\"},\\\n                        },\\\n                        \"required\": [\"age\", \"name\"],\\\n                        \"type\": \"object\",\\\n                    },\\\n                },\\\n            }\\\n        ],\n        \"tool_choice\": {\"type\": \"function\", \"function\": {\"name\": \"User\"}},\n    }\n    \"\"\"\n\ndef log_completion_response(response) -> None:\n    print(\"## Completion response:\")\n    #> ## Completion response:\n    \"\"\"\n    {\n        'id': 'chatcmpl-AWl4Mj5Jrv7m7JkOTIiHXSldQIOFm',\n        'choices': [\\\n            {\\\n                'finish_reason': 'stop',\\\n                'index': 0,\\\n                'logprobs': None,\\\n                'message': {\\\n                    'content': None,\\\n                    'refusal': None,\\\n                    'role': 'assistant',\\\n                    'audio': None,\\\n                    'function_call': None,\\\n                    'tool_calls': [\\\n                        {\\\n                            'id': 'call_6oQ9WXxeSiVEV71B9IYtsbIE',\\\n                            'function': {\\\n                                'arguments': '{\"name\":\"John\",\"age\":-1}',\\\n                                'name': 'User',\\\n                            },\\\n                            'type': 'function',\\\n                        }\\\n                    ],\\\n                },\\\n            }\\\n        ],\n        'created': 1732370794,\n        'model': 'gpt-4o-mini-2024-07-18',\n        'object': 'chat.completion',\n        'service_tier': None,\n        'system_fingerprint': 'fp_0705bf87c0',\n        'usage': {\n            'completion_tokens': 10,\n            'prompt_tokens': 87,\n            'total_tokens': 97,\n            'completion_tokens_details': {\n                'audio_tokens': 0,\n                'reasoning_tokens': 0,\n                'accepted_prediction_tokens': 0,\n                'rejected_prediction_tokens': 0,\n            },\n            'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n        },\n    }\n    \"\"\"\n    print(response.model_dump())\n    \"\"\"\n    {\n        'id': 'chatcmpl-AWl4Mxdq0BUGRlVCA61z8YOIVga7F',\n        'choices': [\\\n            {\\\n                'finish_reason': 'stop',\\\n                'index': 0,\\\n                'logprobs': None,\\\n                'message': {\\\n                    'content': None,\\\n                    'refusal': None,\\\n                    'role': 'assistant',\\\n                    'audio': None,\\\n                    'function_call': None,\\\n                    'tool_calls': [\\\n                        {\\\n                            'id': 'call_EJIEr27Mb6sdbplnYw4iBWlm',\\\n                            'function': {\\\n                                'arguments': '{\"name\":\"John\",\"age\":10}',\\\n                                'name': 'User',\\\n                            },\\\n                            'type': 'function',\\\n                        }\\\n                    ],\\\n                },\\\n            }\\\n        ],\n        'created': 1732370794,\n        'model': 'gpt-4o-mini-2024-07-18',\n        'object': 'chat.completion',\n        'service_tier': None,\n        'system_fingerprint': 'fp_0705bf87c0',\n        'usage': {\n            'completion_tokens': 9,\n            'prompt_tokens': 87,\n            'total_tokens': 96,\n            'completion_tokens_details': {\n                'audio_tokens': 0,\n                'reasoning_tokens': 0,\n                'accepted_prediction_tokens': 0,\n                'rejected_prediction_tokens': 0,\n            },\n            'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n        },\n    }\n    \"\"\"\n\ndef log_completion_error(error) -> None:\n    print(\"## Completion error:\")\n    print({\"error\": error})\n\ndef log_parse_error(error) -> None:\n    print(\"## Parse error:\")\n    #> ## Parse error:\n    print(error)\n    \"\"\"\n    1 validation error for User\n    age\n    Value error, Age cannot be negative [type=value_error, input_value=-10, input_type=int]\n        For further information visit https://errors.pydantic.dev/2.8/v/value_error\n    \"\"\"\n\n# Create an Instructor client\nclient = instructor.from_openai(openai.OpenAI())\n\nclient.on(\"completion:kwargs\", log_completion_kwargs)\nclient.on(\"completion:response\", log_completion_response)\n\nclient.on(\"completion:error\", log_completion_error)\nclient.on(\"parse:error\", log_parse_error)\n\n# Define a model with a validator\nclass User(pydantic.BaseModel):\n    name: str\n    age: int\n\n    @pydantic.field_validator(\"age\")\n    def check_age(cls, v: int) -> int:\n        if v < 0:\n            raise ValueError(\"Age cannot be negative\")\n        return v\n\ntry:\n    # Use the client to create a completion\n    user = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\\\n            {\\\n    #> ## Parse error:\\\n                \"role\": \"user\",\\\n                \"content\": \"Extract the user name and age from the following text: 'John is -1 years old'\",\\\n    \"\"\"\\\n    1 validation error for User\\\n    age\\\n      Value error, Age cannot be negative [type=value_error, input_value=-1, input_type=int]\\\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\\\n    \"\"\"\\\n            }\\\n        ],\n        response_model=User,\n        max_retries=1,\n    )\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\nuser = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Extract the user name and age from the following text: 'John is 10 years old'\",\\\n        }\\\n    ],\n    response_model=User,\n    max_retries=1,\n)\nprint(user)\n#> name='John' age=10\n    \"\"\"\n    Error: 1 validation error for User\n    age\n      Value error, Age cannot be negative [type=value_error, input_value=-1, input_type=int]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n\n```\n\nThis example demonstrates:\n\n1. Defining hook handlers for different events.\n2. Registering the hooks with the Instructor client.\n3. Using a Pydantic model with a validator.\n4. Making a completion request that will trigger various hooks.\n\nThe hooks will log information at different stages of the process, helping with debugging and understanding the flow of data.\n\n## Best Practices [¶](https://python.useinstructor.com/concepts/hooks/?q=\\#best-practices \"Permanent link\")\n\n1. **Error Handling**: Always include error handling in your hook handlers to prevent exceptions from breaking the main execution flow. We will automatically warn if an exception is raised in a hook handler.\n\n2. **Performance**: Keep hook handlers lightweight to avoid impacting the performance of the main application.\n\n3. **Modularity**: Use hooks to separate concerns. For example, use hooks for logging, monitoring, or custom business logic without cluttering the main code.\n\n4. **Consistency**: Use the same naming conventions and patterns across all your hooks for better maintainability.\n\n5. **Documentation**: Document the purpose and expected input/output of each hook handler for easier collaboration and maintenance.\n\n\nBy leveraging hooks effectively, you can create more flexible, debuggable, and maintainable applications when working with the Instructor library and language models.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/hooks/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/hooks/",
      "title": "Understanding Hooks in the Instructor Library - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/hooks/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/hooks.png",
      "ogTitle": "Understanding Hooks in the Instructor Library - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/hooks.png",
      "og:title": "Understanding Hooks in the Instructor Library - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/hooks/?q=",
      "statusCode": 200,
      "description": "Learn how to use hooks for event handling in the Instructor library to enhance logging, error handling, and custom behaviors.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to use hooks for event handling in the Instructor library to enhance logging, error handling, and custom behaviors.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/hooks.png",
      "twitter:title": "Understanding Hooks in the Instructor Library - Instructor",
      "og:description": "Learn how to use hooks for event handling in the Instructor library to enhance logging, error handling, and custom behaviors.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to use hooks for event handling in the Instructor library to enhance logging, error handling, and custom behaviors."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/templating/?q=#prompt-templating)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/templating.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/templating.md \"View source of this page\")\n\n# Prompt Templating [¶](https://python.useinstructor.com/concepts/templating/?q=\\#prompt-templating \"Permanent link\")\n\nWith Instructor's Jinja templating, you can:\n\n- Dynamically adapt prompts to any context\n- Easily manage and version your prompts better\n- Integrate seamlessly with validation processes\n- Handle sensitive information securely\n\nOur solution offers:\n\n- Separation of prompt structure and content\n- Complex logic implementation within prompts\n- Template reusability across scenarios\n- Enhanced prompt versioning and logging\n- Pydantic integration for validation and type safety\n\n## Context is available to the templating engine [¶](https://python.useinstructor.com/concepts/templating/?q=\\#context-is-available-to-the-templating-engine \"Permanent link\")\n\nThe `context` parameter is a dictionary that is passed to the templating engine. It is used to pass in the relevant variables to the templating engine. This single `context` parameter will be passed to jinja to render out the final prompt.\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nresp = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"\"\"Extract the information from the\\\n        following text: `{{ data }}`\"\"\",\\\n        },\\\n    ],\n    response_model=User,\n    context={\"data\": \"John Doe is thirty years old\"},\n)\n\nprint(resp)\n#> name='John Doe' age=30\n\n```\n\n### Context is available to Pydantic validators [¶](https://python.useinstructor.com/concepts/templating/?q=\\#context-is-available-to-pydantic-validators \"Permanent link\")\n\nIn this example, we demonstrate how to leverage the `context` parameter with Pydantic validators to enhance our validation and data processing capabilities. By passing the `context` to the validators, we can implement dynamic validation rules and data transformations based on the input context. This approach allows for flexible and context-aware validation, such as checking for banned words or applying redaction patterns to sensitive information.\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel, ValidationInfo, field_validator\nimport re\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass Response(BaseModel):\n    text: str\n\n    @field_validator('text')\n    @classmethod\n    def redact_regex(cls, v: str, info: ValidationInfo):\n        context = info.context\n        if context:\n            redact_patterns = context.get('redact_patterns', [])\n            for pattern in redact_patterns:\n                v = re.sub(pattern, '****', v)\n        return v\n\nresponse = client.create(\n    model=\"gpt-4o\",\n    response_model=Response,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"\"\"\\\n                Write about a {{ topic }}\\\n\\\n                {% if banned_words %}\\\n                You must not use the following banned words:\\\n\\\n                <banned_words>\\\n                {% for word in banned_words %}\\\n                * {{ word }}\\\n                {% endfor %}\\\n                </banned_words>\\\n                {% endif %}\\\n              \"\"\",\\\n        },\\\n    ],\n    context={\n        \"topic\": \"jason and now his phone number is 123-456-7890\",\n        \"redact_patterns\": [\\\n            r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",  # Phone number pattern\\\n            r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",  # SSN pattern\\\n        ],\n    },\n    max_retries=3,\n)\n\nprint(response.text)\n\"\"\"\nJason is a remarkable individual known for his generosity and lively spirit. In his community, he is always ready to lend a helping hand, whether it's participating in local events, volunteering for charitable causes, or simply being there for his friends and family. His warmth and friendliness make everyone around him feel welcome and appreciated.\n\nJason is an enthusiast of technology and innovation. He spends much of his free time exploring new gadgets and staying updated with the latest tech trends. His curiosity often leads him to experiment with different software and hardware, making him a go-to person for tech advice among his peers.\n\nIn his career, Jason is a dedicated professional, always striving to improve and excel in his field. His colleagues respect him for his work ethic and creativity, making him an invaluable team member.\n\nIn his personal life, Jason enjoys outdoor activities such as hiking and cycling. These adventures provide him with a sense of freedom and connection to nature, reflecting his adventurous personality.\n\nAs much as Jason values his privacy, he is also approachable and open-minded. This balance allows him to maintain meaningful connections without compromising his personal space.\n\nPlease note, sharing personal contact information like phone numbers on public platforms is discouraged to protect privacy. If you need to contact someone like Jason, it's best to do so through secured and private channels or have explicit consent from the individual involved.\n\"\"\"\n\n```\n\n1. Access the variables passed into the `context` variable inside your Pydantic validator\n\n2. Pass in the variables to be used for validation and/or rendering into the `context` parameter\n\n\n### Jinja Syntax [¶](https://python.useinstructor.com/concepts/templating/?q=\\#jinja-syntax \"Permanent link\")\n\nJinja is used to render the prompts, allowing the use of familiar Jinja syntax. This enables rendering of lists, conditionals, and more. It also allows calling functions and methods within Jinja.\n\nThis makes formatting of prompts and rendering logic extremely easy.\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass Citation(BaseModel):\n    source_ids: list[int]\n    text: str\n\nclass Response(BaseModel):\n    answer: list[Citation]\n\nresp = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"\"\"\\\n                You are a {{ role }} tasks with the following question\\\n\\\n                <question>\\\n                {{ question }}\\\n                </question>\\\n\\\n                Use the following context to answer the question, make sure to return [id] for every citation:\\\n\\\n                <context>\\\n                {% for chunk in context %}\\\n                  <context_chunk>\\\n                    <id>{{ chunk.id }}</id>\\\n                    <text>{{ chunk.text }}</text>\\\n                  </context_chunk>\\\n                {% endfor %}\\\n                </context>\\\n\\\n                {% if rules %}\\\n                Make sure to follow these rules:\\\n\\\n                {% for rule in rules %}\\\n                  * {{ rule }}\\\n                {% endfor %}\\\n                {% endif %}\\\n            \"\"\",\\\n        },\\\n    ],\n    response_model=Response,\n    context={\n        \"role\": \"professional educator\",\n        \"question\": \"What is the capital of France?\",\n        \"context\": [\\\n            {\"id\": 1, \"text\": \"Paris is the capital of France.\"},\\\n            {\"id\": 2, \"text\": \"France is a country in Europe.\"},\\\n        ],\n        \"rules\": [\"Use markdown.\"],\n    },\n)\n\nprint(resp)\n#> answer=[Citation(source_ids=[1], text='The capital of France is Paris.')]\n# answer=[Citation(source_ids=[1], text='The capital of France is Paris.')]\n\n```\n\n### Working with Secrets [¶](https://python.useinstructor.com/concepts/templating/?q=\\#working-with-secrets \"Permanent link\")\n\nYour prompts might need to include sensitive user information when they're sent to your model provider. This is probably something you don't want to hard code into your prompt or captured in your logs. An easy way to get around this is to use the `SecretStr` type from `Pydantic` in your model definitions.\n\n```md-code__content\nfrom pydantic import BaseModel, SecretStr\nimport instructor\nimport openai\n\nclass UserContext(BaseModel):\n    name: str\n    address: SecretStr\n\nclass Address(BaseModel):\n    street: SecretStr\n    city: str\n    state: str\n    zipcode: str\n\nclient = instructor.from_openai(openai.OpenAI())\ncontext = UserContext(name=\"scolvin\", address=\"secret address\")\n\naddress = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"{{ user.name }} is `{{ user.address.get_secret_value() }}`, normalize it to an address object\",\\\n        },\\\n    ],\n    context={\"user\": context},\n    response_model=Address,\n)\nprint(context)\n#> name='scolvin' address=SecretStr('**********')\nprint(address)\n#> street=SecretStr('**********') city='scolvin' state='' zipcode=''\n\n```\n\nThis allows you to preserve your sensitive information while still using it in your prompts.\n\n## Security [¶](https://python.useinstructor.com/concepts/templating/?q=\\#security \"Permanent link\")\n\nWe use the `jinja2.sandbox.SandboxedEnvironment` to prevent security issues with the templating engine. This means that you can't use arbitrary python code in your prompts. But this doesn't mean that you should pass untrusted input to the templating engine, as this could still be abused for things like Denial of Service attacks.\n\nYou should [always sanitize](https://jinja.palletsprojects.com/en/stable/sandbox/#security-considerations) any input that you pass to the templating engine.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/templating/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/templating/",
      "title": "Effective Prompt Templating with Jinja and Pydantic - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/templating/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/templating.png",
      "ogTitle": "Effective Prompt Templating with Jinja and Pydantic - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/templating.png",
      "og:title": "Effective Prompt Templating with Jinja and Pydantic - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/templating/?q=",
      "statusCode": 200,
      "description": "Learn to dynamically create prompts using Jinja templating and validate them with Pydantic for enhanced flexibility and security.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn to dynamically create prompts using Jinja templating and validate them with Pydantic for enhanced flexibility and security.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/templating.png",
      "twitter:title": "Effective Prompt Templating with Jinja and Pydantic - Instructor",
      "og:description": "Learn to dynamically create prompts using Jinja templating and validate them with Pydantic for enhanced flexibility and security.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn to dynamically create prompts using Jinja templating and validate them with Pydantic for enhanced flexibility and security."
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/logging.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/logging.md \"View source of this page\")\n\n# Logging\n\nIn order to see the requests made to OpenAI and the responses, you can set logging to DEBUG. This will show the requests and responses made to OpenAI. This can be useful for debugging and understanding the requests and responses made to OpenAI. I would love some contributions that make this a lot cleaner, but for now this is the fastest way to see the prompts.\n\n```md-code__content\nimport instructor\nimport openai\nimport logging\n\nfrom pydantic import BaseModel\n\n# Set logging to DEBUG\nlogging.basicConfig(level=logging.DEBUG)\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\\\n    ],\n)  # type: ignore\n\n\"\"\"\n...\nDEBUG:instructor:Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\nDEBUG:instructor:Instructor Request: mode.value='tool_call', response_model=<class '__main__.UserDetail'>, new_kwargs={'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Extract Jason is 25 years old'}], 'tools': [{'type': 'function', 'function': {'name': 'UserDetail', 'description': 'Correctly extracted `UserDetail` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'UserDetail'}}}\nDEBUG:instructor:max_retries: 1\n...\nDEBUG:instructor:Instructor Pre-Response: ChatCompletion(id='chatcmpl-8zBxMxsOqm5Sj6yeEI38PnU2r6ncC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_E1cftF5U0zEjzIbWt3q0ZLbN', function=Function(arguments='{\"name\":\"Jason\",\"age\":25}', name='UserDetail'), type='function')]))], created=1709594660, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_2b778c6b35', usage=CompletionUsage(completion_tokens=9, prompt_tokens=81, total_tokens=90))\nDEBUG:httpcore.connection:close.started\nDEBUG:httpcore.connection:close.complete\n\"\"\"\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/logging/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/logging/",
      "title": "Debugging OpenAI Requests with Python Logging - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/logging/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/logging.png",
      "ogTitle": "Debugging OpenAI Requests with Python Logging - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/logging.png",
      "og:title": "Debugging OpenAI Requests with Python Logging - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/logging/?q=",
      "statusCode": 200,
      "description": "Learn how to log OpenAI requests and responses in Python using DEBUG level logging for efficient debugging.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to log OpenAI requests and responses in Python using DEBUG level logging for efficient debugging.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/logging.png",
      "twitter:title": "Debugging OpenAI Requests with Python Logging - Instructor",
      "og:description": "Learn how to log OpenAI requests and responses in Python using DEBUG level logging for efficient debugging.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to log OpenAI requests and responses in Python using DEBUG level logging for efficient debugging."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/patching/?q=#patching)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/patching.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/patching.md \"View source of this page\")\n\n# Patching [¶](https://python.useinstructor.com/concepts/patching/?q=\\#patching \"Permanent link\")\n\nInstructor enhances client functionality with three new keywords for backwards compatibility. This allows use of the enhanced client as usual, with structured output benefits.\n\n- `response_model`: Defines the response type for `chat.completions.create`.\n- `max_retries`: Determines retry attempts for failed `chat.completions.create` validations.\n- `validation_context`: Provides extra context to the validation process.\n\nThe default mode is `instructor.Mode.TOOLS` which is the recommended mode for OpenAI clients. This mode is the most stable and is the most recommended for OpenAI clients. The other modes are for other clients and are not recommended for OpenAI clients.\n\n## Tool Calling [¶](https://python.useinstructor.com/concepts/patching/?q=\\#tool-calling \"Permanent link\")\n\nThis is the recommended method for OpenAI clients. It is the most stable as functions is being deprecated soon.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.TOOLS)\n\n```\n\n### Gemini Tool Calling [¶](https://python.useinstructor.com/concepts/patching/?q=\\#gemini-tool-calling \"Permanent link\")\n\nGemini supports tool calling for stuctured data extraction. Gemini tool calling requires `jsonref` to be installed.\n\nLimitations\n\nGemini tool calling comes with some known limitations:\n\n```\n- `strict` Pydantic validation can fail for integer/float and enum validations\n- Gemini tool calling is incompatible with Pydantic schema customizations such as examples due to API limitations and may result in errors\n- Gemini can sometimes call the wrong function name, resulting in malformed or invalid json\n- Gemini tool calling could fail with enum and literal field types\n\n```\n\n```md-code__content\nimport instructor\nimport google.generativeai as genai\n\nclient = instructor.from_gemini(\n    genai.GenerativeModel(), mode=instructor.Mode.GEMINI_TOOLS\n)\n\n```\n\n### Gemini Vertex AI Tool Callin [¶](https://python.useinstructor.com/concepts/patching/?q=\\#gemini-vertex-ai-tool-callin \"Permanent link\")\n\nThis method allows us to get structured output from Gemini via tool calling with the Vertex AI SDK.\n\n**Note:** Gemini Tool Calling is in preview and there are some limitations, you can learn more in the [Vertex AI examples notebook](https://python.useinstructor.com/integrations/vertex/).\n\n```md-code__content\nimport instructor\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nimport vertexai\n\nvertexai.init(project=\"vertexai-generative-models\")\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-1.5-pro-preview-0409\"),\n    mode=instructor.Mode.VERTEXAI_TOOLS,\n)\n\n```\n\n## Parallel Tool Calling [¶](https://python.useinstructor.com/concepts/patching/?q=\\#parallel-tool-calling \"Permanent link\")\n\nParallel tool calling is also an option but you must set `response_model` to be `Iterable[Union[...]]` types since we expect an array of results. Check out [Parallel Tool Calling](https://python.useinstructor.com/concepts/parallel/) for more information.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.PARALLEL_TOOLS)\n\n```\n\n## Function Calling [¶](https://python.useinstructor.com/concepts/patching/?q=\\#function-calling \"Permanent link\")\n\nNote that function calling is soon to be deprecated in favor of TOOL mode for OpenAI. But will still be supported for other clients.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.TOOLS)\n\n```\n\n## JSON Mode [¶](https://python.useinstructor.com/concepts/patching/?q=\\#json-mode \"Permanent link\")\n\nJSON mode uses OpenAI's JSON format for responses by setting `response_format={\"type\": \"json_object\"}` in the `chat.completions.create` method.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.JSON)\n\n```\n\nJSON mode is also required for [the Gemini Models via OpenAI's SDK](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/call-gemini-using-openai-library#client-setup).\n\n```md-code__content\npip install google-auth\n\n```\n\n```md-code__content\nimport google.auth\nimport google.auth.transport.requests\nimport instructor\nfrom openai import OpenAI\n\ncreds, project = google.auth.default()\nauth_req = google.auth.transport.requests.Request()\ncreds.refresh(auth_req)\n\n# Pass the Vertex endpoint and authentication to the OpenAI SDK\nPROJECT = 'PROJECT_ID'\nLOCATION = 'LOCATION'\n\nbase_url = f'https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT}/locations/{LOCATION}/endpoints/openapi'\nclient = instructor.from_openai(\n    OpenAI(base_url=base_url, api_key=creds.token), mode=instructor.Mode.JSON\n)\n\n```\n\n### Gemini JSON Mode [¶](https://python.useinstructor.com/concepts/patching/?q=\\#gemini-json-mode \"Permanent link\")\n\nThis mode uses Gemini's response mimetype field to generate a response in JSON format using the schema provided.\n\n```md-code__content\nimport instructor\nimport google.generativeai as genai\n\nclient = instructor.from_gemini(\n    genai.GenerativeModel(), mode=instructor.Mode.GEMINI_JSON\n)\n\n```\n\n## Markdown JSON Mode [¶](https://python.useinstructor.com/concepts/patching/?q=\\#markdown-json-mode \"Permanent link\")\n\nThis just asks for the response in JSON format, but it is not recommended, and may not be supported in the future, this is just left to support vision models and models provided by Databricks and will not give you the full benefits of instructor.\n\nExperimental\n\nThis is not recommended, and may not be supported in the future, this is just left to support vision models and models provided by Databricks.\n\nGeneral syntax:\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.MD_JSON)\n\n```\n\nDatabricks syntax:\n\n```md-code__content\nimport instructor\nimport os\nfrom openai import OpenAI\n\nDATABRICKS_TOKEN = os.environ.get(\"DATABRICKS_TOKEN\", \"\")\nDATABRICKS_HOST = os.environ.get(\"DATABRICKS_HOST\", \"\")\n\n# Assuming Databricks environment variables are set\nclient = instructor.from_openai(\n    OpenAI(\n        api_key=DATABRICKS_TOKEN,\n        base_url=f\"{DATABRICKS_HOST}/serving-endpoints\",\n    ),\n    mode=instructor.Mode.MD_JSON,\n)\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/patching/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/patching/",
      "title": "Enhancing OpenAI Client Functionality with New Keywords - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/patching/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/patching.png",
      "ogTitle": "Enhancing OpenAI Client Functionality with New Keywords - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/patching.png",
      "og:title": "Enhancing OpenAI Client Functionality with New Keywords - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/patching/?q=",
      "statusCode": 200,
      "description": "Learn how to use new keywords for backward compatibility in the OpenAI client, improving functionality and structured output.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to use new keywords for backward compatibility in the OpenAI client, improving functionality and structured output.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/patching.png",
      "twitter:title": "Enhancing OpenAI Client Functionality with New Keywords - Instructor",
      "og:description": "Learn how to use new keywords for backward compatibility in the OpenAI client, improving functionality and structured output.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to use new keywords for backward compatibility in the OpenAI client, improving functionality and structured output."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/union/?q=#unions-for-multiple-types)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/union.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/union.md \"View source of this page\")\n\n# Union\n\nPydantic models also support `Union` types, which are used to represent a value that can be one of several types.\n\nWhile many libraries support multiple function calls, and tool calls support multiple returns, the goal is to provide only one way to do things.\n\n## Unions for Multiple Types [¶](https://python.useinstructor.com/concepts/union/?q=\\#unions-for-multiple-types \"Permanent link\")\n\nYou can use `Union` types to write _agents_ that can dynamically choose actions - by choosing an output class. For example, in a search and lookup function, the LLM can determine whether to execute another search, lookup or other action.\n\n```md-code__content\nfrom pydantic import BaseModel\nfrom typing import Union\n\nclass Search(BaseModel):\n    query: str\n\n    def execute(self):\n        return ...\n\nclass Lookup(BaseModel):\n    key: str\n\n    def execute(self):\n        return ...\n\nclass Action(BaseModel):\n    action: Union[Search, Lookup]\n\n    def execute(self):\n        return self.action.execute()\n\n```\n\nSee 'examples/union/run.py' for a working example.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/union/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/union/",
      "title": "Using Union Types in Pydantic Models - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/union/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/union.png",
      "ogTitle": "Using Union Types in Pydantic Models - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/union.png",
      "og:title": "Using Union Types in Pydantic Models - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/union/?q=",
      "statusCode": 200,
      "description": "Learn how to implement Union types in Pydantic models to handle multiple action types in Python.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to implement Union types in Pydantic models to handle multiple action types in Python.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/union.png",
      "twitter:title": "Using Union Types in Pydantic Models - Instructor",
      "og:description": "Learn how to implement Union types in Pydantic models to handle multiple action types in Python.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to implement Union types in Pydantic models to handle multiple action types in Python."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/distillation/?q=#distilling-python-functions-into-llm)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/distillation.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/distillation.md \"View source of this page\")\n\n# Distilling python functions into LLM [¶](https://python.useinstructor.com/concepts/distillation/?q=\\#distilling-python-functions-into-llm \"Permanent link\")\n\n`Instructions` from the `Instructor` library offers a seamless way to make language models backward compatible with existing Python functions. By employing Pydantic type hints, it not only ensures compatibility but also facilitates fine-tuning `gpt-3.5-turbo` to emulate these functions end-to-end.\n\nIf you want to see the full example checkout [examples/distillation](https://github.com/jxnl/instructor/tree/main/examples/distilations)\n\n## The Challenges in Function-Level Fine-Tuning [¶](https://python.useinstructor.com/concepts/distillation/?q=\\#the-challenges-in-function-level-fine-tuning \"Permanent link\")\n\nReplicating the behavior of a Python function in a language model involves intricate data preparation. For instance, teaching a model to execute three-digit multiplication is not as trivial as implementing `def f(a, b): return a * b`. OpenAI's fine-tuning script coupled with their function calling utility provides a structured output, thereby simplifying the data collection process. Additionally, this eliminates the need for passing the schema to the model, thus conserving tokens.\n\n## The Role of `Instructions` in Simplifying the Fine-Tuning Process [¶](https://python.useinstructor.com/concepts/distillation/?q=\\#the-role-of-instructions-in-simplifying-the-fine-tuning-process \"Permanent link\")\n\nBy using `Instructions`, you can annotate a Python function that returns a Pydantic object, thereby automating the dataset creation for fine-tuning. A handler for logging is all that's needed to build this dataset.\n\n## How to Implement `Instructions` in Your Code [¶](https://python.useinstructor.com/concepts/distillation/?q=\\#how-to-implement-instructions-in-your-code \"Permanent link\")\n\n## Quick Start: How to Use Instructor's Distillation Feature [¶](https://python.useinstructor.com/concepts/distillation/?q=\\#quick-start-how-to-use-instructors-distillation-feature \"Permanent link\")\n\nBefore we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file.\n\n```md-code__content\nimport logging\nimport random\nfrom pydantic import BaseModel\nfrom instructor import Instructions  # pip install instructor\n\n# Logging setup\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs!\n    log_handlers=[logging.FileHandler(\"math_finetunes.jsonl\")],\n)\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n# Define a function with distillation\n# The decorator will automatically generate a dataset for fine-tuning\n# They must return a pydantic model to leverage function calling\n@instructions.distil\ndef fn(a: int, b: int) -> Multiply:\n    resp = a * b\n    return Multiply(a=a, b=b, result=resp)\n\n# Generate some data\nfor _ in range(10):\n    random.seed(42)\n    a = random.randint(100, 999)\n    b = random.randint(100, 999)\n    print(fn(a, b))\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n\n```\n\n## The Intricacies of Fine-tuning Language Models [¶](https://python.useinstructor.com/concepts/distillation/?q=\\#the-intricacies-of-fine-tuning-language-models \"Permanent link\")\n\nFine-tuning isn't just about writing a function like `def f(a, b): return a * b`. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this.\n\n## Why Instructor and Distillation are Game Changers [¶](https://python.useinstructor.com/concepts/distillation/?q=\\#why-instructor-and-distillation-are-game-changers \"Permanent link\")\n\nThe library offers two main benefits:\n\n1. **Efficiency**: Streamlines functions, distilling requirements into model weights and a few lines of code.\n2. **Integration**: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions.\n\n## Role of Instructor in Simplifying Fine-Tuning [¶](https://python.useinstructor.com/concepts/distillation/?q=\\#role-of-instructor-in-simplifying-fine-tuning \"Permanent link\")\n\nThe `from instructor import Instructions` feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior.\n\n## Logging Output and Running a Finetune [¶](https://python.useinstructor.com/concepts/distillation/?q=\\#logging-output-and-running-a-finetune \"Permanent link\")\n\nHere's how the logging output would look:\n\n```md-code__content\n{\n    \"messages\": [\\\n        {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'},\\\n        {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'},\\\n        {\\\n            \"role\": \"assistant\",\\\n            \"function_call\": {\\\n                \"name\": \"Multiply\",\\\n                \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}',\\\n            },\\\n        },\\\n    ],\n    \"functions\": [\\\n        {\"name\": \"Multiply\", \"description\": \"Correctly extracted `Multiply`...\"}\\\n    ],\n}\n\n```\n\nRun a finetune like this:\n\n```md-code__content\ninstructor jobs create-from-file math_finetunes.jsonl\n\n```\n\nOnce a model is trained you can simply change `mode` to `dispatch` and it will use the model to run the function!\n\n```md-code__content\nfrom instructor import Instructions\nfrom pydantic import BaseModel\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n)\n\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")\ndef fn(a: int, b: int) -> Multiply:\n    # now this code will be short circuited and the model will be used instead.\n    resp = a + b\n    return Multiply(a=a, b=b, result=resp)\n\n```\n\nWith this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/distillation/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/distillation/",
      "title": "Seamless Fine-Tuning of Python Functions Using Instructor's Distillation - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/distillation/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/distillation.png",
      "ogTitle": "Seamless Fine-Tuning of Python Functions Using Instructor's Distillation - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/distillation.png",
      "og:title": "Seamless Fine-Tuning of Python Functions Using Instructor's Distillation - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/distillation/?q=",
      "statusCode": 200,
      "description": "Learn how to fine-tune language models with Python functions using Instructor's `Instructions` for efficient data preparation and logging.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to fine-tune language models with Python functions using Instructor's `Instructions` for efficient data preparation and logging.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/distillation.png",
      "twitter:title": "Seamless Fine-Tuning of Python Functions Using Instructor's Distillation - Instructor",
      "og:description": "Learn how to fine-tune language models with Python functions using Instructor's `Instructions` for efficient data preparation and logging.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to fine-tune language models with Python functions using Instructor's `Instructions` for efficient data preparation and logging."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/multimodal/?q=#multimodal)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/multimodal.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/multimodal.md \"View source of this page\")\n\n# Multimodal [¶](https://python.useinstructor.com/concepts/multimodal/?q=\\#multimodal \"Permanent link\")\n\nInstructor supports multimodal interactions by providing helper classes that are automatically converted to the correct format for different providers, allowing you to work with both text and images in your prompts and responses. This functionality is implemented in the `multimodal.py` module and provides a seamless way to handle images alongside text for various AI models.\n\n## `Image` [¶](https://python.useinstructor.com/concepts/multimodal/?q=\\#image \"Permanent link\")\n\nThe core of multimodal support in Instructor is the `Image` class. This class represents an image that can be loaded from a URL or file path. It provides methods to create `Image` instances and convert them to formats compatible with different AI providers.\n\nIt's important to note that Anthropic and OpenAI have different formats for handling images in their API requests. The `Image` class in Instructor abstracts away these differences, allowing you to work with a unified interface.\n\n### Usage [¶](https://python.useinstructor.com/concepts/multimodal/?q=\\#usage \"Permanent link\")\n\nYou can create an `Image` instance from a URL or file path using the `from_url` or `from_path` methods. The `Image` class will automatically convert the image to a base64-encoded string and include it in the API request.\n\n```md-code__content\nimport instructor\nimport openai\nfrom pydantic import BaseModel\n\nclass ImageAnalyzer(BaseModel):\n    description: str\n\nimage1 = instructor.Image.from_url(url)\nimage2 = instructor.Image.from_path(\"muffin.jpg\")\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=ImageAnalyzer,\n    messages=[\\\n        {\"role\": \"user\", \"content\": [\"What is in this two images?\", image1, image2]}\\\n    ],\n)\n\nprint(response.model_dump_json())\n\"\"\"\n{\"description\":\"A tray of freshly baked blueberry muffins. The muffins have a golden-brown top, are placed in paper liners, and some have blueberries peeking out. In the background, more muffins are visible, along with a single blueberry on the tray.\"}\n\"\"\"\n\n```\n\nThe `Image` class takes care of the necessary conversions and formatting, ensuring that your code remains clean and provider-agnostic. This flexibility is particularly valuable when you're experimenting with different models or when you need to switch providers based on specific project requirements.\n\nBy leveraging Instructor's multimodal capabilities, you can focus on building your application logic without worrying about the intricacies of each provider's image handling format. This not only saves development time but also makes your code more maintainable and adaptable to future changes in AI provider APIs.\n\nAlternatively, by passing `autodetect_images=True` to `client.chat.completions.create`, you can pass file paths, URLs, or base64 encoded content directly as strings.\n\n```md-code__content\nimport instructor\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=ImageAnalyzer,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": [\\\n                \"What is in this two images?\",\\\n                \"https://example.com/image.jpg\",\\\n                \"path/to/image.jpg\",\\\n            ],\\\n        }\\\n    ],\n    autodetect_images=True,\n)\n\n```\n\n### Anthropic Prompt Caching [¶](https://python.useinstructor.com/concepts/multimodal/?q=\\#anthropic-prompt-caching \"Permanent link\")\n\nInstructor supports Anthropic prompt caching with images. To activate prompt caching, you can pass image content as a dictionary of the form\n\n```md-code__content\n{\"type\": \"image\", \"source\": <path_or_url_or_base64_encoding>, \"cache_control\": True}\n\n```\n\nand set `autodetect_images=True`, or flag it within a constructor such as `instructor.Image.from_path(\"path/to/image.jpg\", cache_control=True)`. For example:\n\n```md-code__content\nimport instructor\nfrom anthropic import Anthropic\n\nclient = instructor.from_anthropic(Anthropic(), enable_prompt_caching=True)\n\ncache_control = {\"type\": \"ephemeral\"}\nresponse = client.chat.completions.create(\n    model=\"claude-3-haiku-20240307\",\n    response_model=ImageAnalyzer,  # This can be set to `None` to return an Anthropic prompt caching message\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": [\\\n                \"What is in this two images?\",\\\n                {\\\n                    \"type\": \"image\",\\\n                    \"source\": \"https://example.com/image.jpg\",\\\n                    \"cache_control\": cache_control,\\\n                },\\\n                {\\\n                    \"type\": \"image\",\\\n                    \"source\": \"path/to/image.jpg\",\\\n                    \"cache_control\": cache_control,\\\n                },\\\n            ],\\\n        }\\\n    ],\n    autodetect_images=True,\n)\n\n```\n\n## `Audio` [¶](https://python.useinstructor.com/concepts/multimodal/?q=\\#audio \"Permanent link\")\n\nThe `Audio` class represents an audio file that can be loaded from a URL or file path. It provides methods to create `Audio` instances but currently only OpenAI supports it. You can create an instance using the `from_path` and `from_url` methods. The `Audio` class will automatically convert it to a base64-encoded image and include it in the API request.\n\n### Usage [¶](https://python.useinstructor.com/concepts/multimodal/?q=\\#usage_1 \"Permanent link\")\n\n```md-code__content\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nimport instructor\nfrom instructor.multimodal import Audio\n\nclient = instructor.from_openai(OpenAI())\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nresp = client.chat.completions.create(\n    model=\"gpt-4o-audio-preview\",\n    response_model=User,\n    modalities=[\"text\"],\n    audio={\"voice\": \"alloy\", \"format\": \"wav\"},\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": [\\\n                \"Extract the following information from the audio:\",\\\n                Audio.from_path(\"./output.wav\"),\\\n            ],\\\n        },  # type: ignore\\\n    ],\n)\n\nprint(resp)\n#> name='Jason' age=20\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/multimodal/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/multimodal/",
      "title": "Seamless Multimodal Interactions with Instructor - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/multimodal/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/multimodal.png",
      "ogTitle": "Seamless Multimodal Interactions with Instructor - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/multimodal.png",
      "og:title": "Seamless Multimodal Interactions with Instructor - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/multimodal/?q=",
      "statusCode": 200,
      "description": "Learn how the Image and Audio class in Instructor enables seamless handling of images, audio and text across different AI models.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how the Image and Audio class in Instructor enables seamless handling of images, audio and text across different AI models.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/multimodal.png",
      "twitter:title": "Seamless Multimodal Interactions with Instructor - Instructor",
      "og:description": "Learn how the Image and Audio class in Instructor enables seamless handling of images, audio and text across different AI models.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how the Image and Audio class in Instructor enables seamless handling of images, audio and text across different AI models."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/raw_response/?q=#creating-a-model-with-completions)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/raw_response.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/raw_response.md \"View source of this page\")\n\n# Creating a model with completions [¶](https://python.useinstructor.com/concepts/raw_response/?q=\\#creating-a-model-with-completions \"Permanent link\")\n\nIn instructor>1.0.0 we have a custom client, if you wish to use the raw response you can do the following\n\n```md-code__content\nimport instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\nuser, completion = client.chat.completions.create_with_completion(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserExtract,\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\\\n    ],\n)\n\nprint(user)\n#> name='Jason' age=25\n\nprint(completion)\n\"\"\"\nChatCompletion(\n    id='chatcmpl-AWl4kOf2XIrMZ2cBWC41gXCkFCpQs',\n    choices=[\\\n        Choice(\\\n            finish_reason='stop',\\\n            index=0,\\\n            logprobs=None,\\\n            message=ChatCompletionMessage(\\\n                content=None,\\\n                refusal=None,\\\n                role='assistant',\\\n                audio=None,\\\n                function_call=None,\\\n                tool_calls=[\\\n                    ChatCompletionMessageToolCall(\\\n                        id='call_bGBFg2QrTqw30Y8zXCs9RYGY',\\\n                        function=Function(\\\n                            arguments='{\"name\":\"Jason\",\"age\":25}', name='UserExtract'\\\n                        ),\\\n                        type='function',\\\n                    )\\\n                ],\\\n            ),\\\n        )\\\n    ],\n    created=1732370818,\n    model='gpt-3.5-turbo-0125',\n    object='chat.completion',\n    service_tier=None,\n    system_fingerprint=None,\n    usage=CompletionUsage(\n        completion_tokens=9,\n        prompt_tokens=82,\n        total_tokens=91,\n        completion_tokens_details=CompletionTokensDetails(\n            audio_tokens=0, reasoning_tokens=0\n        ),\n        prompt_tokens_details=None,\n        prompt_token_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0),\n    ),\n)\n\"\"\"\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/raw_response/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/raw_response/",
      "title": "Creating a Model with OpenAI Completions - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/raw_response/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/raw_response.png",
      "ogTitle": "Creating a Model with OpenAI Completions - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/raw_response.png",
      "og:title": "Creating a Model with OpenAI Completions - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/raw_response/?q=",
      "statusCode": 200,
      "description": "Learn how to create a custom model using OpenAI's API to extract user data efficiently with Python.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to create a custom model using OpenAI's API to extract user data efficiently with Python.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/raw_response.png",
      "twitter:title": "Creating a Model with OpenAI Completions - Instructor",
      "og:description": "Learn how to create a custom model using OpenAI's API to extract user data efficiently with Python.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to create a custom model using OpenAI's API to extract user data efficiently with Python."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/fastapi/?q=#integrating-pydantic-models-with-fastapi)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/fastapi.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/fastapi.md \"View source of this page\")\n\n# Integrating Pydantic Models with FastAPI [¶](https://python.useinstructor.com/concepts/fastapi/?q=\\#integrating-pydantic-models-with-fastapi \"Permanent link\")\n\n[FastAPI](https://fastapi.tiangolo.com/) is an enjoyable tool for building web applications in Python. It is well known for its integration with `Pydantic` models, which makes defining and validating data structures straightforward and efficient. In this guide, we explore how simple functions that return `Pydantic` models can seamlessly integrate with `FastAPI`.\n\n## Why Choose FastAPI and Pydantic? [¶](https://python.useinstructor.com/concepts/fastapi/?q=\\#why-choose-fastapi-and-pydantic \"Permanent link\")\n\n- FastAPI is a modern, high-performance web framework for building APIs with Python.\n- Supports OpenAPI and JSON Schema for automatic documentation and validation.\n- Supports AsyncIO for asynchronous programming leveraging the AsyncOpenAI() client\n\n## Code Example: Starting a FastAPI App with a POST Request [¶](https://python.useinstructor.com/concepts/fastapi/?q=\\#code-example-starting-a-fastapi-app-with-a-post-request \"Permanent link\")\n\nThe following code snippet demonstrates how to start a `FastAPI` app with a POST endpoint. This endpoint accepts and returns data defined by a `Pydantic` model.\n\n```md-code__content\nimport instructor\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\n\n# Enables response_model\nclient = instructor.from_openai(AsyncOpenAI())\napp = FastAPI()\n\nclass UserData(BaseModel):\n    # This can be the model for the input data\n    query: str\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@app.post(\"/endpoint\", response_model=UserDetail)\nasync def endpoint_function(data: UserData) -> UserDetail:\n    user_detail = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\\\n            {\"role\": \"user\", \"content\": f\"Extract: `{data.query}`\"},\\\n        ],\n    )\n    return user_detail\n\n```\n\n## Streaming Responses with FastAPI [¶](https://python.useinstructor.com/concepts/fastapi/?q=\\#streaming-responses-with-fastapi \"Permanent link\")\n\n`FastAPI` supports streaming responses, which is useful for returning large amounts of data. This feature is particularly useful when working with large language models (LLMs) that generate a large amount of data.\n\n```md-code__content\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass UserData(BaseModel):\n    query: str\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n# Route to handle SSE events and return users\n@app.post(\"/extract\", response_class=StreamingResponse)\nasync def extract(data: UserData):\n    users = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        stream=True,\n        messages=[\\\n            {\"role\": \"user\", \"content\": data.query},\\\n        ],\n    )\n\n    async def generate():\n        async for user in users:\n            resp_json = user.model_dump_json()\n            yield f\"data: {resp_json}\"\n        yield \"data: [DONE]\"\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n\n```\n\n## Automatic Documentation with FastAPI [¶](https://python.useinstructor.com/concepts/fastapi/?q=\\#automatic-documentation-with-fastapi \"Permanent link\")\n\nFastAPI leverages the OpenAPI specification to automatically generate a dynamic and interactive documentation page, commonly referred to as the `/docs` page. This feature is incredibly useful for developers, as it offers a live environment to test API endpoints directly through the browser.\n\nTo explore the capabilities of your API, follow these steps:\n\n1. Run the API using the Uvicorn command: `uvicorn main:app --reload`.\n2. Open your web browser and navigate to `http://127.0.0.1:8000/docs`.\n3. You will find an interactive UI where you can send different requests to your API and see the responses in real-time.\n\n![Screenshot of FastAPI /docs page](https://python.useinstructor.com/concepts/response.png)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/fastapi/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/fastapi/",
      "title": "Integrating Pydantic with FastAPI for Efficient APIs - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/fastapi/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/fastapi.png",
      "ogTitle": "Integrating Pydantic with FastAPI for Efficient APIs - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/fastapi.png",
      "og:title": "Integrating Pydantic with FastAPI for Efficient APIs - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/fastapi/?q=",
      "statusCode": 200,
      "description": "Learn how to leverage Pydantic models with FastAPI for seamless API development and automatic documentation.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to leverage Pydantic models with FastAPI for seamless API development and automatic documentation.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/fastapi.png",
      "twitter:title": "Integrating Pydantic with FastAPI for Efficient APIs - Instructor",
      "og:description": "Learn how to leverage Pydantic models with FastAPI for seamless API development and automatic documentation.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to leverage Pydantic models with FastAPI for seamless API development and automatic documentation."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/caching/?q=#1-functoolscache-for-simple-in-memory-caching)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/caching.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/caching.md \"View source of this page\")\n\n# Caching\n\n* * *\n\ntitle: Caching Techniques in Python: In-Memory, Disk, and Redis description: Explore caching methods in Python with functools, diskcache, and Redis for improved performance in your applications.\n\n* * *\n\nIf you want to learn more about concepts in caching and how to use them in your own projects, check out our [blog](https://python.useinstructor.com/blog/2023/11/26/python-caching/) on the topic.\n\n## 1\\. `functools.cache` for Simple In-Memory Caching [¶](https://python.useinstructor.com/concepts/caching/?q=\\#1-functoolscache-for-simple-in-memory-caching \"Permanent link\")\n\n**When to Use**: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session. or in an application where we don't need to persist the cache between sessions.\n\n```md-code__content\nimport time\nimport functools\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@functools.cache\ndef extract(data) -> UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\\\n            {\"role\": \"user\", \"content\": data},\\\n        ],\n    )\n\nstart = time.perf_counter()  #\nmodel = extract(\"Extract jason is 25 years old\")\nprint(f\"Time taken: {time.perf_counter() - start}\")\n#> Time taken: 0.38183529197704047\n\nstart = time.perf_counter()\nmodel = extract(\"Extract jason is 25 years old\")  #\nprint(f\"Time taken: {time.perf_counter() - start}\")\n#> Time taken: 8.75093974173069e-07\n\n```\n\nChanging the Model does not Invalidate the Cache\n\nNote that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result.\n\nNow we can call `extract` multiple times with the same argument, and the result will be cached in memory for faster access.\n\n**Benefits**: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries.\n\nWhat is a decorator?\n\nA decorator is a function that takes another function and extends the behavior of the latter function without explicitly modifying it. In Python, decorators are functions that take a function as an argument and return a closure.\n\n```md-code__content\ndef decorator(func):\n    def wrapper(*args, **kwargs):\n        print(\"Do something before\")  #\n        #> Do something before\n        result = func(*args, **kwargs)\n        print(\"Do something after\")  #\n        #> Do something after\n        return result\n\n    return wrapper\n\n@decorator\ndef say_hello():\n    #> Hello!\n    print(\"Hello!\")\n    #> Hello!\n\nsay_hello()\n#> \"Do something before\"\n#> \"Hello!\"\n#> \"Do something after\"\n\n```\n\n## 2\\. `diskcache` for Persistent, Large Data Caching [¶](https://python.useinstructor.com/concepts/caching/?q=\\#2-diskcache-for-persistent-large-data-caching \"Permanent link\")\n\nCopy Caching Code\n\nWe'll be using the same `instructor_cache` decorator for both `diskcache` and `redis` caching. You can copy the code below and use it for both examples.\n\n```md-code__content\nimport functools\nimport inspect\nimport diskcache\n\ncache = diskcache.Cache('./my_cache_directory')  #\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  #\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n```\n\nRemember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the `Model.model_json_schema()` as part of the key.\n\n**When to Use**: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data!\n\n```md-code__content\nimport functools\nimport inspect\nimport instructor\nimport diskcache\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\ncache = diskcache.Cache('./my_cache_directory')\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation  #\n    if not issubclass(return_type, BaseModel):  #\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (\n            f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  #\n        )\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@instructor_cache\ndef extract(data) -> UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\\\n            {\"role\": \"user\", \"content\": data},\\\n        ],\n    )\n\n```\n\n**Benefits**: Reduces computation time for heavy data processing, provides disk-based caching for persistence.\n\n## 2\\. Redis Caching Decorator for Distributed Systems [¶](https://python.useinstructor.com/concepts/caching/?q=\\#2-redis-caching-decorator-for-distributed-systems \"Permanent link\")\n\nCopy Caching Code\n\nWe'll be using the same `instructor_cache` decorator for both `diskcache` and `redis` caching. You can copy the code below and use it for both examples.\n\n```md-code__content\nimport functools\nimport inspect\nimport redis\n\ncache = redis.Redis(\"localhost\")\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n```\n\nRemember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the `Model.model_json_schema()` as part of the key.\n\n**When to Use**: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures.\n\n```md-code__content\nimport redis\nimport functools\nimport inspect\nimport instructor\n\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\ncache = redis.Redis(\"localhost\")\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  #\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  #\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@instructor_cache\ndef extract(data) -> UserDetail:\n    # Assuming client.chat.completions.create returns a UserDetail instance\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\\\n            {\"role\": \"user\", \"content\": data},\\\n        ],\n    )\n\n```\n\n**Benefits**: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types.\n\nLooking carefully\n\nIf you look carefully at the code above you'll notice that we're using the same `instructor_cache` decorator as before. The implementation is the same, but we're using a different caching backend!\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/caching/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/caching/",
      "title": "Caching - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/caching/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/caching.png",
      "ogTitle": "Caching - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/caching.png",
      "og:title": "Caching - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/caching/?q=",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/caching.png",
      "twitter:title": "Caching - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/?q=#concepts)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/index.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/index.md \"View source of this page\")\n\n# Concepts [¶](https://python.useinstructor.com/concepts/?q=\\#concepts \"Permanent link\")\n\nWelcome to the Concepts section of Instructor documentation. This section provides an in-depth exploration of key ideas and techniques that form the foundation of working with structured outputs in AI applications using Instructor.\n\n## Introduction [¶](https://python.useinstructor.com/concepts/?q=\\#introduction \"Permanent link\")\n\nInstructor is designed to simplify the process of extracting structured data from large language models (LLMs). By leveraging the power of Pydantic and OpenAI's function calling API, Instructor enables developers to create robust, type-safe applications that can efficiently process and validate AI-generated outputs.\n\nIn this section, we'll cover a range of concepts that are crucial for understanding and effectively using Instructor. Whether you're new to the library or looking to deepen your knowledge, these guides will provide valuable insights into the core principles and advanced features of Instructor.\n\n## Key Concepts [¶](https://python.useinstructor.com/concepts/?q=\\#key-concepts \"Permanent link\")\n\nHere's an overview of the concepts we'll explore:\n\n1. [Aliases](https://python.useinstructor.com/concepts/alias/): Learn how to use aliases to customize field names in your Pydantic models.\n\n2. [Caching](https://python.useinstructor.com/concepts/caching/): Discover techniques for improving performance through effective data management and caching strategies.\n\n3. [Templating](https://python.useinstructor.com/concepts/templating/): Explore Jinja templating for dynamic and efficient prompt management.\n\n4. [Type Adapter](https://python.useinstructor.com/concepts/typeadapter/): Understand Pydantic's Type Adapter for enhanced data validation and parsing.\n\n5. [TypedDicts](https://python.useinstructor.com/concepts/typeddicts/): Learn about using TypedDicts for structured data handling with OpenAI's API.\n\n6. [Types](https://python.useinstructor.com/concepts/types/): Dive into the various data types supported by Instructor, from simple to complex.\n\n7. [Union](https://python.useinstructor.com/concepts/union/): Explore the use of Union types for flexible and dynamic operations in your models.\n\n8. [Usage](https://python.useinstructor.com/concepts/usage/): Get insights on handling non-streaming requests and managing token usage with the OpenAI API.\n\n\nEach of these concepts plays a crucial role in building efficient, type-safe, and robust applications with Instructor. By mastering these ideas, you'll be well-equipped to tackle complex data extraction and validation tasks in your AI-powered projects.\n\nWe encourage you to explore these concepts in depth and see how they can be applied to your specific use cases. Remember, the power of Instructor lies in its ability to combine these concepts seamlessly, allowing you to create sophisticated applications with ease.\n\nHappy learning, and enjoy your journey through the world of structured outputs with Instructor!\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/",
      "title": "Key Concepts for Structured Outputs in AI - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/index.png",
      "ogTitle": "Key Concepts for Structured Outputs in AI - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/index.png",
      "og:title": "Key Concepts for Structured Outputs in AI - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/?q=",
      "statusCode": 200,
      "description": "Explore essential concepts in Instructor for efficient extraction and validation of structured data from AI models.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Explore essential concepts in Instructor for efficient extraction and validation of structured data from AI models.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/index.png",
      "twitter:title": "Key Concepts for Structured Outputs in AI - Instructor",
      "og:description": "Explore essential concepts in Instructor for efficient extraction and validation of structured data from AI models.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Explore essential concepts in Instructor for efficient extraction and validation of structured data from AI models."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/models/?q=#response-model)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/models.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/models.md \"View source of this page\")\n\n# Response Model [¶](https://python.useinstructor.com/concepts/models/?q=\\#response-model \"Permanent link\")\n\nDefining LLM output schemas in Pydantic is done via `pydantic.BaseModel`. To learn more about models in Pydantic, check out their [documentation](https://docs.pydantic.dev/latest/concepts/models/).\n\nAfter defining a Pydantic model, we can use it as the `response_model` in your client `create` calls to OpenAI or any other supported model. The job of the `response_model` parameter is to:\n\n- Define the schema and prompts for the language model\n- Validate the response from the API\n- Return a Pydantic model instance.\n\n## Prompting [¶](https://python.useinstructor.com/concepts/models/?q=\\#prompting \"Permanent link\")\n\nWhen defining a response model, we can use docstrings and field annotations to define the prompt that will be used to generate the response.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\n\nclass User(BaseModel):\n    \"\"\"\n    This is the prompt that will be used to generate the response.\n    Any instructions here will be passed to the language model.\n    \"\"\"\n\n    name: str = Field(description=\"The name of the user.\")\n    age: int = Field(description=\"The age of the user.\")\n\n```\n\nHere all docstrings, types, and field annotations will be used to generate the prompt. The prompt will be generated by the `create` method of the client and will be used to generate the response.\n\n## Optional Values [¶](https://python.useinstructor.com/concepts/models/?q=\\#optional-values \"Permanent link\")\n\nIf we use `Optional` and `default`, they will be considered not required when sent to the language model.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the user.\")\n    age: int = Field(description=\"The age of the user.\")\n    email: Optional[str] = Field(description=\"The email of the user.\", default=None)\n\n```\n\nNote that fields can also be omitted entirely from being sent to the language model by using Pydantic's `SkipJsonSchema` annotation. See [Fields](https://python.useinstructor.com/concepts/fields/#omitting-fields-from-schema-sent-to-the-language-model) for additional details.\n\n## Dynamic model creation [¶](https://python.useinstructor.com/concepts/models/?q=\\#dynamic-model-creation \"Permanent link\")\n\nThere are some occasions where it is desirable to create a model using runtime information to specify the fields. For this, Pydantic provides the create\\_model function to allow models to be created on the fly:\n\n```md-code__content\nfrom pydantic import BaseModel, create_model\n\nclass FooModel(BaseModel):\n    foo: str\n    bar: int = 123\n\nBarModel = create_model(\n    'BarModel',\n    apple=(str, 'russet'),\n    banana=(str, 'yellow'),\n    __base__=FooModel,\n)\nprint(BarModel)\n#> <class '__main__.BarModel'>\nprint(BarModel.model_fields.keys())\n#> dict_keys(['foo', 'bar', 'apple', 'banana'])\n\n```\n\nWhen would I use this?\n\nConsider a situation where the model is dynamically defined, based on some configuration or database. For example, we could have a database table that stores the properties of a model for some model name or id. We could then query the database for the properties of the model and use that to create the model.\n\n```md-code__content\nSELECT property_name, property_type, description\nFROM prompt\nWHERE model_name = {model_name}\n\n```\n\nWe can then use this information to create the model.\n\n```md-code__content\nfrom pydantic import BaseModel, create_model\nfrom typing import List\n\ntypes = {\n    'string': str,\n    'integer': int,\n    'boolean': bool,\n    'number': float,\n    'List[str]': List[str],\n}\n\n# Mocked cursor.fetchall()\ncursor = [\\\n    ('name', 'string', 'The name of the user.'),\\\n    ('age', 'integer', 'The age of the user.'),\\\n    ('email', 'string', 'The email of the user.'),\\\n]\n\nBarModel = create_model(\n    'User',\n    **{\n        property_name: (types[property_type], description)\n        for property_name, property_type, description in cursor\n    },\n    __base__=BaseModel,\n)\n\nprint(BarModel.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'name': {'default': 'The name of the user.', 'title': 'Name', 'type': 'string'},\n        'age': {'default': 'The age of the user.', 'title': 'Age', 'type': 'integer'},\n        'email': {\n            'default': 'The email of the user.',\n            'title': 'Email',\n            'type': 'string',\n        },\n    },\n    'title': 'User',\n    'type': 'object',\n}\n\"\"\"\n\n```\n\nThis would be useful when different users have different descriptions for the same model. We can use the same model but have different prompts for each user.\n\n## Adding Behavior [¶](https://python.useinstructor.com/concepts/models/?q=\\#adding-behavior \"Permanent link\")\n\nWe can add methods to our Pydantic models, just as any plain Python class. We might want to do this to add some custom logic to our models.\n\n```md-code__content\nfrom pydantic import BaseModel\nfrom typing import Literal\n\nfrom openai import OpenAI\n\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\n\nclass SearchQuery(BaseModel):\n    query: str\n    query_type: Literal[\"web\", \"image\", \"video\"]\n\n    def execute(self):\n        print(f\"Searching for {self.query} of type {self.query_type}\")\n        #> Searching for cat of type image\n        return \"Results for cat\"\n\nquery = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Search for a picture of a cat\"}],\n    response_model=SearchQuery,\n)\n\nresults = query.execute()\nprint(results)\n#> Results for cat\n\n```\n\nNow we can call `execute` on our model instance after extracting it from a language model. If you want to see more examples of this checkout our post on [RAG is more than embeddings](https://python.useinstructor.com/blog/2023/09/17/rag-is-more-than-just-embedding-search/)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/models/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/models/",
      "title": "Using Pydantic for Dynamic LLM Response Models - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/models/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/models.png",
      "ogTitle": "Using Pydantic for Dynamic LLM Response Models - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/models.png",
      "og:title": "Using Pydantic for Dynamic LLM Response Models - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/models/?q=",
      "statusCode": 200,
      "description": "Learn how to define and manage LLM output schemas with Pydantic, including dynamic model creation and adding custom behavior.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to define and manage LLM output schemas with Pydantic, including dynamic model creation and adding custom behavior.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/models.png",
      "twitter:title": "Using Pydantic for Dynamic LLM Response Models - Instructor",
      "og:description": "Learn how to define and manage LLM output schemas with Pydantic, including dynamic model creation and adding custom behavior.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to define and manage LLM output schemas with Pydantic, including dynamic model creation and adding custom behavior."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/prompt_caching/?q=#prompt-caching)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/prompt_caching.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/prompt_caching.md \"View source of this page\")\n\n# Prompt Caching [¶](https://python.useinstructor.com/concepts/prompt_caching/?q=\\#prompt-caching \"Permanent link\")\n\nPrompt Caching is a feature that allows you to cache portions of your prompt, optimizing performance for multiple API calls with shared context. This helps to reduce cost and improve response times.\n\n## Prompt Caching in OpenAI [¶](https://python.useinstructor.com/concepts/prompt_caching/?q=\\#prompt-caching-in-openai \"Permanent link\")\n\nOpenAI implements a prompt caching mechanism to optimize performance for API requests with similar prompts.\n\n> Prompt Caching works automatically on all your API requests (no code changes required) and has no additional fees associated with it.\n\nThis optimization is especially useful for applications making multiple API calls with shared context, minimizing redundant processing and improving overall performance.\n\nPrompt Caching is enabled for the following models:\n\n- gpt-4o\n- gpt-4o-mini\n- o1-preview\n- o1-mini\n\nCaching is based on prefix matching, so if you're using a system prompt that contains a common set of instructions, you're likely to see a cache hit as long as you move all variable parts of the prompt to the end of the message when possible.\n\n## Prompt Caching in Anthropic [¶](https://python.useinstructor.com/concepts/prompt_caching/?q=\\#prompt-caching-in-anthropic \"Permanent link\")\n\nThe `anthropic.beta.prompt_caching.messages.create` method enables you to:\n\n1. Cache specific prompt portions\n2. Reuse cached content in subsequent calls\n3. Reduce processed data per request\n\nBy implementing prompt caching, you can potentially enhance efficiency and reduce costs, especially when dealing with large, shared contexts across multiple API interactions.\n\nSource Text\n\nIn the following example, we'll be using a short excerpt from the novel \"Pride and Prejudice\" by Jane Austen. This text serves as an example of a substantial context that might typically lead to slow response times and high costs when working with language models. You can download it manually [here](https://www.gutenberg.org/cache/epub/1342/pg1342.txt)\n\n```md-code__content\n    _Walt Whitman has somewhere a fine and just distinction between “loving\nby allowance” and “loving with personal love.” This distinction applies\nto books as well as to men and women; and in the case of the not very\nnumerous authors who are the objects of the personal affection, it\nbrings a curious consequence with it. There is much more difference as\nto their best work than in the case of those others who are loved “by\nallowance” by convention, and because it is felt to be the right and\nproper thing to love them. And in the sect--fairly large and yet\nunusually choice--of Austenians or Janites, there would probably be\nfound partisans of the claim to primacy of almost every one of the\nnovels. To some the delightful freshness and humour of_ Northanger\nAbbey, _its completeness, finish, and_ entrain, _obscure the undoubted\ncritical facts that its scale is small, and its scheme, after all, that\nof burlesque or parody, a kind in which the first rank is reached with\ndifficulty._ Persuasion, _relatively faint in tone, and not enthralling\nin interest, has devotees who exalt above all the others its exquisite\ndelicacy and keeping. The catastrophe of_ Mansfield Park _is admittedly\ntheatrical, the hero and heroine are insipid, and the author has almost\nwickedly destroyed all romantic interest by expressly admitting that\nEdmund only took Fanny because Mary shocked him, and that Fanny might\nvery likely have taken Crawford if he had been a little more assiduous;\nyet the matchless rehearsal-scenes and the characters of Mrs. Norris and\nothers have secured, I believe, a considerable party for it._ Sense and\nSensibility _has perhaps the fewest out-and-out admirers; but it does\nnot want them._\n_I suppose, however, that the majority of at least competent votes\nwould, all things considered, be divided between_ Emma _and the present\nbook; and perhaps the vulgar verdict (if indeed a fondness for Miss\nAusten be not of itself a patent of exemption from any possible charge\nof vulgarity) would go for_ Emma. _It is the larger, the more varied, the\nmore popular; the author had by the time of its composition seen rather\nmore of the world, and had improved her general, though not her most\npeculiar and characteristic dialogue; such figures as Miss Bates, as the\nEltons, cannot but unite the suffrages of everybody. On the other hand,\nI, for my part, declare for_ Pride and Prejudice _unhesitatingly. It\nseems to me the most perfect, the most characteristic, the most\neminently quintessential of its author’s works; and for this contention\nin such narrow space as is permitted to me, I propose here to show\ncause._\n_In the first place, the book (it may be barely necessary to remind the\nreader) was in its first shape written very early, somewhere about 1796,\nwhen Miss Austen was barely twenty-one; though it was revised and\nfinished at Chawton some fifteen years later, and was not published till\n1813, only four years before her death. I do not know whether, in this\ncombination of the fresh and vigorous projection of youth, and the\ncritical revision of middle life, there may be traced the distinct\nsuperiority in point of construction, which, as it seems to me, it\npossesses over all the others. The plot, though not elaborate, is almost\nregular enough for Fielding; hardly a character, hardly an incident\ncould be retrenched without loss to the story. The elopement of Lydia\nand Wickham is not, like that of Crawford and Mrs. Rushworth, a_ coup de\nthéâtre; _it connects itself in the strictest way with the course of the\nstory earlier, and brings about the denouement with complete propriety.\nAll the minor passages--the loves of Jane and Bingley, the advent of Mr.\nCollins, the visit to Hunsford, the Derbyshire tour--fit in after the\nsame unostentatious, but masterly fashion. There is no attempt at the\nhide-and-seek, in-and-out business, which in the transactions between\nFrank Churchill and Jane Fairfax contributes no doubt a good deal to the\nintrigue of_ Emma, _but contributes it in a fashion which I do not think\nthe best feature of that otherwise admirable book. Although Miss Austen\nalways liked something of the misunderstanding kind, which afforded her\nopportunities for the display of the peculiar and incomparable talent to\nbe noticed presently, she has been satisfied here with the perfectly\nnatural occasions provided by the false account of Darcy’s conduct given\nby Wickham, and by the awkwardness (arising with equal naturalness) from\nthe gradual transformation of Elizabeth’s own feelings from positive\naversion to actual love. I do not know whether the all-grasping hand of\nthe playwright has ever been laid upon_ Pride and Prejudice; _and I dare\nsay that, if it were, the situations would prove not startling or\ngarish enough for the footlights, the character-scheme too subtle and\ndelicate for pit and gallery. But if the attempt were made, it would\ncertainly not be hampered by any of those loosenesses of construction,\nwhich, sometimes disguised by the conveniences of which the novelist can\navail himself, appear at once on the stage._\n_I think, however, though the thought will doubtless seem heretical to\nmore than one school of critics, that construction is not the highest\nmerit, the choicest gift, of the novelist. It sets off his other gifts\nand graces most advantageously to the critical eye; and the want of it\nwill sometimes mar those graces--appreciably, though not quite\nconsciously--to eyes by no means ultra-critical. But a very badly-built\nnovel which excelled in pathetic or humorous character, or which\ndisplayed consummate command of dialogue--perhaps the rarest of all\nfaculties--would be an infinitely better thing than a faultless plot\nacted and told by puppets with pebbles in their mouths. And despite the\nability which Miss Austen has shown in working out the story, I for one\nshould put_ Pride and Prejudice _far lower if it did not contain what\nseem to me the very masterpieces of Miss Austen’s humour and of her\nfaculty of character-creation--masterpieces who may indeed admit John\nThorpe, the Eltons, Mrs. Norris, and one or two others to their company,\nbut who, in one instance certainly, and perhaps in others, are still\nsuperior to them._\n_The characteristics of Miss Austen’s humour are so subtle and delicate\nthat they are, perhaps, at all times easier to apprehend than to\nexpress, and at any particular time likely to be differently\napprehended by different persons. To me this humour seems to possess a\ngreater affinity, on the whole, to that of Addison than to any other of\nthe numerous species of this great British genus. The differences of\nscheme, of time, of subject, of literary convention, are, of course,\nobvious enough; the difference of sex does not, perhaps, count for much,\nfor there was a distinctly feminine element in “Mr. Spectator,” and in\nJane Austen’s genius there was, though nothing mannish, much that was\nmasculine. But the likeness of quality consists in a great number of\ncommon subdivisions of quality--demureness, extreme minuteness of touch,\navoidance of loud tones and glaring effects. Also there is in both a\ncertain not inhuman or unamiable cruelty. It is the custom with those\nwho judge grossly to contrast the good nature of Addison with the\nsavagery of Swift, the mildness of Miss Austen with the boisterousness\nof Fielding and Smollett, even with the ferocious practical jokes that\nher immediate predecessor, Miss Burney, allowed without very much\nprotest. Yet, both in Mr. Addison and in Miss Austen there is, though a\nrestrained and well-mannered, an insatiable and ruthless delight in\nroasting and cutting up a fool. A man in the early eighteenth century,\nof course, could push this taste further than a lady in the early\nnineteenth; and no doubt Miss Austen’s principles, as well as her heart,\nwould have shrunk from such things as the letter from the unfortunate\nhusband in the_ Spectator, _who describes, with all the gusto and all the\ninnocence in the world, how his wife and his friend induce him to play\nat blind-man’s-buff. But another_ Spectator _letter--that of the damsel\nof fourteen who wishes to marry Mr. Shapely, and assures her selected\nMentor that “he admires your_ Spectators _mightily”--might have been\nwritten by a rather more ladylike and intelligent Lydia Bennet in the\ndays of Lydia’s great-grandmother; while, on the other hand, some (I\nthink unreasonably) have found “cynicism” in touches of Miss Austen’s\nown, such as her satire of Mrs. Musgrove’s self-deceiving regrets over\nher son. But this word “cynical” is one of the most misused in the\nEnglish language, especially when, by a glaring and gratuitous\nfalsification of its original sense, it is applied, not to rough and\nsnarling invective, but to gentle and oblique satire. If cynicism means\nthe perception of “the other side,” the sense of “the accepted hells\nbeneath,” the consciousness that motives are nearly always mixed, and\nthat to seem is not identical with to be--if this be cynicism, then\nevery man and woman who is not a fool, who does not care to live in a\nfool’s paradise, who has knowledge of nature and the world and life, is\na cynic. And in that sense Miss Austen certainly was one. She may even\nhave been one in the further sense that, like her own Mr. Bennet, she\ntook an epicurean delight in dissecting, in displaying, in setting at\nwork her fools and her mean persons. I think she did take this delight,\nand I do not think at all the worse of her for it as a woman, while she\nwas immensely the better for it as an artist.\n\n```\n\n```md-code__content\nfrom instructor import Instructor, Mode, patch\nfrom anthropic import Anthropic\nfrom pydantic import BaseModel\n\nclient = Instructor(\n    client=Anthropic(),\n    create=patch(\n        create=Anthropic().beta.prompt_caching.messages.create,\n        mode=Mode.ANTHROPIC_TOOLS,\n    ),\n    mode=Mode.ANTHROPIC_TOOLS,\n)\n\nclass Character(BaseModel):\n    name: str\n    description: str\n\nwith open(\"./book.txt\") as f:\n    book = f.read()\n\nresp = client.chat.completions.create(\n    model=\"claude-3-haiku-20240307\",\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": [\\\n                {\\\n                    \"type\": \"text\",\\\n                    \"text\": \"<book>\" + book + \"</book>\",\\\n                    \"cache_control\": {\"type\": \"ephemeral\"},\\\n                },\\\n                {\\\n                    \"type\": \"text\",\\\n                    \"text\": \"Extract a character from the text given above\",\\\n                },\\\n            ],\\\n        },\\\n    ],\n    response_model=Character,\n    max_tokens=1000,\n)\n\n```\n\nCaching Considerations\n\n**Minimum cache size**: For Claude Haiku, your cached content needs to be a minimum of 2048 tokens. For Claude Sonnet, the minimum is 1024 tokens.\n\n**Benefits**: The cost of reading from the cache is 10x lower than if we were to process the same message again and enables us to execute our queries significantly faster.\n\nWe've written a more detailed blog on how to use the `create_with_completion` method [here](https://python.useinstructor.com/blog/2024/09/14/why-should-i-use-prompt-caching/) to validate you're getting a cache hit with instructor.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/prompt_caching/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/prompt_caching/",
      "title": "Understanding Prompt Caching for API Efficiency - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/prompt_caching/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/prompt_caching.png",
      "ogTitle": "Understanding Prompt Caching for API Efficiency - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/prompt_caching.png",
      "og:title": "Understanding Prompt Caching for API Efficiency - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/prompt_caching/?q=",
      "statusCode": 200,
      "description": "Explore how prompt caching optimizes performance for API calls in OpenAI and Anthropic, enhancing efficiency and reducing costs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Explore how prompt caching optimizes performance for API calls in OpenAI and Anthropic, enhancing efficiency and reducing costs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/prompt_caching.png",
      "twitter:title": "Understanding Prompt Caching for API Efficiency - Instructor",
      "og:description": "Explore how prompt caching optimizes performance for API calls in OpenAI and Anthropic, enhancing efficiency and reducing costs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Explore how prompt caching optimizes performance for API calls in OpenAI and Anthropic, enhancing efficiency and reducing costs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/philosophy/?q=#philosophy)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/philosophy.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/philosophy.md \"View source of this page\")\n\n* * *\n\ntitle: Simplifying AI with Instructor: Flexibility and Transparency in Python Programming description: Discover how Instructor empowers Python developers with simplicity, flexibility, and transparent LLM integration for better AI engineering.\n\n* * *\n\n# Philosophy [¶](https://python.useinstructor.com/concepts/philosophy/?q=\\#philosophy \"Permanent link\")\n\nThe instructor values [simplicity](https://eugeneyan.com/writing/simplicity/) and flexibility in leveraging language models (LLMs). It offers a streamlined approach for structured output, avoiding unnecessary dependencies or complex abstractions. Let [Pydantic](https://docs.pydantic.dev/latest/) do the heavy lifting.\n\n> “Simplicity is a great virtue but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.” — Edsger Dijkstra\n\n### Proof that its simple [¶](https://python.useinstructor.com/concepts/philosophy/?q=\\#proof-that-its-simple \"Permanent link\")\n\n1. Most users will only need to learn `response_model` and `patch` to get started.\n2. No new prompting language to learn, no new abstractions to learn.\n\n### Proof that its transparent [¶](https://python.useinstructor.com/concepts/philosophy/?q=\\#proof-that-its-transparent \"Permanent link\")\n\n1. We write very little prompts, and we don't try to hide the prompts from you.\n2. We'll do better in the future to give you config over the 2 prompts we do write, Reasking and JSON\\_MODE prompts.\n\n### Proof that its flexible [¶](https://python.useinstructor.com/concepts/philosophy/?q=\\#proof-that-its-flexible \"Permanent link\")\n\n1. If you build a system with OpenAI directly, it is easy to incrementally adopt instructor.\n2. Add `response_model` and if you want to revert, just remove it.\n\n## The zen of `instructor` [¶](https://python.useinstructor.com/concepts/philosophy/?q=\\#the-zen-of-instructor \"Permanent link\")\n\nMaintain the flexibility and power of Python, without unnecessary constraints.\n\nBegin with a function and a return type hint – simplicity is key. With my experience maintaining a large enterprise framework at my previous job over many years I've learned that the goal of making a useful framework is minimizing regret, both for the author and hopefully for the user.\n\n1. Define a Schema `class StructuredData(BaseModel):`\n2. Define validators and methods on your schema.\n3. Encapsulate all your LLM logic into a function `def extract(a) -> StructuredData:`\n4. Define typed computations against your data with `def compute(data: StructuredData):` or call methods on your schema `data.compute()`\n\nIt should be that simple.\n\n## My Goals [¶](https://python.useinstructor.com/concepts/philosophy/?q=\\#my-goals \"Permanent link\")\n\nThe goal for the library, [documentation](https://jxnl.github.io/instructor/), and [blog](https://jxnl.github.io/instructor/blog/), is to help you be a better python programmer and as a result a better AI engineer.\n\n- The library is a result of my desire for simplicity.\n- The library should help maintain simplicity in your codebase.\n- I won't try to write prompts for you,\n- I don't try to create indirections or abstractions that make it hard to debug in the future\n\nPlease note that the library is designed to be adaptable and open-ended, allowing you to customize and extend its functionality based on your specific requirements. If you have any further questions or ideas hit me up on [twitter](https://twitter.com/jxnlco)\n\nCheers!\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/philosophy/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/philosophy/",
      "title": "Philosophy - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/philosophy/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/philosophy.png",
      "ogTitle": "Philosophy - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/philosophy.png",
      "og:title": "Philosophy - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/philosophy/?q=",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/philosophy.png",
      "twitter:title": "Philosophy - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/usage.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/usage.md \"View source of this page\")\n\n# Usage Tokens\n\nThe easiest way to get usage for non streaming requests is to access the raw response.\n\n```md-code__content\nimport instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\nuser, completion = client.chat.completions.create_with_completion(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserExtract,\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\\\n    ],\n)\n\nprint(completion.usage)\n\"\"\"\nCompletionUsage(\n    completion_tokens=9,\n    prompt_tokens=82,\n    total_tokens=91,\n    completion_tokens_details=CompletionTokensDetails(\n        audio_tokens=0, reasoning_tokens=0\n    ),\n    prompt_tokens_details=None,\n    prompt_token_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0),\n)\n\"\"\"\n\n```\n\nYou can catch an IncompleteOutputException whenever the context length is exceeded and react accordingly, such as by trimming your prompt by the number of exceeding tokens.\n\n```md-code__content\nfrom instructor.exceptions import IncompleteOutputException\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\ntry:\n    client.chat.completions.create_with_completion(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserExtract,\n        messages=[\\\n            {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\\\n        ],\n    )\nexcept IncompleteOutputException as e:\n    token_count = e.last_completion.usage.total_tokens  # type: ignore\n    # your logic here\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/usage/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/usage/",
      "title": "Handling Non-Streaming Requests in OpenAI with Usage Tracking - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/usage/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/usage.png",
      "ogTitle": "Handling Non-Streaming Requests in OpenAI with Usage Tracking - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/usage.png",
      "og:title": "Handling Non-Streaming Requests in OpenAI with Usage Tracking - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/usage/?q=",
      "statusCode": 200,
      "description": "Learn how to manage non-streaming requests in OpenAI, track token usage, and handle exceptions with Python.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to manage non-streaming requests in OpenAI, track token usage, and handle exceptions with Python.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/usage.png",
      "twitter:title": "Handling Non-Streaming Requests in OpenAI with Usage Tracking - Instructor",
      "og:description": "Learn how to manage non-streaming requests in OpenAI, track token usage, and handle exceptions with Python.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to manage non-streaming requests in OpenAI, track token usage, and handle exceptions with Python."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/parallel/?q=#parallel-tools)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/parallel.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/parallel.md \"View source of this page\")\n\n# Parallel Tools [¶](https://python.useinstructor.com/concepts/parallel/?q=\\#parallel-tools \"Permanent link\")\n\nOne of the latest capabilities that OpenAI has recently introduced is parallel function calling. To learn more you can read up on [this](https://platform.openai.com/docs/guides/function-calling/parallel-function-calling)\n\nExperimental Feature\n\nThis feature is currently in preview and is subject to change. only supported by the `gpt-4-turbo-preview` model.\n\n## Understanding Parallel Function Calling [¶](https://python.useinstructor.com/concepts/parallel/?q=\\#understanding-parallel-function-calling \"Permanent link\")\n\nBy using parallel function callings that allow you to call multiple functions in a single request, you can significantly reduce the latency of your application without having to use tricks with now one builds a schema.\n\n```md-code__content\nfrom __future__ import annotations\n\nimport openai\nimport instructor\n\nfrom typing import Iterable, Literal\nfrom pydantic import BaseModel\n\nclass Weather(BaseModel):\n    location: str\n    units: Literal[\"imperial\", \"metric\"]\n\nclass GoogleSearch(BaseModel):\n    query: str\n\nclient = instructor.from_openai(\n    openai.OpenAI(), mode=instructor.Mode.PARALLEL_TOOLS\n)\n\nfunction_calls = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\\\n        {\"role\": \"system\", \"content\": \"You must always use tools\"},\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"What is the weather in toronto and dallas and who won the super bowl?\",\\\n        },\\\n    ],\n    response_model=Iterable[Weather | GoogleSearch],\n)\n\nfor fc in function_calls:\n    print(fc)\n    #> location='Toronto' units='metric'\n    #> location='Dallas' units='imperial'\n    #> query='who won the super bowl'\n\n```\n\nNoticed that the `response_model` Must be in the form `Iterable[Type1 | Type2 | ...]` or `Iterable[Type1]` where `Type1` and `Type2` are the types of the objects that will be returned in the response.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/parallel/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/parallel/",
      "title": "Understanding Parallel Function Calling in OpenAI - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/parallel/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/parallel.png",
      "ogTitle": "Understanding Parallel Function Calling in OpenAI - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/parallel.png",
      "og:title": "Understanding Parallel Function Calling in OpenAI - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/parallel/?q=",
      "statusCode": 200,
      "description": "Learn about OpenAI's experimental parallel function calling to reduce latency and improve application performance.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn about OpenAI's experimental parallel function calling to reduce latency and improve application performance.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/parallel.png",
      "twitter:title": "Understanding Parallel Function Calling in OpenAI - Instructor",
      "og:description": "Learn about OpenAI's experimental parallel function calling to reduce latency and improve application performance.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn about OpenAI's experimental parallel function calling to reduce latency and improve application performance."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/validation/?q=#validation-in-instructor)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/validation.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/validation.md \"View source of this page\")\n\n# Validation in Instructor [¶](https://python.useinstructor.com/concepts/validation/?q=\\#validation-in-instructor \"Permanent link\")\n\nThis guide covers validation concepts and best practices when using Instructor for structured outputs.\n\n## Overview [¶](https://python.useinstructor.com/concepts/validation/?q=\\#overview \"Permanent link\")\n\nValidation in Instructor ensures that the output from language models matches your expected schema. This is crucial for: - Data consistency - Error handling - Type safety - Business logic enforcement\n\n## Basic Validation [¶](https://python.useinstructor.com/concepts/validation/?q=\\#basic-validation \"Permanent link\")\n\nInstructor uses Pydantic for validation, which provides: 1. Type checking 2. Data coercion 3. Custom validators 4. Field constraints\n\n```md-code__content\nfrom pydantic import BaseModel, Field, validator\nfrom typing import List\n\nclass User(BaseModel):\n    name: str = Field(..., min_length=2)\n    age: int = Field(..., ge=0, le=150)\n    emails: List[str]\n\n    @validator('emails')\n    def validate_emails(cls, v):\n        if not all('@' in email for email in v):\n            raise ValueError('Invalid email format')\n        return v\n\n```\n\n## Validation Strategies [¶](https://python.useinstructor.com/concepts/validation/?q=\\#validation-strategies \"Permanent link\")\n\n### 1\\. Field Validation [¶](https://python.useinstructor.com/concepts/validation/?q=\\#1-field-validation \"Permanent link\")\n\nUse Field() for basic constraints:\n\n```md-code__content\nclass Product(BaseModel):\n    name: str = Field(..., min_length=1, max_length=100)\n    price: float = Field(..., gt=0)\n    quantity: int = Field(..., ge=0)\n\n```\n\n### 2\\. Custom Validators [¶](https://python.useinstructor.com/concepts/validation/?q=\\#2-custom-validators \"Permanent link\")\n\nUse [@validator](https://github.com/validator \"GitHub User: validator\") for complex validation:\n\n```md-code__content\nclass Order(BaseModel):\n    items: List[str]\n    total: float\n\n    @validator('total')\n    def validate_total(cls, v, values):\n        if v < 0:\n            raise ValueError('Total cannot be negative')\n        return v\n\n```\n\n### 3\\. Pre-validation Hooks [¶](https://python.useinstructor.com/concepts/validation/?q=\\#3-pre-validation-hooks \"Permanent link\")\n\nUse pre-validation hooks for data transformation:\n\n```md-code__content\nclass UserProfile(BaseModel):\n    username: str\n\n    @validator('username', pre=True)\n    def lowercase_username(cls, v):\n        return v.lower()\n\n```\n\n## Error Handling [¶](https://python.useinstructor.com/concepts/validation/?q=\\#error-handling \"Permanent link\")\n\nInstructor provides robust error handling for validation failures:\n\n```md-code__content\nfrom instructor import patch\nimport openai\n\nclient = patch(openai.OpenAI())\n\ntry:\n    user = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=User,\n        messages=[{\"role\": \"user\", \"content\": \"Extract: John Doe, age: -5\"}],\n    )\nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n\n```\n\n## Best Practices [¶](https://python.useinstructor.com/concepts/validation/?q=\\#best-practices \"Permanent link\")\n\n1. **Start Simple**: Begin with basic type validation before adding complex rules\n2. **Use Type Hints**: Always specify types for better code clarity\n3. **Document Constraints**: Add clear descriptions to Field() definitions\n4. **Handle Errors**: Implement proper error handling for validation failures\n5. **Test Edge Cases**: Verify validation works with unexpected inputs\n\n## Common Patterns [¶](https://python.useinstructor.com/concepts/validation/?q=\\#common-patterns \"Permanent link\")\n\n### Optional Fields [¶](https://python.useinstructor.com/concepts/validation/?q=\\#optional-fields \"Permanent link\")\n\n```md-code__content\nclass Profile(BaseModel):\n    name: str\n    bio: Optional[str] = None\n\n```\n\n### Nested Validation [¶](https://python.useinstructor.com/concepts/validation/?q=\\#nested-validation \"Permanent link\")\n\n```md-code__content\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\nclass User(BaseModel):\n    name: str\n    addresses: List[Address]\n\n```\n\n### Complex Validation [¶](https://python.useinstructor.com/concepts/validation/?q=\\#complex-validation \"Permanent link\")\n\n```md-code__content\nclass Transaction(BaseModel):\n    amount: float\n    currency: str\n    timestamp: datetime\n\n    @validator('currency')\n    def validate_currency(cls, v):\n        valid_currencies = ['USD', 'EUR', 'GBP']\n        if v not in valid_currencies:\n            raise ValueError(f'Currency must be one of {valid_currencies}')\n        return v\n\n```\n\n## Related Resources [¶](https://python.useinstructor.com/concepts/validation/?q=\\#related-resources \"Permanent link\")\n\n- [Pydantic Documentation](https://docs.pydantic.dev/)\n- [OpenAI Function Calling](https://platform.openai.com/docs/guides/gpt/function-calling)\n- [Instructor Examples](https://python.useinstructor.com/examples/)\n\n## Updates and Compatibility [¶](https://python.useinstructor.com/concepts/validation/?q=\\#updates-and-compatibility \"Permanent link\")\n\n- Works with all supported LLM providers\n- Compatible with latest Pydantic versions\n- Regular updates for new validation features\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/validation/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/validation/",
      "title": "Validation - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/validation/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/validation.png",
      "ogTitle": "Validation - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/validation.png",
      "og:title": "Validation - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/validation/?q=",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/validation.png",
      "twitter:title": "Validation - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/iterable/?q=#multi-task-and-streaming)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/iterable.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/iterable.md \"View source of this page\")\n\n# Multi-task and Streaming [¶](https://python.useinstructor.com/concepts/iterable/?q=\\#multi-task-and-streaming \"Permanent link\")\n\nA common use case of structured extraction is defining a single schema class and then making another schema to create a list to do multiple extraction\n\n```md-code__content\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nclass Users(BaseModel):\n    users: List[User]\n\nprint(Users.model_json_schema())\n\"\"\"\n{\n    '$defs': {\n        'User': {\n            'properties': {\n                'name': {'title': 'Name', 'type': 'string'},\n                'age': {'title': 'Age', 'type': 'integer'},\n            },\n            'required': ['name', 'age'],\n            'title': 'User',\n            'type': 'object',\n        }\n    },\n    'properties': {\n        'users': {'items': {'$ref': '#/$defs/User'}, 'title': 'Users', 'type': 'array'}\n    },\n    'required': ['users'],\n    'title': 'Users',\n    'type': 'object',\n}\n\"\"\"\n\n```\n\nDefining a task and creating a list of classes is a common enough pattern that we make this convenient by making use of `Iterable[T]`. This lets us dynamically create a new class that:\n\n1. Has dynamic docstrings and class name based on the task\n2. Support streaming by collecting tokens until a task is received back out.\n\n## Extracting Tasks using Iterable [¶](https://python.useinstructor.com/concepts/iterable/?q=\\#extracting-tasks-using-iterable \"Permanent link\")\n\nBy using `Iterable` you get a very convenient class with prompts and names automatically defined:\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nusers = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[User],\n    stream=False,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Consider this data: Jason is 10 and John is 30.\\\\\n                         Correctly segment it into entitites\\\\\n                        Make sure the JSON is correct\",\\\n        },\\\n    ],\n)\nfor user in users:\n    print(user)\n    #> name='Jason' age=10\n    #> name='John' age=30\n\n```\n\n## Streaming Tasks [¶](https://python.useinstructor.com/concepts/iterable/?q=\\#streaming-tasks \"Permanent link\")\n\nWe can also generate tasks as the tokens are streamed in by defining an `Iterable[T]` type.\n\nLets look at an example in action with the same class\n\n```md-code__content\nimport instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nusers = client.chat.completions.create(\n    model=\"gpt-4\",\n    temperature=0.1,\n    stream=True,\n    response_model=Iterable[User],\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"You are a perfect entity extraction system\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": (f\"Extract `Jason is 10 and John is 10`\"),\\\n        },\\\n    ],\n    max_tokens=1000,\n)\n\nfor user in users:\n    print(user)\n    #> name='Jason' age=10\n    #> name='John' age=10\n\n```\n\n## Asynchronous Streaming [¶](https://python.useinstructor.com/concepts/iterable/?q=\\#asynchronous-streaming \"Permanent link\")\n\nI also just want to call out in this example that `instructor` also supports asynchronous streaming. This is useful when you want to stream a response model and process the results as they come in, but you'll need to use the `async for` syntax to iterate over the results.\n\n```md-code__content\nimport instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\nasync def print_iterable_results():\n    model = await client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=Iterable[UserExtract],\n        max_retries=2,\n        stream=True,\n        messages=[\\\n            {\"role\": \"user\", \"content\": \"Make two up people\"},\\\n        ],\n    )\n    async for m in model:\n        print(m)\n        #> name='John Doe' age=27\n        #> name='Jane Smith' age=32\n\nimport asyncio\n\nasyncio.run(print_iterable_results())\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/iterable/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/iterable/",
      "title": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/iterable/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/iterable.png",
      "ogTitle": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/iterable.png",
      "og:title": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/iterable/?q=",
      "statusCode": 200,
      "description": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/iterable.png",
      "twitter:title": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "og:description": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/unions/?q=#working-with-union-types-in-instructor)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/unions.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/unions.md \"View source of this page\")\n\n# Working with Union Types in Instructor [¶](https://python.useinstructor.com/concepts/unions/?q=\\#working-with-union-types-in-instructor \"Permanent link\")\n\nThis guide explains how to work with union types in Instructor, allowing you to handle multiple possible response types from language models.\n\n## Basic Union Types [¶](https://python.useinstructor.com/concepts/unions/?q=\\#basic-union-types \"Permanent link\")\n\nUnion types let you specify that a field can be one of several types:\n\n```md-code__content\nfrom typing import Union\nfrom pydantic import BaseModel\n\nclass Response(BaseModel):\n    value: Union[str, int]  # Can be either string or integer\n\n```\n\n## Discriminated Unions [¶](https://python.useinstructor.com/concepts/unions/?q=\\#discriminated-unions \"Permanent link\")\n\nUse discriminated unions to handle different response types:\n\n```md-code__content\nfrom typing import Literal, Union\nfrom pydantic import BaseModel\n\nclass UserQuery(BaseModel):\n    type: Literal[\"user\"]\n    username: str\n\nclass SystemQuery(BaseModel):\n    type: Literal[\"system\"]\n    command: str\n\nQuery = Union[UserQuery, SystemQuery]\n\n# Usage with Instructor\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Query,\n    messages=[{\"role\": \"user\", \"content\": \"Parse: user lookup jsmith\"}],\n)\n\n```\n\n## Optional Fields [¶](https://python.useinstructor.com/concepts/unions/?q=\\#optional-fields \"Permanent link\")\n\nCombine Union with Optional for nullable fields:\n\n```md-code__content\nfrom typing import Optional\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    email: Optional[str] = None  # Same as Union[str, None]\n\n```\n\n## Best Practices [¶](https://python.useinstructor.com/concepts/unions/?q=\\#best-practices \"Permanent link\")\n\n1. **Type Hints**: Use proper type hints for clarity\n2. **Discriminators**: Add discriminator fields for complex unions\n3. **Validation**: Add validators for union fields\n4. **Documentation**: Document expected types clearly\n\n## Common Patterns [¶](https://python.useinstructor.com/concepts/unions/?q=\\#common-patterns \"Permanent link\")\n\n### Multiple Response Types [¶](https://python.useinstructor.com/concepts/unions/?q=\\#multiple-response-types \"Permanent link\")\n\n```md-code__content\nfrom typing import Union, Literal\nfrom pydantic import BaseModel\n\nclass SuccessResponse(BaseModel):\n    status: Literal[\"success\"]\n    data: dict\n\nclass ErrorResponse(BaseModel):\n    status: Literal[\"error\"]\n    message: str\n\nResponse = Union[SuccessResponse, ErrorResponse]\n\n```\n\n### Nested Unions [¶](https://python.useinstructor.com/concepts/unions/?q=\\#nested-unions \"Permanent link\")\n\n```md-code__content\nfrom typing import Union, List\nfrom pydantic import BaseModel\n\nclass TextContent(BaseModel):\n    type: Literal[\"text\"]\n    text: str\n\nclass ImageContent(BaseModel):\n    type: Literal[\"image\"]\n    url: str\n\nclass Message(BaseModel):\n    content: List[Union[TextContent, ImageContent]]\n\n```\n\n## Integration with Instructor [¶](https://python.useinstructor.com/concepts/unions/?q=\\#integration-with-instructor \"Permanent link\")\n\n### Validation with Unions [¶](https://python.useinstructor.com/concepts/unions/?q=\\#validation-with-unions \"Permanent link\")\n\n```md-code__content\nfrom instructor import patch\nfrom openai import OpenAI\n\nclient = patch(OpenAI())\n\ndef validate_response(response: Response) -> bool:\n    if isinstance(response, ErrorResponse):\n        return len(response.message) > 0\n    return True\n\nresult = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Response,\n    validation_hook=validate_response,\n    messages=[{\"role\": \"user\", \"content\": \"Process this request\"}],\n)\n\n```\n\n### Streaming with Unions [¶](https://python.useinstructor.com/concepts/unions/?q=\\#streaming-with-unions \"Permanent link\")\n\n```md-code__content\ndef stream_content():\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Message,\n        stream=True,\n        messages=[{\"role\": \"user\", \"content\": \"Generate mixed content\"}],\n    )\n    for partial in response:\n        if partial.content:\n            for item in partial.content:\n                if isinstance(item, TextContent):\n                    print(f\"Text: {item.text}\")\n                elif isinstance(item, ImageContent):\n                    print(f\"Image: {item.url}\")\n\n```\n\n## Error Handling [¶](https://python.useinstructor.com/concepts/unions/?q=\\#error-handling \"Permanent link\")\n\nHandle union type validation errors:\n\n```md-code__content\nfrom pydantic import ValidationError\n\ntry:\n    response = Response(status=\"invalid\", data={\"key\": \"value\"})  # Invalid status\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\n\n```\n\n## Type Checking [¶](https://python.useinstructor.com/concepts/unions/?q=\\#type-checking \"Permanent link\")\n\nUse isinstance() for runtime type checking:\n\n```md-code__content\ndef process_response(response: Response):\n    if isinstance(response, SuccessResponse):\n        # Handle success case\n        process_data(response.data)\n    elif isinstance(response, ErrorResponse):\n        # Handle error case\n        log_error(response.message)\n\n```\n\nFor more information about union types, check out the [Pydantic documentation on unions](https://docs.pydantic.dev/latest/concepts/types/#unions).\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/unions/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/unions/",
      "title": "Unions - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/unions/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/unions.png",
      "ogTitle": "Unions - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/unions.png",
      "og:title": "Unions - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/unions/?q=",
      "statusCode": 200,
      "description": "A lightweight library for structured outputs with LLMs.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "A lightweight library for structured outputs with LLMs.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/unions.png",
      "twitter:title": "Unions - Instructor",
      "og:description": "A lightweight library for structured outputs with LLMs.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "A lightweight library for structured outputs with LLMs."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/types/?q=#support-for-simple-types)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/types.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/types.md \"View source of this page\")\n\n# Support for Simple Types [¶](https://python.useinstructor.com/concepts/types/?q=\\#support-for-simple-types \"Permanent link\")\n\nAside from the recommended `pydantic.BaseModel`, and [Iterable](https://python.useinstructor.com/concepts/lists/), and [Partial](https://python.useinstructor.com/concepts/partial/),\n\nInstructor supports simple types like `str`, `int`, `float`, `bool`, `Union`, `Literal`, out of the box. You can use these types directly in your response models.\n\nTo add more descriptions you can also use `typing.Annotated` to include more information about the type.\n\n## What happens behind the scenes? [¶](https://python.useinstructor.com/concepts/types/?q=\\#what-happens-behind-the-scenes \"Permanent link\")\n\nWe will actually wrap the response model with a `pydantic.BaseModel` of the following form:\n\n```md-code__content\nfrom typing import Annotated\nfrom pydantic import create_model, Field, BaseModel\n\ntypehint = Annotated[bool, Field(description=\"Sample Description\")]\n\nmodel = create_model(\"Response\", content=(typehint, ...), __base__=BaseModel)\n\nprint(model.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'content': {\n            'description': 'Sample Description',\n            'title': 'Content',\n            'type': 'boolean',\n        }\n    },\n    'required': ['content'],\n    'title': 'Response',\n    'type': 'object',\n}\n\"\"\"\n\n```\n\n## Primitive Types (str, int, float, bool) [¶](https://python.useinstructor.com/concepts/types/?q=\\#primitive-types-str-int-float-bool \"Permanent link\")\n\n```md-code__content\nimport instructor\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n\n# Response model with simple types like str, int, float, bool\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=bool,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Is it true that Paris is the capital of France?\",\\\n        },\\\n    ],\n)\nassert resp is True, \"Paris is the capital of France\"\nprint(resp)\n#> True\n\n```\n\n## Annotated [¶](https://python.useinstructor.com/concepts/types/?q=\\#annotated \"Permanent link\")\n\nAnnotations can be used to add more information about the type. This can be useful for adding descriptions to the type, along with more complex information like field names, and more.\n\n```md-code__content\nimport instructor\nimport openai\nfrom typing import Annotated\nfrom pydantic import Field\n\nclient = instructor.from_openai(openai.OpenAI())\n\nUpperCaseStr = Annotated[str, Field(description=\"string must be upper case\")]\n\n# Response model with simple types like str, int, float, bool\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UpperCaseStr,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"What is the capital of france?\",\\\n        },\\\n    ],\n)\nassert resp == \"PARIS\", \"Paris is the capital of France\"\nprint(resp)\n#> PARIS\n\n```\n\n## Literal [¶](https://python.useinstructor.com/concepts/types/?q=\\#literal \"Permanent link\")\n\nWhen doing simple classification Literals go quite well, they support literal of string, int, bool.\n\n```md-code__content\nimport instructor\nimport openai\nfrom typing import Literal\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Literal[\"BILLING\", \"SHIPPING\"],\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Classify the following messages: 'I am having trouble with my billing'\",\\\n        },\\\n    ],\n)\nassert resp == \"BILLING\"\nprint(resp)\n#> BILLING\n\n```\n\n## Enum [¶](https://python.useinstructor.com/concepts/types/?q=\\#enum \"Permanent link\")\n\nEnums are harder to get right without some addition promping but are useful if these are values that are shared across the application.\n\n```md-code__content\nimport instructor\nimport openai\nfrom enum import Enum\n\nclass Label(str, Enum):\n    BILLING = \"BILLING\"\n    SHIPPING = \"SHIPPING\"\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Label,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Classify the following messages: 'I am having trouble with my billing'\",\\\n        },\\\n    ],\n)\nassert resp == Label.BILLING\nprint(resp)\n#> BILLING\n\n```\n\n## List [¶](https://python.useinstructor.com/concepts/types/?q=\\#list \"Permanent link\")\n\n```md-code__content\nimport instructor\nimport openai\nfrom typing import List\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=List[int],\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Give me the first 5 prime numbers\",\\\n        },\\\n    ],\n)\n\nassert resp == [2, 3, 5, 7, 11]\nprint(resp)\n#> [2, 3, 5, 7, 11]\n\n```\n\n## Union [¶](https://python.useinstructor.com/concepts/types/?q=\\#union \"Permanent link\")\n\nUnion is a great way to handle multiple types of responses, similar to multiple function calls but not limited to the function calling api, like in JSON\\_SCHEMA modes.\n\n```md-code__content\nimport instructor\nimport openai\nfrom pydantic import BaseModel\nfrom typing import Union\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass Add(BaseModel):\n    a: int\n    b: int\n\nclass Weather(BaseModel):\n    location: str\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Union[Add, Weather],\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"What is 5 + 5?\",\\\n        },\\\n    ],\n)\n\nassert resp == Add(a=5, b=5)\nprint(resp)\n#> a=5 b=5\n\n```\n\n## Complex Types [¶](https://python.useinstructor.com/concepts/types/?q=\\#complex-types \"Permanent link\")\n\n### Pandas DataFrame [¶](https://python.useinstructor.com/concepts/types/?q=\\#pandas-dataframe \"Permanent link\")\n\nThis is a more complex example, where we use a custom type to convert markdown to a pandas DataFrame.\n\n```md-code__content\nfrom io import StringIO\nfrom typing import Annotated, Any\nfrom pydantic import BeforeValidator, PlainSerializer, InstanceOf, WithJsonSchema\nimport pandas as pd\nimport instructor\nimport openai\n\ndef md_to_df(data: Any) -> Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\nMarkdownDataFrame = Annotated[\\\n    # Validates final type\\\n    InstanceOf[pd.DataFrame],\\\n    # Converts markdown to DataFrame\\\n    BeforeValidator(md_to_df),\\\n    # Converts DataFrame to markdown on model_dump_json\\\n    PlainSerializer(lambda df: df.to_markdown()),\\\n    # Adds a description to the type\\\n    WithJsonSchema(\\\n        {\\\n            \"type\": \"string\",\\\n            \"description\": \"\"\"\\\n            The markdown representation of the table,\\\n            each one should be tidy, do not try to join\\\n            tables that should be seperate\"\"\",\\\n        }\\\n    ),\\\n]\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=MarkdownDataFrame,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Jason is 20, Sarah is 30, and John is 40\",\\\n        },\\\n    ],\n)\n\nassert isinstance(resp, pd.DataFrame)\nprint(resp)\n\"\"\"\n        Age\n Name\nJason     20\nSarah     30\nJohn      40\n\"\"\"\n\n```\n\n### Lists of Unions [¶](https://python.useinstructor.com/concepts/types/?q=\\#lists-of-unions \"Permanent link\")\n\nJust like Unions we can use List of Unions to represent multiple types of responses. This will feel similar to the parallel function calls but not limited to the function calling api, like in JSON\\_SCHEMA modes.\n\n```md-code__content\nimport instructor\nimport openai\nfrom pydantic import BaseModel\nfrom typing import Union, List\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass Weather(BaseModel, frozen=True):\n    location: str\n\nclass Add(BaseModel, frozen=True):\n    a: int\n    b: int\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=List[Union[Add, Weather]],\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Add 5 and 5, and also whats the weather in Toronto?\",\\\n        },\\\n    ],\n)\n\nassert resp == [Add(a=5, b=5), Weather(location=\"Toronto\")]\nprint(resp)\n#> [Add(a=5, b=5), Weather(location='Toronto')]\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/types/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/types/",
      "title": "Support for Simple Types in Python with Pydantic - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/types/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/types.png",
      "ogTitle": "Support for Simple Types in Python with Pydantic - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/types.png",
      "og:title": "Support for Simple Types in Python with Pydantic - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/types/?q=",
      "statusCode": 200,
      "description": "Learn how to use simple types like str, int, float, and more in Python Pydantic models for effective API responses.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to use simple types like str, int, float, and more in Python Pydantic models for effective API responses.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/types.png",
      "twitter:title": "Support for Simple Types in Python with Pydantic - Instructor",
      "og:description": "Learn how to use simple types like str, int, float, and more in Python Pydantic models for effective API responses.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to use simple types like str, int, float, and more in Python Pydantic models for effective API responses."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/fields/?q=#default-values)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/fields.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/fields.md \"View source of this page\")\n\nThe `pydantic.Field` function is used to customize and add metadata to fields of models. To learn more, check out the Pydantic [documentation](https://docs.pydantic.dev/latest/concepts/fields/) as this is a near replica of that documentation that is relevant to prompting.\n\n## Default values [¶](https://python.useinstructor.com/concepts/fields/?q=\\#default-values \"Permanent link\")\n\nThe `default` parameter is used to define a default value for a field.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\n\nclass User(BaseModel):\n    name: str = Field(default='John Doe')\n\nuser = User()\nprint(user)\n#> name='John Doe'\n\n```\n\nYou can also use `default_factory` to define a callable that will be called to generate a default value.\n\n```md-code__content\nfrom uuid import uuid4\n\nfrom pydantic import BaseModel, Field\n\nclass User(BaseModel):\n    id: str = Field(default_factory=lambda: uuid4().hex)\n\n```\n\nInfo\n\nThe `default` and `default_factory` parameters are mutually exclusive.\n\nNote\n\nIf you use `typing.Optional`, it doesn't mean that the field has a default value of `None` you must use `default` or `default_factory` to define a default value. Then it will be considered `not required` when sent to the language model.\n\n## Using `Annotated` [¶](https://python.useinstructor.com/concepts/fields/?q=\\#using-annotated \"Permanent link\")\n\nThe `Field` function can also be used together with `Annotated`.\n\n```md-code__content\nfrom uuid import uuid4\nfrom typing_extensions import Annotated\nfrom pydantic import BaseModel, Field\n\nclass User(BaseModel):\n    id: Annotated[str, Field(default_factory=lambda: uuid4().hex)]\n\n```\n\n## Exclude [¶](https://python.useinstructor.com/concepts/fields/?q=\\#exclude \"Permanent link\")\n\nThe `exclude` parameter can be used to control which fields should be excluded from the model when exporting the model. This is helpful when you want to exclude fields that are not relevant to the model generation like `scratch_pad` or `chain_of_thought`\n\nSee the following example:\n\n```md-code__content\nfrom pydantic import BaseModel, Field\nfrom datetime import date\n\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Reasoning behind the date range.\", exclude=True\n    )\n    start_date: date\n    end_date: date\n\ndate_range = DateRange(\n    chain_of_thought=\"\"\"\n        I want to find the date range for the last 30 days.\n        Today is 2021-01-30 therefore the start date\n        should be 2021-01-01 and the end date is 2021-01-30\"\"\",\n    start_date=date(2021, 1, 1),\n    end_date=date(2021, 1, 30),\n)\nprint(date_range.model_dump_json())\n#> {\"start_date\":\"2021-01-01\",\"end_date\":\"2021-01-30\"}\n\n```\n\n## Omitting fields from schema sent to the language model [¶](https://python.useinstructor.com/concepts/fields/?q=\\#omitting-fields-from-schema-sent-to-the-language-model \"Permanent link\")\n\nIn some cases, you may wish to have the language model ignore certain fields in your model. You can do this by using Pydantic's `SkipJsonSchema` annotation. This omits a field from the JSON schema emitted by Pydantic (which `instructor` uses for constructing its prompts and tool definitions). For example:\n\n```md-code__content\nfrom pydantic import BaseModel\nfrom pydantic.json_schema import SkipJsonSchema\nfrom typing import Union\n\nclass Response(BaseModel):\n    question: str\n    answer: str\n    private_field: SkipJsonSchema[Union[str, None]] = None\n\nassert \"private_field\" not in Response.model_json_schema()[\"properties\"]\n\n```\n\nNote that because the language model will never return a value for `private_field`, you'll need a default value (this can be a generator via a declared Pydantic `Field`).\n\n## Customizing JSON Schema [¶](https://python.useinstructor.com/concepts/fields/?q=\\#customizing-json-schema \"Permanent link\")\n\nThere are some fields that are exclusively used to customise the generated JSON Schema:\n\n- `title`: The title of the field.\n- `description`: The description of the field.\n- `examples`: The examples of the field.\n- `json_schema_extra`: Extra JSON Schema properties to be added to the field.\n\nThese all work as great opportunities to add more information to the JSON schema as part of your prompt engineering.\n\nHere's an example:\n\n```md-code__content\nfrom pydantic import BaseModel, Field, SecretStr\n\nclass User(BaseModel):\n    age: int = Field(description='Age of the user')\n    name: str = Field(title='Username')\n    password: SecretStr = Field(\n        json_schema_extra={\n            'title': 'Password',\n            'description': 'Password of the user',\n            'examples': ['123456'],\n        }\n    )\n\nprint(User.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'age': {'description': 'Age of the user', 'title': 'Age', 'type': 'integer'},\n        'name': {'title': 'Username', 'type': 'string'},\n        'password': {\n            'description': 'Password of the user',\n            'examples': ['123456'],\n            'format': 'password',\n            'title': 'Password',\n            'type': 'string',\n            'writeOnly': True,\n        },\n    },\n    'required': ['age', 'name', 'password'],\n    'title': 'User',\n    'type': 'object',\n}\n\"\"\"\n\n```\n\n# General notes on JSON schema generation [¶](https://python.useinstructor.com/concepts/fields/?q=\\#general-notes-on-json-schema-generation \"Permanent link\")\n\n- The JSON schema for Optional fields indicates that the value null is allowed.\n- The Decimal type is exposed in JSON schema (and serialized) as a string.\n- The JSON schema does not preserve namedtuples as namedtuples.\n- When they differ, you can specify whether you want the JSON schema to represent the inputs to validation or the outputs from serialization.\n- Sub-models used are added to the `$defs` JSON attribute and referenced, as per the spec.\n- Sub-models with modifications (via the Field class) like a custom title, description, or default value, are recursively included instead of referenced.\n- The description for models is taken from either the docstring of the class or the argument description to the Field class.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/fields/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/fields/",
      "title": "Customizing Pydantic Models with Field Metadata - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/fields/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/fields.png",
      "ogTitle": "Customizing Pydantic Models with Field Metadata - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/fields.png",
      "og:title": "Customizing Pydantic Models with Field Metadata - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/fields/?q=",
      "statusCode": 200,
      "description": "Learn how to enhance Pydantic models with metadata using Field, including default values, JSON schema customization, and more.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to enhance Pydantic models with metadata using Field, including default values, JSON schema customization, and more.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/fields.png",
      "twitter:title": "Customizing Pydantic Models with Field Metadata - Instructor",
      "og:description": "Learn how to enhance Pydantic models with metadata using Field, including default values, JSON schema customization, and more.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to enhance Pydantic models with metadata using Field, including default values, JSON schema customization, and more."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/typeddicts/?q=#typeddicts)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/typeddicts.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/typeddicts.md \"View source of this page\")\n\n# TypedDicts [¶](https://python.useinstructor.com/concepts/typeddicts/?q=\\#typeddicts \"Permanent link\")\n\nWe also support typed dicts.\n\n```md-code__content\nfrom typing_extensions import TypedDict\nfrom openai import OpenAI\nimport instructor\n\nclass User(TypedDict):\n    name: str\n    age: int\n\nclient = instructor.from_openai(OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=User,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Timothy is a man from New York who is turning 32 this year\",\\\n        }\\\n    ],\n)\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/typeddicts/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/typeddicts/",
      "title": "Using TypedDicts with OpenAI API - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/typeddicts/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/typeddicts.png",
      "ogTitle": "Using TypedDicts with OpenAI API - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/typeddicts.png",
      "og:title": "Using TypedDicts with OpenAI API - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/typeddicts/?q=",
      "statusCode": 200,
      "description": "Learn how to utilize TypedDicts in Python with the OpenAI API for structured data responses.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to utilize TypedDicts in Python with the OpenAI API for structured data responses.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/typeddicts.png",
      "twitter:title": "Using TypedDicts with OpenAI API - Instructor",
      "og:description": "Learn how to utilize TypedDicts in Python with the OpenAI API for structured data responses.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to utilize TypedDicts in Python with the OpenAI API for structured data responses."
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/typeadapter.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/typeadapter.md \"View source of this page\")\n\n# Type Adapter\n\nThis page is a work in progress\n\nThis page is a work in progress. Check out [Pydantic's documentation](https://docs.pydantic.dev/latest/concepts/type_adapter/)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/typeadapter/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/typeadapter/",
      "title": "Pydantic Type Adapter Overview - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/typeadapter/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/typeadapter.png",
      "ogTitle": "Pydantic Type Adapter Overview - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/typeadapter.png",
      "og:title": "Pydantic Type Adapter Overview - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/typeadapter/?q=",
      "statusCode": 200,
      "description": "Explore the ongoing updates of Pydantic's Type Adapter concepts and access the official documentation.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Explore the ongoing updates of Pydantic's Type Adapter concepts and access the official documentation.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/typeadapter.png",
      "twitter:title": "Pydantic Type Adapter Overview - Instructor",
      "og:description": "Explore the ongoing updates of Pydantic's Type Adapter concepts and access the official documentation.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Explore the ongoing updates of Pydantic's Type Adapter concepts and access the official documentation."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/prompting/?q=#general-tips-for-prompt-engineering)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/prompting.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/prompting.md \"View source of this page\")\n\n# General Tips for Prompt Engineering [¶](https://python.useinstructor.com/concepts/prompting/?q=\\#general-tips-for-prompt-engineering \"Permanent link\")\n\nThe overarching theme of using Instructor and Pydantic for function calling is to make the models as self-descriptive, modular, and flexible as possible, while maintaining data integrity and ease of use.\n\n- **Modularity**: Design self-contained components for reuse.\n- **Self-Description**: Use Pydantic's `Field` for clear field descriptions.\n- **Optionality**: Use Python's `Optional` type for nullable fields and set sensible defaults.\n- **Standardization**: Employ enumerations for fields with a fixed set of values; include a fallback option.\n- **Dynamic Data**: Use key-value pairs for arbitrary properties and limit list lengths.\n- **Entity Relationships**: Define explicit identifiers and relationship fields.\n- **Contextual Logic**: Optionally add a \"chain of thought\" field in reusable components for extra context.\n\n## Modular Chain of Thought [¶](https://python.useinstructor.com/concepts/prompting/?q=\\#chain-of-thought \"Permanent link\")\n\nThis approach to \"chain of thought\" improves data quality but can have modular components rather than global CoT.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\n\nclass Role(BaseModel):\n    chain_of_thought: str = Field(\n        ..., description=\"Think step by step to determine the correct title\"\n    )\n    title: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n\n```\n\n## Utilize Optional Attributes [¶](https://python.useinstructor.com/concepts/prompting/?q=\\#utilize-optional-attributes \"Permanent link\")\n\nUse Python's Optional type and set a default value to prevent undesired defaults like empty strings.\n\n```md-code__content\nfrom typing import Optional\nfrom pydantic import BaseModel, Field\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\n```\n\n## Handling Errors Within Function Calls [¶](https://python.useinstructor.com/concepts/prompting/?q=\\#handling-errors-within-function-calls \"Permanent link\")\n\nYou can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None\n\n```\n\nWith the `MaybeUser` class, you can either receive a `UserDetail` object in result or get an error message in message.\n\n### Simplification with the Maybe Pattern [¶](https://python.useinstructor.com/concepts/prompting/?q=\\#simplification-with-the-maybe-pattern \"Permanent link\")\n\nYou can further simplify this using instructor to create the `Maybe` pattern dynamically from any `BaseModel`.\n\n```md-code__content\nimport instructor\nfrom pydantic import BaseModel\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n\nMaybeUser = instructor.Maybe(UserDetail)\n\n```\n\nThis allows you to quickly create a Maybe type for any class, streamlining the process.\n\n## Tips for Enumerations [¶](https://python.useinstructor.com/concepts/prompting/?q=\\#tips-for-enumerations \"Permanent link\")\n\nTo prevent data misalignment, use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.\n\n```md-code__content\nfrom enum import Enum, auto\nfrom pydantic import BaseModel, Field\n\nclass Role(Enum):\n    PRINCIPAL = auto()\n    TEACHER = auto()\n    STUDENT = auto()\n    OTHER = auto()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(\n        description=\"Correctly assign one of the predefined roles to the user.\"\n    )\n\n```\n\n## Literals [¶](https://python.useinstructor.com/concepts/prompting/?q=\\#literals \"Permanent link\")\n\nIf you're having a hard time with `Enum` an alternative is to use `Literal`\n\n```md-code__content\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n\n```\n\nIf you'd like to improve performance more you can reiterate the requirements in the field descriptions or in the docstrings.\n\n## Reiterate Long Instructions [¶](https://python.useinstructor.com/concepts/prompting/?q=\\#reiterate-long-instructions \"Permanent link\")\n\nFor complex attributes, it helps to reiterate the instructions in the field's description.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\n\nclass Role(BaseModel):\n    \"\"\"\n    Extract the role based on the following rules ...\n    \"\"\"\n\n    instructions: str = Field(\n        ...,\n        description=\"Restate the instructions and rules to correctly determine the title.\",\n    )\n    title: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n\n```\n\n## Handle Arbitrary Properties [¶](https://python.useinstructor.com/concepts/prompting/?q=\\#handle-arbitrary-properties \"Permanent link\")\n\nWhen you need to extract undefined attributes, use a list of key-value pairs.\n\n```md-code__content\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\nclass Property(BaseModel):\n    key: str\n    value: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(\n        ..., description=\"Extract any other properties that might be relevant.\"\n    )\n\n```\n\n## Limiting the Length of Lists [¶](https://python.useinstructor.com/concepts/prompting/?q=\\#limiting-the-length-of-lists \"Permanent link\")\n\nWhen dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.\n\n```md-code__content\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\nclass Property(BaseModel):\n    index: str = Field(..., description=\"Monotonically increasing ID\")\n    key: str\n    value: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be less than 6\",\n    )\n\n```\n\n**Using Tuples for Simple Types**\n\nFor simple types, tuples can be a more compact alternative to custom classes, especially when the properties don't require additional descriptions.\n\n```md-code__content\nfrom typing import List, Tuple\nfrom pydantic import BaseModel, Field\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Tuple[int, str]] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be less than 6\",\n    )\n\n```\n\n## Advanced Arbitrary Properties [¶](https://python.useinstructor.com/concepts/prompting/?q=\\#advanced-arbitrary-properties \"Permanent link\")\n\nFor multiple users, aim to use consistent key names when extracting properties.\n\n```md-code__content\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n\nclass UserDetails(BaseModel):\n    \"\"\"\n    Extract information for multiple users.\n    Use consistent key names for properties across users.\n    \"\"\"\n\n    users: List[UserDetail]\n\n```\n\nThis refined guide should offer a cleaner and more organized approach to structure engineering in Python.\n\n## Defining Relationships Between Entities [¶](https://python.useinstructor.com/concepts/prompting/?q=\\#defining-relationships-between-entities \"Permanent link\")\n\nIn cases where relationships exist between entities, it's vital to define them explicitly in the model. The following example demonstrates how to define relationships between users by incorporating an id and a friends field:\n\n```md-code__content\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    friends: List[int] = Field(\n        ...,\n        description=\"Correct and complete list of friend IDs, representing relationships between users.\",\n    )\n\nclass UserRelationships(BaseModel):\n    users: List[UserDetail] = Field(\n        ...,\n        description=\"Collection of users, correctly capturing the relationships among them.\",\n    )\n\n```\n\n## Reusing Components with Different Contexts [¶](https://python.useinstructor.com/concepts/prompting/?q=\\#reusing-components-with-different-contexts \"Permanent link\")\n\nYou can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both work\\_time and leisure\\_time.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\n\nclass TimeRange(BaseModel):\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    work_time: TimeRange = Field(\n        ..., description=\"Time range during which the user is working.\"\n    )\n    leisure_time: TimeRange = Field(\n        ..., description=\"Time range reserved for leisure activities.\"\n    )\n\n```\n\nSometimes, a component like TimeRange may require some context or additional logic to be used effectively. Employing a \"chain of thought\" field within the component can help in understanding or optimizing the time range allocations.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\n\nclass TimeRange(BaseModel):\n    chain_of_thought: str = Field(\n        ..., description=\"Step by step reasoning to get the correct time range\"\n    )\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/prompting/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/prompting/",
      "title": "Effective Prompt Engineering with Pydantic and Instructor - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/prompting/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/prompting.png",
      "ogTitle": "Effective Prompt Engineering with Pydantic and Instructor - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/prompting.png",
      "og:title": "Effective Prompt Engineering with Pydantic and Instructor - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/prompting/?q=",
      "statusCode": 200,
      "description": "Discover best practices for prompt engineering using Pydantic and Instructor to enhance modularity, flexibility, and data integrity.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Discover best practices for prompt engineering using Pydantic and Instructor to enhance modularity, flexibility, and data integrity.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/prompting.png",
      "twitter:title": "Effective Prompt Engineering with Pydantic and Instructor - Instructor",
      "og:description": "Discover best practices for prompt engineering using Pydantic and Instructor to enhance modularity, flexibility, and data integrity.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Discover best practices for prompt engineering using Pydantic and Instructor to enhance modularity, flexibility, and data integrity."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/partial/?q=#streaming-partial-responses)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/partial.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/partial.md \"View source of this page\")\n\n# Streaming Partial Responses [¶](https://python.useinstructor.com/concepts/partial/?q=\\#streaming-partial-responses \"Permanent link\")\n\nLiteral\n\nIf the data structure you're using has literal values, you need to make sure to import the `PartialLiteralMixin` mixin.\n\n```md-code__content\nfrom instructor.dsl.partial import PartialLiteralMixin\n\nclass User(BaseModel, PartialLiteralMixin):\n    name: str\n    age: int\n    category: Literal[\"admin\", \"user\", \"guest\"]\n\n// The rest of your code below\n\n```\n\nThis is because `jiter` throws an error otherwise if it encounters a incomplete Literal value while it's being streamed in\n\nField level streaming provides incremental snapshots of the current state of the response model that are immediately useable. This approach is particularly relevant in contexts like rendering UI components.\n\nInstructor supports this pattern by making use of `create_partial`. This lets us dynamically create a new class that treats all of the original model's fields as `Optional`.\n\n## Understanding Partial Responses [¶](https://python.useinstructor.com/concepts/partial/?q=\\#understanding-partial-responses \"Permanent link\")\n\nConsider what happens whene we define a response model:\n\n```md-code__content\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n```\n\nIf we streamed json out from OpenAI, we would only be able to parse when the object is completed returned!\n\n```md-code__content\n{\"name\": \"Jo\n{\"name\": \"John\", \"ag\n{\"name\": \"John\", \"age\":\n{\"name\": \"John\", \"age\": 25} # Completed\n\n```\n\nWhen specifying a `create_partial` and setting `stream=True`, the response from `instructor` becomes a `Generator[T]`. As the generator yields results, you can iterate over these incremental updates. The last value yielded by the generator represents the completed extraction!\n\n```md-code__content\n{\"name\": \"Jo                 => User(name=\"Jo\", age=None)\n{\"name\": \"John\", \"ag         => User(name=\"John\", age=None)\n{\"name\": \"John\", \"age:       => User(name=\"John\", age=None)\n{\"name\": \"John\", \"age\": 25}  => User(name=\"John\", age=25)\n\n```\n\nLimited Validator Support\n\nDue to the streaming nature of the response model, we do not support validators since they would not be able to be applied to the streaming response.\n\nLet's look at an example of streaming an extraction of conference information, that would be used to stream in an react component.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import List\nfrom rich.console import Console\n\nclient = instructor.from_openai(OpenAI())\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss the upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024, at the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher, will be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities. Each participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meetingis scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\nextraction_stream = client.chat.completions.create_partial(\n    model=\"gpt-4\",\n    response_model=MeetingInfo,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\\\n        },\\\n    ],\n    stream=True,\n)\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n\nprint(extraction.model_dump_json(indent=2))\n\"\"\"\n{\n  \"users\": [\\\n    {\\\n      \"name\": \"John Doe\",\\\n      \"email\": \"johndoe@email.com\",\\\n      \"twitter\": \"@TechGuru44\"\\\n    },\\\n    {\\\n      \"name\": \"Jane Smith\",\\\n      \"email\": \"janesmith@email.com\",\\\n      \"twitter\": \"@DigitalDiva88\"\\\n    },\\\n    {\\\n      \"name\": \"Alex Johnson\",\\\n      \"email\": \"alexj@email.com\",\\\n      \"twitter\": \"@CodeMaster2023\"\\\n    }\\\n  ],\n  \"date\": \"2024-03-15\",\n  \"location\": \"Grand Tech Arena located at 4521 Innovation Drive\",\n  \"budget\": 50000,\n  \"deadline\": \"2024-02-20\"\n}\n\"\"\"\n\n```\n\nThis will output the following:\n\n![Partial Streaming Gif](https://python.useinstructor.com/img/partial.gif)\n\n## Asynchronous Streaming [¶](https://python.useinstructor.com/concepts/partial/?q=\\#asynchronous-streaming \"Permanent link\")\n\nI also just want to call out in this example that `instructor` also supports asynchronous streaming. This is useful when you want to stream a response model and process the results as they come in, but you'll need to use the `async for` syntax to iterate over the results.\n\n```md-code__content\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(AsyncOpenAI())\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nasync def print_partial_results():\n    user = client.chat.completions.create_partial(\n        model=\"gpt-4-turbo-preview\",\n        response_model=User,\n        max_retries=2,\n        stream=True,\n        messages=[\\\n            {\"role\": \"user\", \"content\": \"Jason is 12 years old\"},\\\n        ],\n    )\n    async for m in user:\n        print(m)\n        #> name=None age=None\n        #> name=None age=None\n        #> name=None age=None\n        #> name=None age=12\n        #> name=None age=12\n        #> name=None age=12\n        #> name='' age=12\n        #> name='Jason' age=12\n        #> name='Jason' age=12\n\nimport asyncio\n\nasyncio.run(print_partial_results())\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/partial/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/partial/",
      "title": "Streaming Partial Responses with Instructor and OpenAI - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/partial/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/partial.png",
      "ogTitle": "Streaming Partial Responses with Instructor and OpenAI - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/partial.png",
      "og:title": "Streaming Partial Responses with Instructor and OpenAI - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/partial/?q=",
      "statusCode": 200,
      "description": "Learn to utilize field-level streaming with Instructor and OpenAI for incremental responses in Python.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn to utilize field-level streaming with Instructor and OpenAI for incremental responses in Python.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/partial.png",
      "twitter:title": "Streaming Partial Responses with Instructor and OpenAI - Instructor",
      "og:description": "Learn to utilize field-level streaming with Instructor and OpenAI for incremental responses in Python.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn to utilize field-level streaming with Instructor and OpenAI for incremental responses in Python."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/maybe/?q=#handling-missing-data)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/maybe.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/maybe.md \"View source of this page\")\n\n# Handling Missing Data [¶](https://python.useinstructor.com/concepts/maybe/?q=\\#handling-missing-data \"Permanent link\")\n\nThe `Maybe` pattern is a concept in functional programming used for error handling. Instead of raising exceptions or returning `None`, you can use a `Maybe` type to encapsulate both the result and potential errors.\n\nThis pattern is particularly useful when making LLM calls, as providing language models with an escape hatch can effectively reduce hallucinations.\n\n## Defining the Model [¶](https://python.useinstructor.com/concepts/maybe/?q=\\#defining-the-model \"Permanent link\")\n\nUsing Pydantic, we'll first define the `UserDetail` and `MaybeUser` classes.\n\n```md-code__content\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n\n    def __bool__(self):\n        return self.result is not None\n\n```\n\nNotice that `MaybeUser` has a `result` field that is an optional `UserDetail` instance where the extracted data will be stored. The `error` field is a boolean that indicates whether an error occurred, and the `message` field is an optional string that contains the error message.\n\n## Defining the function [¶](https://python.useinstructor.com/concepts/maybe/?q=\\#defining-the-function \"Permanent link\")\n\nOnce we have the model defined, we can create a function that uses the `Maybe` pattern to extract the data.\n\n```md-code__content\nimport instructor\nimport openai\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\n# This enables the `response_model` keyword\nclient = instructor.from_openai(openai.OpenAI())\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n\n    def __bool__(self):\n        return self.result is not None\n\ndef extract(content: str) -> MaybeUser:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MaybeUser,\n        messages=[\\\n            {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},\\\n        ],\n    )\n\nuser1 = extract(\"Jason is a 25-year-old scientist\")\nprint(user1.model_dump_json(indent=2))\n\"\"\"\n{\n  \"result\": {\n    \"age\": 25,\n    \"name\": \"Jason\",\n    \"role\": \"scientist\"\n  },\n  \"error\": false,\n  \"message\": null\n}\n\"\"\"\n\nuser2 = extract(\"Unknown user\")\nprint(user2.model_dump_json(indent=2))\n\"\"\"\n{\n  \"result\": null,\n  \"error\": false,\n  \"message\": null\n}\n\"\"\"\n\n```\n\nAs you can see, when the data is extracted successfully, the `result` field contains the `UserDetail` instance. When an error occurs, the `error` field is set to `True`, and the `message` field contains the error message.\n\nIf you want to learn more about pattern matching, check out Pydantic's docs on [Structural Pattern Matching](https://docs.pydantic.dev/latest/concepts/models/#structural-pattern-matching)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/maybe/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/maybe/",
      "title": "Implementing the Maybe Pattern for Error Handling in Functional Programming - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/maybe/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/maybe.png",
      "ogTitle": "Implementing the Maybe Pattern for Error Handling in Functional Programming - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/maybe.png",
      "og:title": "Implementing the Maybe Pattern for Error Handling in Functional Programming - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/maybe/?q=",
      "statusCode": 200,
      "description": "Learn how to use the Maybe pattern with Pydantic in Python for robust error handling when extracting user details.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to use the Maybe pattern with Pydantic in Python for robust error handling when extracting user details.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/maybe.png",
      "twitter:title": "Implementing the Maybe Pattern for Error Handling in Functional Programming - Instructor",
      "og:description": "Learn how to use the Maybe pattern with Pydantic in Python for robust error handling when extracting user details.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to use the Maybe pattern with Pydantic in Python for robust error handling when extracting user details."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/lists/?q=#multi-task-and-streaming)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/lists.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/lists.md \"View source of this page\")\n\n# Multi-task and Streaming [¶](https://python.useinstructor.com/concepts/lists/?q=\\#multi-task-and-streaming \"Permanent link\")\n\nA common use case of structured extraction is defining a single schema class and then making another schema to create a list to do multiple extraction\n\n```md-code__content\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nclass Users(BaseModel):\n    users: List[User]\n\nprint(Users.model_json_schema())\n\"\"\"\n{\n    '$defs': {\n        'User': {\n            'properties': {\n                'name': {'title': 'Name', 'type': 'string'},\n                'age': {'title': 'Age', 'type': 'integer'},\n            },\n            'required': ['name', 'age'],\n            'title': 'User',\n            'type': 'object',\n        }\n    },\n    'properties': {\n        'users': {'items': {'$ref': '#/$defs/User'}, 'title': 'Users', 'type': 'array'}\n    },\n    'required': ['users'],\n    'title': 'Users',\n    'type': 'object',\n}\n\"\"\"\n\n```\n\nDefining a task and creating a list of classes is a common enough pattern that we make this convenient by making use of `Iterable[T]`. This lets us dynamically create a new class that:\n\n1. Has dynamic docstrings and class name based on the task\n2. Support streaming by collecting tokens until a task is received back out.\n\n## Extracting Tasks using Iterable [¶](https://python.useinstructor.com/concepts/lists/?q=\\#extracting-tasks-using-iterable \"Permanent link\")\n\nBy using `Iterable` you get a very convenient class with prompts and names automatically defined:\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nusers = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[User],\n    stream=False,\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Consider this data: Jason is 10 and John is 30.\\\\\n                         Correctly segment it into entitites\\\\\n                        Make sure the JSON is correct\",\\\n        },\\\n    ],\n)\nfor user in users:\n    print(user)\n    #> name='Jason' age=10\n    #> name='John' age=30\n\n```\n\n## Streaming Tasks [¶](https://python.useinstructor.com/concepts/lists/?q=\\#streaming-tasks \"Permanent link\")\n\nWe can also generate tasks as the tokens are streamed in by defining an `Iterable[T]` type.\n\nLets look at an example in action with the same class\n\n```md-code__content\nimport instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nusers = client.chat.completions.create(\n    model=\"gpt-4\",\n    temperature=0.1,\n    stream=True,\n    response_model=Iterable[User],\n    messages=[\\\n        {\\\n            \"role\": \"system\",\\\n            \"content\": \"You are a perfect entity extraction system\",\\\n        },\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": (f\"Extract `Jason is 10 and John is 10`\"),\\\n        },\\\n    ],\n    max_tokens=1000,\n)\n\nfor user in users:\n    print(user)\n    #> name='Jason' age=10\n    #> name='John' age=10\n\n```\n\n## Asynchronous Streaming [¶](https://python.useinstructor.com/concepts/lists/?q=\\#asynchronous-streaming \"Permanent link\")\n\nI also just want to call out in this example that `instructor` also supports asynchronous streaming. This is useful when you want to stream a response model and process the results as they come in, but you'll need to use the `async for` syntax to iterate over the results.\n\n```md-code__content\nimport instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\nasync def print_iterable_results():\n    model = await client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=Iterable[UserExtract],\n        max_retries=2,\n        stream=True,\n        messages=[\\\n            {\"role\": \"user\", \"content\": \"Make two up people\"},\\\n        ],\n    )\n    async for m in model:\n        print(m)\n        #> name='John Doe' age=34\n        #> name='Jane Smith' age=27\n\nimport asyncio\n\nasyncio.run(print_iterable_results())\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/lists/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/lists/",
      "title": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/lists/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/lists.png",
      "ogTitle": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/lists.png",
      "og:title": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/lists/?q=",
      "statusCode": 200,
      "description": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/lists.png",
      "twitter:title": "Extracting Structured Data with Iterable and Streaming in Python - Instructor",
      "og:description": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn to use Iterable and streaming for structured data extraction with Pydantic and OpenAI in Python."
    }
  },
  {
    "markdown": "[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/alias.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/alias.md \"View source of this page\")\n\n# Alias\n\nThis page is a work in progress\n\nThis page is a work in progress. Check out [Pydantic's documentation](https://docs.pydantic.dev/latest/concepts/alias/)\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/alias/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/alias/",
      "title": "Pydantic Aliases Overview - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/alias/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/alias.png",
      "ogTitle": "Pydantic Aliases Overview - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/alias.png",
      "og:title": "Pydantic Aliases Overview - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/alias/?q=",
      "statusCode": 200,
      "description": "Explore the concept of aliases in Pydantic. Discover the latest documentation and features for better data validation.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Explore the concept of aliases in Pydantic. Discover the latest documentation and features for better data validation.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/alias.png",
      "twitter:title": "Pydantic Aliases Overview - Instructor",
      "og:description": "Explore the concept of aliases in Pydantic. Discover the latest documentation and features for better data validation.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Explore the concept of aliases in Pydantic. Discover the latest documentation and features for better data validation."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/reask_validation/?q=#validation-and-reasking)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/reask_validation.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/reask_validation.md \"View source of this page\")\n\n# Validation and Reasking [¶](https://python.useinstructor.com/concepts/reask_validation/?q=\\#validation-and-reasking \"Permanent link\")\n\nInstead of framing \"self-critique\" or \"self-reflection\" in AI as new concepts, we can view them as validation errors with clear error messages that the system can use to self-correct.\n\n## Pydantic [¶](https://python.useinstructor.com/concepts/reask_validation/?q=\\#pydantic \"Permanent link\")\n\nPydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the [Pydantic docs](https://docs.pydantic.dev/latest/concepts/validators/) on validators.\n\nGood llm validation is just good validation\n\nIf you want to see some more examples on validators checkout our blog post [Good LLM validation is just good validation](https://jxnl.github.io/instructor/blog/2023/10/23/good-llm-validation-is-just-good-validation/)\n\n### Code-based Validation Example [¶](https://python.useinstructor.com/concepts/reask_validation/?q=\\#code-based-validation-example \"Permanent link\")\n\nFirst define a Pydantic model with a validator using the `Annotation` class from `typing_extensions`.\n\nEnforce a naming rule using Pydantic's built-in validation:\n\n```md-code__content\nfrom pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated\nfrom pydantic import AfterValidator\n\ndef name_must_contain_space(v: str) -> str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\ntry:\n    person = UserDetail(age=29, name=\"Jason\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n\n```\n\n#### Output for Code-Based Validation [¶](https://python.useinstructor.com/concepts/reask_validation/?q=\\#output-for-code-based-validation \"Permanent link\")\n\n```md-code__content\n1 validation error for UserDetail\nname\n   Value error, name must contain a space (type=value_error)\n\n```\n\nAs we can see, Pydantic raises a validation error when the name attribute does not contain a space. This is a simple example, but it demonstrates how Pydantic can be used to validate attributes of a model.\n\n### LLM-Based Validation Example [¶](https://python.useinstructor.com/concepts/reask_validation/?q=\\#llm-based-validation-example \"Permanent link\")\n\nLLM-based validation can also be plugged into the same Pydantic model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error.\n\n```md-code__content\nimport instructor\nfrom openai import OpenAI\nfrom instructor import llm_validator\nfrom pydantic import BaseModel, ValidationError, BeforeValidator\nfrom typing_extensions import Annotated\n\n# Apply the patch to the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\\\n        str,\\\n        BeforeValidator(llm_validator(\"don't say objectionable things\", client=client)),\\\n    ]\n\ntry:\n    qa = QuestionAnswer(\n        question=\"What is the meaning of life?\",\n        answer=\"The meaning of life is to be evil and steal\",\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for QuestionAnswer\n    answer\n      Assertion failed, The statement promotes objectionable behavior by encouraging evil and stealing. [type=assertion_error, input_value='The meaning of life is to be evil and steal', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/assertion_error\n    \"\"\"\n\n```\n\n#### Output for LLM-Based Validation [¶](https://python.useinstructor.com/concepts/reask_validation/?q=\\#output-for-llm-based-validation \"Permanent link\")\n\nIt is important to note here that the error message is generated by the LLM, not the code, so it'll be helpful for re-asking the model.\n\n```md-code__content\n1 validation error for QuestionAnswer\nanswer\n   Assertion failed, The statement is objectionable. (type=assertion_error)\n\n```\n\n## Using Reasking Logic to Correct Outputs [¶](https://python.useinstructor.com/concepts/reask_validation/?q=\\#using-reasking-logic-to-correct-outputs \"Permanent link\")\n\nValidators are a great tool for ensuring some property of the outputs. When you use the `patch()` method with the `openai` client, you can use the `max_retries` parameter to set the number of times you can reask the model to correct the output.\n\nIt is a great layer of defense against bad outputs of two forms:\n\n1. Pydantic Validation Errors (code or llm based)\n2. JSON Decoding Errors (when the model returns a bad response)\n\n### Step 1: Define the Response Model with Validators [¶](https://python.useinstructor.com/concepts/reask_validation/?q=\\#step-1-define-the-response-model-with-validators \"Permanent link\")\n\nNotice that the field validator wants the name in uppercase, but the user input is lowercase. The validator will raise a `ValueError` if the name is not in uppercase.\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel, field_validator\n\n# Apply the patch to the OpenAI client\nclient = instructor.from_openai(openai.OpenAI())\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n\n```\n\n### Step 2. Using the Client with Retries [¶](https://python.useinstructor.com/concepts/reask_validation/?q=\\#step-2-using-the-client-with-retries \"Permanent link\")\n\nHere, the `UserDetails` model is passed as the `response_model`, and `max_retries` is set to 2.\n\n```md-code__content\nimport instructor\nimport openai\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\nmodel = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetails,\n    max_retries=2,\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\\\n    ],\n)\n\nprint(model.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Jason\",\n  \"age\": 25\n}\n\"\"\"\n\n```\n\n### What happens behind the scenes? [¶](https://python.useinstructor.com/concepts/reask_validation/?q=\\#what-happens-behind-the-scenes \"Permanent link\")\n\nBehind the scenes, the `instructor.from_openai()` method adds a `max_retries` parameter to the `openai.ChatCompletion.create()` method. The `max_retries` parameter will trigger up to 2 reattempts if the `name` attribute fails the uppercase validation in `UserDetails`.\n\n```md-code__content\nfrom pydantic import ValidationError\n\ntry:\n    ...\nexcept ValidationError as e:\n    kwargs[\"messages\"].append(response.choices[0].message)\n    kwargs[\"messages\"].append(\n        {\n            \"role\": \"user\",\n            \"content\": f\"Please correct the function call; errors encountered:\\n{e}\",\n        }\n    )\n\n```\n\n## Advanced Validation Techniques [¶](https://python.useinstructor.com/concepts/reask_validation/?q=\\#advanced-validation-techniques \"Permanent link\")\n\nThe docs are currently incomplete, but we have a few advanced validation techniques that we're working on documenting better such as model level validation, and using a validation context. Check out our example on [verifying citations](https://python.useinstructor.com/examples/exact_citations/) which covers:\n\n1. Validate the entire object with all attributes rather than one attribute at a time\n2. Using some 'context' to validate the object: In this case, we use the `context` to check if the citation existed in the original text.\n\n## Optimizing Token usage [¶](https://python.useinstructor.com/concepts/reask_validation/?q=\\#optimizing-token-usage \"Permanent link\")\n\nPydantic automatically includes a URL within the error message itself when an error is thrown so that users can learn more about the specific error that was thrown. Some users might want to remove this URL since it adds extra tokens that otherwise might not add much value to the validation process.\n\nWe've created a small helper function that you can use below which removes this url in the error message\n\n```md-code__content\nfrom instructor.utils import disable_pydantic_error_url\nfrom pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated\nfrom pydantic import AfterValidator\n\ndisable_pydantic_error_url()\n\ndef name_must_contain_space(v: str) -> str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\ntry:\n    person = UserDetail(age=29, name=\"Jason\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n\n```\n\n## Takeaways [¶](https://python.useinstructor.com/concepts/reask_validation/?q=\\#takeaways \"Permanent link\")\n\nBy integrating these advanced validation techniques, we not only improve the quality and reliability of LLM-generated content, but also pave the way for more autonomous and effective systems.\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/reask_validation/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/reask_validation/",
      "title": "Enhancing AI Validations with Pydantic's Framework - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/reask_validation/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/reask_validation.png",
      "ogTitle": "Enhancing AI Validations with Pydantic's Framework - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/reask_validation.png",
      "og:title": "Enhancing AI Validations with Pydantic's Framework - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/reask_validation/?q=",
      "statusCode": 200,
      "description": "Learn how to improve AI outputs using Pydantic for validation and reasking techniques.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to improve AI outputs using Pydantic for validation and reasking techniques.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/reask_validation.png",
      "twitter:title": "Enhancing AI Validations with Pydantic's Framework - Instructor",
      "og:description": "Learn how to improve AI outputs using Pydantic for validation and reasking techniques.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to improve AI outputs using Pydantic for validation and reasking techniques."
    }
  },
  {
    "markdown": "[Skip to content](https://python.useinstructor.com/concepts/retrying/?q=#retrying)\n\n[Edit this page](https://github.com/jxnl/instructor/edit/main/docs/concepts/retrying.md \"Edit this page\") [View source of this page](https://github.com/jxnl/instructor/raw/main/docs/concepts/retrying.md \"View source of this page\")\n\n# Retrying [¶](https://python.useinstructor.com/concepts/retrying/?q=\\#retrying \"Permanent link\")\n\nOne of the benefits of having Pydantic is the ease with which we can define validators. We cover this topic in many articles, like [Reasking Validation](https://python.useinstructor.com/concepts/reask_validation/) and in our blog post [Good LLM validation is just good validation](https://python.useinstructor.com/blog/2023/10/23/good-llm-validation-is-just-good-validation/).\n\nThis post will mostly describe how to use simple and more complex retry and logic.\n\n## Example of a Validator [¶](https://python.useinstructor.com/concepts/retrying/?q=\\#example-of-a-validator \"Permanent link\")\n\nBefore we begin, we'll use a simple example of a validator. One that checks that the name is in all caps. While we could obviously prompt that we want the name in all caps, this serves as an example of how we can build in additional logic without changing our prompts.\n\n```md-code__content\nfrom typing import Annotated\nfrom pydantic import AfterValidator, BaseModel\n\ndef uppercase_validator(v):\n    if v.islower():\n        raise ValueError(\"Name must be ALL CAPS\")\n    return v\n\nclass UserDetail(BaseModel):\n    name: Annotated[str, AfterValidator(uppercase_validator)]\n    age: int\n\ntry:\n    UserDetail(name=\"jason\", age=12)\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must be ALL CAPS [type=value_error, input_value='jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n\n```\n\n## Simple: Max Retries [¶](https://python.useinstructor.com/concepts/retrying/?q=\\#simple-max-retries \"Permanent link\")\n\nThe simplest way to set up retries is to assign an integer value to `max_retries`.\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\\\n    ],\n    max_retries=3,\n)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n\n```\n\n## Catching Retry Exceptions [¶](https://python.useinstructor.com/concepts/retrying/?q=\\#catching-retry-exceptions \"Permanent link\")\n\nIf you want to catch the retry exceptions, you can do so and access the `last_completion`, `n_attempts` and `messages` attributes.\n\n```md-code__content\nfrom pydantic import BaseModel, field_validator\nimport openai\nimport instructor\nfrom instructor.exceptions import InstructorRetryException\nfrom tenacity import Retrying, retry_if_not_exception_type, stop_after_attempt\n\n# Patch the OpenAI client to enable response_model\nclient = instructor.from_openai(openai.OpenAI())\n\n# Define a Pydantic model for the user details\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"age\")\n    def validate_age(cls, v: int):\n        raise ValueError(f\"You will never succeed with {str(v)}\")\n\nretries = Retrying(\n    retry=retry_if_not_exception_type(ZeroDivisionError), stop=stop_after_attempt(3)\n)\n# Use the client to create a user detail\ntry:\n    user = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[{\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}],\n        max_retries=retries,\n    )\nexcept InstructorRetryException as e:\n    print(e.messages[-1][\"content\"])  # type: ignore\n    \"\"\"\n    Validation Error found:\n    1 validation error for UserDetail\n    age\n      Value error, You will never succeed with 25 [type=value_error, input_value=25, input_type=int]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    Recall the function correctly, fix the errors\n    \"\"\"\n\n    print(e.n_attempts)\n    #> 3\n\n    print(e.last_completion)\n    \"\"\"\n    ChatCompletion(\n        id='chatcmpl-AWl4B5JrGm7QSxBPQhx7lQH89WHxg',\n        choices=[\\\n            Choice(\\\n                finish_reason='stop',\\\n                index=0,\\\n                logprobs=None,\\\n                message=ChatCompletionMessage(\\\n                    content=None,\\\n                    refusal=None,\\\n                    role='assistant',\\\n                    audio=None,\\\n                    function_call=None,\\\n                    tool_calls=[\\\n                        ChatCompletionMessageToolCall(\\\n                            id='call_LGFqmLGaMvlkriHANf2nqLus',\\\n                            function=Function(\\\n                                arguments='{\"name\":\"Jason\",\"age\":25}', name='UserDetail'\\\n                            ),\\\n                            type='function',\\\n                        )\\\n                    ],\\\n                ),\\\n            )\\\n        ],\n        created=1732370783,\n        model='gpt-3.5-turbo-0125',\n        object='chat.completion',\n        service_tier=None,\n        system_fingerprint=None,\n        usage=CompletionUsage(\n            completion_tokens=27,\n            prompt_tokens=522,\n            total_tokens=549,\n            completion_tokens_details=CompletionTokensDetails(\n                audio_tokens=0, reasoning_tokens=0\n            ),\n            prompt_tokens_details=None,\n            prompt_token_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0),\n        ),\n    )\n    \"\"\"\n\n```\n\n## Advanced: Retry Logic [¶](https://python.useinstructor.com/concepts/retrying/?q=\\#advanced-retry-logic \"Permanent link\")\n\nIf you want more control over how we define retries such as back-offs and additional retry logic we can use a library called Tenacity. To learn more, check out the documentation on the [Tenacity](https://tenacity.readthedocs.io/en/latest/) website.\n\nRather than using the decorator `@retry`, we can use the `Retrying` and `AsyncRetrying` classes to define our own retry logic.\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel\nfrom tenacity import Retrying, stop_after_attempt, wait_fixed\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\\\n    ],\n    max_retries=Retrying(\n        stop=stop_after_attempt(2),\n        wait=wait_fixed(1),\n    ),\n)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n\n```\n\n### asynchronous retries [¶](https://python.useinstructor.com/concepts/retrying/?q=\\#asynchronous-retries \"Permanent link\")\n\nIf you're using asynchronous code, you can use `AsyncRetrying` instead.\n\n```md-code__content\nimport openai\nimport instructor\nfrom pydantic import BaseModel\nfrom tenacity import AsyncRetrying, stop_after_attempt, wait_fixed\n\nclient = instructor.from_openai(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\ntask = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\\\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\\\n    ],\n    max_retries=AsyncRetrying(\n        stop=stop_after_attempt(2),\n        wait=wait_fixed(1),\n    ),\n)\n\nimport asyncio\n\nresponse = asyncio.run(task)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n\n```\n\n## Other Features of Tenacity [¶](https://python.useinstructor.com/concepts/retrying/?q=\\#other-features-of-tenacity \"Permanent link\")\n\nTenacity features a huge number of different retrying capabilities. A few of them are listed below.\n\n- `Retrying(stop=stop_after_attempt(2))`: Stop after 2 attempts\n- `Retrying(stop=stop_after_delay(10))`: Stop after 10 seconds\n- `Retrying(wait=wait_fixed(1))`: Wait 1 second between each attempt\n- `Retrying(wait=wait_random(0, 1))`: Wait a random amount of time between 0 and 1 seconds\n- `Retrying(wait=wait_exponential(multiplier=1, min=4, max=10))`: Wait an exponential amount of time between 4 and 10 seconds\n- `Retrying(wait=(stop_after_attempt(2) | stop_after_delay(10)))`: Stop after 2 attempts or 10 seconds\n- `Retrying(wait=(wait_fixed(1) + wait_random(0.2)))`: Wait at least 1 second and add up to 0.2 seconds\n\nRemember that for async clients you need to use `AsyncRetrying` instead of `Retrying`!\n\n## Retry Callbacks [¶](https://python.useinstructor.com/concepts/retrying/?q=\\#retry-callbacks \"Permanent link\")\n\nYou can also define callbacks to be called before and after each attempt. This is useful for logging or debugging.\n\n```md-code__content\nfrom pydantic import BaseModel, field_validator\nimport instructor\nimport tenacity\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    def name_is_uppercase(cls, v: str):\n        assert v.isupper(), \"Name must be uppercase\"\n        return v\n\nresp = client.messages.create(\n    model=\"gpt-3.5-turbo\",\n    max_tokens=1024,\n    max_retries=tenacity.Retrying(\n        stop=tenacity.stop_after_attempt(3),\n        before=lambda _: print(\"before:\", _),\n\"\"\"\nbefore:\n<RetryCallState 5565220688: attempt #1; slept for 0.0; last result: none yet>\n\"\"\"\n        after=lambda _: print(\"after:\", _),\n    ),  # type: ignore\n    messages=[\\\n        {\\\n            \"role\": \"user\",\\\n            \"content\": \"Extract John is 18 years old.\",\\\n        }\\\n    ],\n    response_model=User,\n)\n\nassert isinstance(resp, User)\nassert resp.name == \"JOHN\"  # due to validation\nassert resp.age == 18\nprint(resp)\n#> name='JOHN' age=18\n\n\"\"\"\nbefore: <RetryCallState 4421908816: attempt #1; slept for 0.0; last result: none yet>\nafter: <RetryCallState 4421908816: attempt #1; slept for 0.0; last result: failed (ValidationError 1 validation error for User\nname\n  Assertion failed, Name must be uppercase [type=assertion_error, input_value='John', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error)>\n\nbefore: <RetryCallState 4421908816: attempt #2; slept for 0.0; last result: none yet>\nname='JOHN' age=18\n\"\"\"\n\n```\n\nWas this page helpful?\n\nThanks for your feedback!\n\nThanks for your feedback! Help us improve this page by using our [feedback form](https://forms.gle/ijr9Zrcg2QWgKoWs7).\n\nBack to top",
    "metadata": {
      "url": "https://python.useinstructor.com/concepts/retrying/?q=",
      "ogUrl": "https://python.useinstructor.com/concepts/retrying/",
      "title": "Implementing Effective Retry Logic with Pydantic and Tenacity - Instructor",
      "author": "Jason Liu",
      "og:url": "https://python.useinstructor.com/concepts/retrying/",
      "og:type": "website",
      "ogImage": "https://python.useinstructor.com/assets/images/social/concepts/retrying.png",
      "ogTitle": "Implementing Effective Retry Logic with Pydantic and Tenacity - Instructor",
      "language": "en",
      "og:image": "https://python.useinstructor.com/assets/images/social/concepts/retrying.png",
      "og:title": "Implementing Effective Retry Logic with Pydantic and Tenacity - Instructor",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.46",
      "sourceURL": "https://python.useinstructor.com/concepts/retrying/?q=",
      "statusCode": 200,
      "description": "Learn how to establish simple and advanced retry strategies in Python using Pydantic and Tenacity for robust application behavior.",
      "theme-color": "#00000000",
      "color-scheme": "normal",
      "twitter:card": "summary_large_image",
      "og:image:type": "image/png",
      "ogDescription": "Learn how to establish simple and advanced retry strategies in Python using Pydantic and Tenacity for robust application behavior.",
      "twitter:image": "https://python.useinstructor.com/assets/images/social/concepts/retrying.png",
      "twitter:title": "Implementing Effective Retry Logic with Pydantic and Tenacity - Instructor",
      "og:description": "Learn how to establish simple and advanced retry strategies in Python using Pydantic and Tenacity for robust application behavior.",
      "og:image:width": "1200",
      "og:image:height": "630",
      "ogLocaleAlternate": [],
      "twitter:description": "Learn how to establish simple and advanced retry strategies in Python using Pydantic and Tenacity for robust application behavior."
    }
  }
]